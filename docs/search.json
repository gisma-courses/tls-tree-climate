[
  {
    "objectID": "doc/new_utils.html",
    "href": "doc/new_utils.html",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "",
    "text": "This document provides a detailed explanation of the core functions used in the ENVI-met 3DPLANT ALS pipeline. These functions handle voxelization, LAD conversion, trait computation, clustering, and export preparation.\nEach function is explained with:\n\nDescription and purpose\nInputs and outputs\nDetailed internal logic\nUsage examples (if applicable)",
    "crumbs": [
      "Function Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils.html#overview",
    "href": "doc/new_utils.html#overview",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "",
    "text": "This document provides a detailed explanation of the core functions used in the ENVI-met 3DPLANT ALS pipeline. These functions handle voxelization, LAD conversion, trait computation, clustering, and export preparation.\nEach function is explained with:\n\nDescription and purpose\nInputs and outputs\nDetailed internal logic\nUsage examples (if applicable)",
    "crumbs": [
      "Function Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils.html#preprocess_voxels",
    "href": "doc/new_utils.html#preprocess_voxels",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "preprocess_voxels()",
    "text": "preprocess_voxels()\n#' Preprocess ALS point cloud into voxel grid\n#'\n#' @param las_norm Normalized LAS object\n#' @param res_xy Horizontal resolution (in meters)\n#' @param res_z Vertical resolution (in meters)\n#'\n#' @return A list of voxel information used for LAD calculation\nThis function converts a normalized ALS point cloud into a structured voxel representation. Each voxel counts the number of returns in a fixed 3D grid.\nInternally uses grid_metrics() with z-based slicing to accumulate point counts.",
    "crumbs": [
      "Function Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils.html#convert_to_lad_beer",
    "href": "doc/new_utils.html#convert_to_lad_beer",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "convert_to_LAD_beer()",
    "text": "convert_to_LAD_beer()\n#' Convert voxel pulse counts to LAD using Beer–Lambert Law\n#'\n#' @param df Data frame containing voxel counts per vertical bin (e.g. \"layer_1\", ..., \"layer_n\")\n#' @param grainsize Vertical voxel height (in meters)\n#' @param k Extinction coefficient (default 0.3–0.5)\n#' @param scale_factor Optional scaling multiplier\n#' @param lad_max Optional upper LAD threshold\n#' @param lad_min Optional lower LAD threshold\n#' @param keep_pulses Whether to retain original pulse columns\n#'\n#' @return Data frame with LAD values by voxel column\n\nExplanation\nThis function applies the Beer–Lambert law to pulse return counts to estimate LAD per voxel. The formula is:\n\\[\n\\text{LAD}_i = -\\frac{\\ln(1 - \\frac{N_i}{N_{max}})}{k \\cdot \\Delta z}\n\\]\n\n\\(N_i\\): Number of pulses in voxel \\(i\\)\n\\(N_{max}\\): Maximum number of pulses in that column\n\\(k\\): Extinction coefficient\n\\(\\Delta z\\): Voxel height\n\nThe result is a per-voxel LAD profile suitable for vertical vegetation structure modeling.",
    "crumbs": [
      "Function Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils.html#compute_traits_from_lad",
    "href": "doc/new_utils.html#compute_traits_from_lad",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "compute_traits_from_lad()",
    "text": "compute_traits_from_lad()\n#' Compute plant traits from LAD profile\n#'\n#' @param lad_df Data frame with LAD profile (long format)\n#' @param res_z Voxel height (in meters)\n#'\n#' @return Data frame with structural traits per ENVIMET_ID\nComputes key biophysical traits needed by ENVI-met 3DPLANT:\n\nLAI (Leaf Area Index)\nMax LAD\nCrown Height\nTotal Height\nRoughness Length\nLeaf Thickness (from species table)\nLAD Cutoff (default: inferred)\n\nThese are required for synthetic plant representation in .pld files.",
    "crumbs": [
      "Function Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils.html#export_lad_to_envimet_p3d",
    "href": "doc/new_utils.html#export_lad_to_envimet_p3d",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "export_lad_to_envimet_p3d()",
    "text": "export_lad_to_envimet_p3d()\n#' Export clustered LAD profiles to ENVI-met PLANT3D XML\n#'\n#' @param lad_df LAD profile in long format with traits and species info\n#' @param file_out Path to output `.pld` file\n#' @param res_z Vertical resolution (meters)\n#' @param trait_df Optional trait table to override defaults\nGenerates a PLANT3D-compliant .pld file with:\n\n&lt;plant&gt; entries for each cluster (with &lt;LAD&gt; series)\nMetadata including LAI, MaxLAD, RoughnessLength, etc.\n&lt;plantclass&gt; label and species name\n\nEach vertical LAD profile is saved per ENVIMET_ID for use in simulation.\n\nLet me know if you’d like to: - Include example plots - Add function internals (e.g., how LAD cutoff is inferred) - Integrate this into your main .qmd pipeline tutorial - Export to PDF or GitHub Pages",
    "crumbs": [
      "Function Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/osm2envi.html",
    "href": "doc/osm2envi.html",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "",
    "text": "The OSM2Envi_met QGIS Processing tool provides a fully automated workflow for preparing spatial input data from OpenStreetMap (OSM) and elevation datasets for use in ENVI-met 3D simulations. It combines a Python-based QGIS interface with a shell script that performs a robust geospatial preprocessing pipeline via qgis_process, gdal, and ogr2ogr.\nThis document describes how to install the tool, explains the internal processing workflow, and provides a link to the complete codebase.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#requirements",
    "href": "doc/osm2envi.html#requirements",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "Requirements",
    "text": "Requirements\nTo run this tool successfully, you need:\n\nQGIS 3.28+ with qgis_process installed and accessible from the command line\nGDAL with ogr2ogr and gdal_calc.py\nA Bash shell (macOS/Linux, or Git Bash on Windows)",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#setup-steps",
    "href": "doc/osm2envi.html#setup-steps",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "Setup Steps",
    "text": "Setup Steps\n\nDownload the tool:\n\nFrom GitHub as ZIP:\nhttps://github.com/gisma/qgis-processing-workflows/archive/refs/heads/main.zip\nOr clone via Git: git clone https://github.com/gisma/qgis-processing-workflows.git\n\nCopy the relevant scripts to your QGIS processing script directory:\n~/.local/share/QGIS/QGIS3/profiles/default/processing/scripts/ ├── osm2envi_qgis.sh # Main processing Bash script └── osm2envi_tool.py # QGIS Processing tool wrapper\nMake the Bash script executable:\nchmod +x osm2envi_qgis.sh\nRestart QGIS.\nThe tool will now appear under: Processing Toolbox → Envi_met Tools → OSM2Envi_met",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#osm-conversion",
    "href": "doc/osm2envi.html#osm-conversion",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "1. OSM Conversion",
    "text": "1. OSM Conversion\n\nConverts the .osm file into a multi-layered GeoPackage using ogr2ogr.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#feature-extraction",
    "href": "doc/osm2envi.html#feature-extraction",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "2. Feature Extraction",
    "text": "2. Feature Extraction\n\nExtracts thematic layers from multipolygons or lines using SQL expressions:\n\nVegetation: landuse like forest, meadow, orchard, etc.\nSurfaces: roads and natural surfaces from highway and landuse\nBuildings: all OSM geometries tagged as building\n\nEach layer is:\n\nReprojected into the target CRS using qgis_process\nClipped to the specified extent",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#classification-envimet-id",
    "href": "doc/osm2envi.html#classification-envimet-id",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "3. Classification (ENVIMET ID)",
    "text": "3. Classification (ENVIMET ID)\n\nAssigns an ENVIMET_ID attribute based on feature type:\n\nForest → 0000SM\nAsphalt → 0200AK\nIndustrial landuse → 0200AK\nWetland → 0200LI\n\nClassification is done using SQL CASE statements in qgis_process:fieldcalculator.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#height-extraction-optional",
    "href": "doc/osm2envi.html#height-extraction-optional",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "4. Height Extraction (optional)",
    "text": "4. Height Extraction (optional)\nIf both DSM and DEM are provided:\n\nCalculates a difference raster (DSM − DEM) using gdal_calc.py\nComputes mean building heights (height_mean) via zonal statistics\nAdds height attribute to the building layer\n\nIf no elevation data is provided, this step is skipped.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#road-buffering",
    "href": "doc/osm2envi.html#road-buffering",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "5. Road Buffering",
    "text": "5. Road Buffering\n\nBuffers road geometries based on highway type:\n\nPrimary: 10 m, Secondary: 6 m, Tertiary: 4 m, Track: 1 m\n\nBuffers are classified like surfaces and merged later.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#layer-merging",
    "href": "doc/osm2envi.html#layer-merging",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "6. Layer Merging",
    "text": "6. Layer Merging\n\nMerges:\n\nBuffered roads\nSurface landuse polygons\n\nCreates a unified surface layer for ENVIMET (*_surface_final.gpkg)",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#cleanup",
    "href": "doc/osm2envi.html#cleanup",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "7. Cleanup",
    "text": "7. Cleanup\n\nDeletes all intermediate files:\n\n_tmp.gpkg, _proj.gpkg, _clip.gpkg\n\nKeeps only the final classified and merged output layers",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#optimization-for-envi-met",
    "href": "doc/osm2envi.html#optimization-for-envi-met",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "8. Optimization for ENVI-met",
    "text": "8. Optimization for ENVI-met\n\nRetains only necessary fields:\n\nGeometry\nENVIMET_ID\nheight_mean (if computed)\n\nSaves final layers as:\n\n*_surface_final_envimet.gpkg\n*_vegetation_final_envimet.gpkg\n*_buildings_final_envimet.gpkg",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#background-and-method",
    "href": "doc/tls_v1_2.html#background-and-method",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Background and Method",
    "text": "Background and Method\nThis section explains the theoretical principles of leaf area density (LAD) and describes how it can be determined using terrestrial laser scanning (TLS). Leaf area density is an important parameter in environmental modeling, for example for radiation balance and microclimate simulations. It indicates the leaf area per volume (m²/m³) and is therefore a decisive factor for microclimate simulations, radiation models, and energy flows in vegetation stands.\n\n\n\n\n\n\n\n\nApproach Type\nName / Description\nNature\n\n\n\n\nPulse-count based\nSimple linear normalization of return counts or voxel hits\nEmpirical, direct\n\n\nLinear normalization\nStraightforward normalization of pulse counts by voxel volume or max LAD\nEmpirical, basic\n\n\nPulse-density normalization\nAdjusts for occlusion and scan geometry\nSemi-empirical\n\n\nGap fraction models\nEstimate LAD/LAI from canopy openness statistics\nSemi-empirical\n\n\nBeer–Lambert conversion conversion\nUses exponential light attenuation to infer LAD\nPhysically-based\n\n\nVoxel-based inverse modeling\nOptimizes 3D LAD to match observed light attenuation or reflectance\nPhysically-based\n\n\nAllometric / geometric reconstruction\nReconstructs crown volume and distributes LAD using QSM or shape fitting\nGeometric, structural\n\n\n\n\nLinear normalization is a practical baseline: simple, fast, and reproducible.\nBeer–Lambert conversion introduces realism via physical light attenuation.\n\nMore advanced models (e.g. voxel inverse or QSM-based) aim for higher biophysical fidelity at the cost of complexity.\nThe present analysis is based on TLS with a medium-range RIEGL scanner (e.g., VZ-400). This captures millions of 3D points of the vegetation structure with high angular resolution. The point cloud is divided into uniform voxels, from which the leaf area density is estimated in two ways.\n\nLinear normalization (straightforwad)\n\\[\n\\text{LAD}_i = \\frac{N_i}{N_{\\max}} \\cdot \\text{LAD}_{\\max}\n\\] - \\(N_i\\): Number of laser points in voxel \\(i\\)\n- \\(N_{\\max}\\): Maximum across all voxels\n- \\(\\text{LAD}_{\\max}\\): Maximum LAD value from the literature (e.g., 5 m²/m³)\n\n\n\nBeer–Lambert conversion\n\\[\n\\text{LAD}_i = -\\frac{\\ln\\left(1 - \\frac{N_i}{N_{\\max}}\\right)}{k \\cdot \\Delta z}\n\\]\n\n\\(k\\): Extinction coefficient (typically 0.3–0.5)\n\\(\\Delta z\\): vertical voxel height\n\n\n\nOverall Workflow\nWhat happens in the script?\n\n\n\n\n\n\n\n\nStep\nDescription\nRelevant Code\n\n\n\n\n1. Read & Filter LAS\nLoad TLS data, optionally crop and clean it\nreadLAS() and las = filter_poi(...)\n\n\n2. Voxel Grid Setup\nSet up 3D grid at defined grain.size\npassed to pixel_metrics(..., res = grain.size)\n\n\n3. Count Pulses\nCount returns in each voxel height bin\npointsByZSlice() function\n\n\n4. Normalise Pulse Counts\nDivide by global max (relative LAD)\nin convert_to_LAD(): lad = (count / max) * LADmax\n\n\n5. Export Raster\nConvert metrics to raster stack\nterra::rast() from voxel_df\n\n\n6. Visualization\nPlot LAD profiles\nsee plotting section\n\n\n7. Export to Plant3D\nExports the LAD to ENVI-met\nsee export section",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#implemetation",
    "href": "doc/tls_v1_2.html#implemetation",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Implemetation",
    "text": "Implemetation\nTo use this ENVI-met tree modeling workflow in R, follow these steps to load and initialize the project correctly:",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#clone-github-repo-in-rstudio",
    "href": "doc/tls_v1_2.html#clone-github-repo-in-rstudio",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Clone GitHub Repo in RStudio",
    "text": "Clone GitHub Repo in RStudio\n\nOption 1: RStudio GUI\n\nGo to File → New Project → Version Control → Git\nEnter the repository URL:\nhttps://github.com/gisma-courses/tls-tree-climate.git\nChoose a project directory and name\nClick Create Project\n\n\n\n\nOption 2: Terminal\ngit clone https://github.com/gisma-courses/tls-tree-climate.git\nThen open the cloned folder in RStudio (via .Rproj file or “Open Project”).\n\nNote: Make sure Git is installed and configured in\nRStudio → Tools → Global Options → Git/SVN\n\nThe use of the {here} package depends on having a valid RStudio project. Without this, file paths may not resolve correctly.\n\n\nData Input Parameters and Paths\n\n\n\n\n\n\nThe input data set is a cleaned terrestrial laser scan of a single, isolated tree. All surrounding vegetation and ground points have been removed, so the file contains only the tree’s structure—trunk, branches, and foliage. Stored in standard LAS format, it provides high-resolution 3D point data suitable for voxelization, LAD calculation, or input into microclimate and radiative models. This detailed structural data is essential for generating true 3D tree entities in ENVI-met; without it, only simplified vegetation (SimplePlants) can be used.\n\n\n\nSet global parameters for the workflow, such as file paths, voxel resolution, and maximum LAD value for normalization.\n\nlibrary(lidR)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(here)\nlibrary(XML)\nlibrary(stats)\nlibrary(tibble)\nlibrary(data.table)\nlibrary(rprojroot)\n\nzmax &lt;- 40  \ngrain.size &lt;- 1  \nproject_root &lt;- here::here()  \n\n# Choose LAD method: \"linear\" or \"beer\"\n# Beer–Lambert conversion Notes:\n# - Avoids log(0) and 1 by clipping near-extreme values\n# - Use when cumulative light absorption or occlusion is relevant\n# - Suitable if extinction coefficient is known or estimated from prior studies\nlad_method &lt;- \"beer\"  # Set to \"linear\" or \"beer\"\n\n# Optional: extinction coefficient (used only for Beer–Lambert conversion)\nk_extinction &lt;- 0.25\n\n\noutput_voxels &lt;- file.path(project_root, \"data/TLS/LAD_voxDF.rds\")  \noutput_array &lt;- file.path(project_root, \"data/TLS/lad_array_m2m3.rds\")  \noutput_profile_plot &lt;- file.path(project_root, \"data/TLS/lad_vertical_profile.pdf\")  \noutput_envimet_tls_3d &lt;- file.path(project_root, \"data/envimet/tls_envimet_trees.pld\")  \noutput_envimet_als_3d &lt;- file.path(project_root, \"data/envimet/als_envimet_trees.pld\")  \n\n\n\n\nVoxelization of TLS data\nVoxelisation turns a 3D TLS point cloud into a grid of cubes (voxels), where each voxel holds structural information. The number of points per voxel is used to estimate Leaf Area Density (LAD), typically normalized relative to the voxel with the most returns.\n\nEach voxel = a 1×1×1 m³ cube\nCount the laser hits per voxel\nNormalize to maximum\nMultiply by a literature-based LAD_max (e.g. 5 m²/m³)\n\nThis gives a spatially distributed LAD profile suitable for further analysis or models like ENVI-met.\n\n\n\n\n\n\nView Code\n\n\n\n\n\n\nlibrary(terra)\n\n las=lidR::readLAS(\"../data/TLS/tree_08.laz\")\n\n\n[============================&gt;                     ] 56% ETA: 1s     \n[============================&gt;                     ] 56% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[====================================&gt;             ] 72% ETA: 1s     \n[====================================&gt;             ] 72% ETA: 1s     \n[====================================&gt;             ] 72% ETA: 0s     \n[====================================&gt;             ] 72% ETA: 0s     \n[====================================&gt;             ] 72% ETA: 0s     \n[====================================&gt;             ] 72% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n                                                                                \n\n  las@data$Z &lt;- las@data$Z - min(las@data$Z, na.rm = TRUE)  \n  maxZ &lt;- min(floor(max(las@data$Z, na.rm = TRUE)), zmax)  \n  las@data$Z[las@data$Z &gt; maxZ] &lt;- maxZ  \npointsByZSlice = function(Z, maxZ){\n  heightSlices = as.integer(Z) # Round down\n  zSlice = data.table::data.table(Z=Z, heightSlices=heightSlices) # Create a data.table (Z, slices))\n  sliceCount = stats::aggregate(list(V1=Z), list(heightSlices=heightSlices), length) # Count number of returns by slice\n  \n  ##############################################\n  # Add columns to equalize number of columns\n  ##############################################\n  colRange = 0:maxZ\n  addToList = setdiff(colRange, sliceCount$heightSlices)\n  n = length(addToList)\n  if (n &gt; 0) {\n    bindDt = data.frame(heightSlices = addToList, V1=integer(n))\n    sliceCount = rbind(sliceCount, bindDt)\n    # Order by height\n    sliceCount = sliceCount[order(sliceCount$heightSlices),]\n  }\n  \n  colNames = as.character(sliceCount$heightSlices)\n  colNames[1] = \"ground_0_1m\"\n  colNames[-1] = paste0(\"pulses_\", colNames[-1], \"_\", sliceCount$heightSlices[-1]+1, \"m\")\n  metrics = list()\n  metrics[colNames] = sliceCount$V1\n  \n  return(metrics)\n  \n} #end function pointsByZSlice\n\n# --- Main function ---\npreprocess_voxels &lt;- function(normlas, grain.size = 1, maxP =zmax, normalize = TRUE, as_raster = TRUE) {  \n  las &lt;- normlas  \n  \n  # Filter height range\n  las &lt;- filter_poi(las, Z &gt;= 0 & Z &lt;= maxP)  \n  if (lidR::is.empty(las)) return(NULL)\n  # Determine Z-slices\n  maxZ &lt;- floor(max(las@data$Z))  \n  maxZ &lt;- min(maxZ, maxP)  \n  \n  \n  # Compute voxel metrics\n  func &lt;- formula(paste0(\"~pointsByZSlice(Z, \", maxZ, \")\"))  \n  voxels &lt;- pixel_metrics(las, func, res = grain.size)  # Calculate metrics in each voxel (3D grid cell)\n  \n  # Optionally normalize values by voxel volume\n  if (normalize) {\n    vvol &lt;- grain.size^3  \n    voxels &lt;- voxels / vvol  \n  }\n  \n  # Return as both terra::SpatRaster and data.frame\n  result &lt;- list()  \n  \n  if (as_raster) {\n    result$raster &lt;- voxels  \n  }\n  \n  # Convert to data.frame\n  xy &lt;- terra::xyFromCell(voxels, seq_len(ncell(voxels)))  \n  vals &lt;- terra::values(voxels)  \n  df &lt;- cbind(xy, vals)  \n  colnames(df)[1:2] &lt;- c(\"X\", \"Y\")  \n  result$df &lt;- df  \n  \n  return(result)\n}\n\n\n\n\nvox_out &lt;- preprocess_voxels(las, grain.size = 1, maxP = zmax)  \n\n\n\n\n\nConversion to LAD (m²/m³)\nThe conversion to LAD (Leaf Area Density, in m²/m³) from TLS-based voxel pulse counts is done using a relative normalization heuristic which is adopted as a practical approximation in voxel-based canopy structure analysis using TLS (Terrestrial Laser Scanning) data.:\nFor each voxel layer (e.g. pulses_2_3m), the LAD is calculated as:\n\\[\n\\text{LAD}_{\\text{voxel}} = \\left( \\frac{\\text{pulse count in voxel}}{\\text{maximum pulse count over all voxels}} \\right) \\times \\text{LAD}_{\\text{max}}\n\\]\nWhere:\n\npulse count in voxel = number of returns in this voxel layer (from TLS)\nmax_pulse = the maximum pulse count found in any voxel (used for normalization)\nLAD_max = a fixed normalization constant (e.g. 5.0 m²/m³) chosen from literature or calibration\n\n\n\n\n\n\n\nTypical LADₘₐₓ Values by Species\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies / Structure Type\nLADₘₐₓ (m²/m³)\nSource / Notes\n\n\n\n\nFagus sylvatica (European beech)\n3.5–5.5\nCalders et al. (2015), Chen et al. (2018)\n\n\nQuercus robur (English oak)\n3.0–6.0\nHosoi & Omasa (2006), field studies with TLS voxelization\n\n\nConiferous trees (e.g. pine)\n4.0–7.0\nWilkes et al. (2017), higher LAD due to needle density\n\n\nMixed broadleaf forest\n3.0–6.0\nFlynn et al. (2023), canopy averaged estimates\n\n\nShrubs / understorey\n1.5–3.0\nChen et al. (2018),lower vertical structure density\n\n\nUrban street trees\n2.0–4.0\nSimon et al. (2020), depending on pruning and species\n\n\n\nLAD values refer to maximum expected per 1 m vertical voxel. Values depend on species, seasonality, and scanning conditions.\n\n\n\nWhat this means conceptually\nYou’re not measuring absolute LAD, but instead:\n\nUsing the number of TLS returns per voxel as a proxy for leaf density\nThen normalization all voxels relatively to the most “leaf-dense” voxel\nThe LAD_max defines what value the “densest” voxel should reach in terms of LAD\n\nThis is fast, simple, and works well when:\n\nYou want relative structure across the canopy\nYou don’t have absolute calibration (e.g. with destructive sampling or hemispheric photos)\n\nCaveats and assumptions\n\nThis approach assumes the TLS beam returns are proportional to leaf area, which is a simplification\nIt’s sensitive to occlusion and TLS positioning\nThe choice of LAD_max is crucial—common values from literature range from 3–7 m²/m³ for dense canopies\n\nThe LAD conversion in the following code is a relative, normalized mapping of TLS pulse counts to LAD values, normalized by the highest voxel return and normalized using a fixed LAD_max. This gives a plausible LAD field usable for analysis, visualization, or simulation input (e.g. for ENVI-met).\n\n\n\n\n\n\nView Code\n\n\n\n\n\n\nlibrary(terra)\nconvert_matrix_to_df &lt;- function(mat) {  \n  df &lt;- as.data.frame(mat)  \n  colnames(df) &lt;- attr(mat, \"dimnames\")[[2]]  \n  return(df)\n}\n\n# --- Preprocess LiDAR data into voxel metrics -------------------------------\nvox_out &lt;- preprocess_voxels(las, grain.size = 1, maxP = zmax)  # Calculate vertical pulse metrics\nvox_df &lt;- convert_matrix_to_df(vox_out$df)                      # Convert voxel array to data.frame\n\n#' Convert TLS voxel pulse data to LAD using Beer–Lambert conversion conversion with post-normalization\n#'\n#' @param df A data.frame with pulse columns (from TLS voxelization)\n#' @param grainsize Numeric, vertical voxel height (e.g., 1 m)\n#' @param k Extinction coefficient (default: 0.3)\n#' @param scale_factor Optional multiplicative scale factor (default: 1.2)\n#' @param lad_max Optional maximum LAD clamp (e.g. 2.5); set to NULL to disable\n#' @param lad_min Optional minimum LAD threshold (e.g. 0.05); set to NULL to disable\n#' @param keep_pulses Logical, whether to retain pulse columns (default: FALSE)\n#'\n#' @return Data.frame with LAD columns added\n#' @export\nconvert_to_LAD_beer &lt;- function(df,\n                                grainsize = 1,\n                                k = 0.3,\n                                scale_factor = 1.2,\n                                lad_max = 2.5,\n                                lad_min = 0.05,\n                                keep_pulses = FALSE) {\n  df_lad &lt;- df\n  pulse_cols &lt;- grep(\"^pulses_\", names(df_lad), value = TRUE)\n  \n  for (col in pulse_cols) {\n    lad_col &lt;- paste0(\"lad_\", sub(\"pulses_\", \"\", col))\n    p_rel &lt;- df_lad[[col]] / max(df_lad[[col]], na.rm = TRUE)\n    \n    # Avoid log(0) and 1\n    p_rel[p_rel &gt;= 1] &lt;- 0.9999\n    p_rel[p_rel &lt;= 0] &lt;- 1e-5\n    \n    # Apply Beer–Lambert conversion\n    lad_vals &lt;- -log(1 - p_rel) / (k * grainsize)\n    \n    # Apply normalization\n    lad_vals &lt;- lad_vals * scale_factor\n    \n    # Clamp LAD values if needed\n    if (!is.null(lad_max)) {\n      lad_vals &lt;- pmin(lad_vals, lad_max)\n    }\n    if (!is.null(lad_min)) {\n      lad_vals &lt;- pmax(lad_vals, lad_min)\n    }\n    \n    df_lad[[lad_col]] &lt;- lad_vals\n    \n    if (!keep_pulses) {\n      df_lad[[col]] &lt;- NULL\n    }\n  }\n  \n  return(df_lad)\n}\n\n\n#' Convert TLS Pulse Counts to Leaf Area Density (LAD)\n#'\n#' Transforms vertically binned pulse counts (from voxelized TLS data) into Leaf Area Density (LAD, m²/m³)\n#' by normalizing pulse values to a specified LAD maximum.\n#'\n#' @param df A `data.frame` containing voxelized TLS pulse data. Must include columns starting with `\"pulses_\"`, \n#'           each representing pulse returns per vertical layer (e.g. `pulses_1_2m`, `pulses_2_3m`, ...).\n#' @param grainsize Numeric. The voxel edge length in meters (assumed cubic). Default is `1`.\n#' @param LADmax Numeric. The maximum LAD value in m²/m³ for relative normalization. Common values: `4.0`–`6.0`. Default is `5.0`.\n#' @param keep_pulses Logical. If `FALSE` (default), the original pulse columns are removed from the output. If `TRUE`, they are retained alongside the LAD columns.\n#'\n#' @return A modified `data.frame` with new LAD columns (`lad_1_2m`, `lad_2_3m`, ...) in m²/m³, normalized relatively to `LADmax`.\n#'\n#' @details\n#' - Each `pulses_*` column is linearly normalized by the overall maximum value across all vertical bins and locations.\n#' - The result is a relative LAD estimate, useful for ecological modeling, input to microclimate simulations (e.g., ENVI-met), or structural analysis.\n#' - Voxel volume is implicitly considered constant due to cubic assumption (via `grainsize`) but is not explicitly used here.\n#'\n#' @examples\n#' \\dontrun{\n#'   df_vox &lt;- readRDS(\"TLS/voxel_metrics.rds\")\n#'   lad_df &lt;- convert_to_LAD(df_vox, grainsize = 1, LADmax = 5)\n#'   head(names(lad_df))  # Should show lad_* columns\n#' }\n#'\n#' @export\nconvert_to_LAD &lt;- function(df, grainsize = 1, LADmax = 5.0, keep_pulses = FALSE) {  \n  # df: Data frame mit voxelisierten TLS-Daten\n# grainsize: Voxelgröße in m (würfelförmig angenommen)\n# LADmax: maximaler LAD-Wert (Literaturbasiert, z. B. 5.0 m²/m³)\n  df_lad &lt;- df  \n  pulse_cols &lt;- grep(\"^pulses_\", names(df_lad), value = TRUE)  \n  \n  # Schichtanzahl = Anzahl Pulse-Spalten\n  n_layers &lt;- length(pulse_cols)  \n  \n  # Optional: originales Maximum zur linearen Skalierung (relativ)\n  max_pulse &lt;- max(df_lad[, pulse_cols], na.rm = TRUE)  \n  \n  # Umwandlung in LAD (m²/m³) – Skaliert auf LADmax oder absolut (siehe Kommentar)\n  for (col in pulse_cols) {\n    lad_col &lt;- paste0(\"lad_\", sub(\"pulses_\", \"\", col))  \n    \n    # Hier wird RELATIV zu max_pulse skaliert → einfache Normalisierung\n    df_lad[[lad_col]] &lt;- (df_lad[[col]] / max_pulse) * LADmax  \n    \n    # Optional: löschen der Pulse-Spalten\n    if (!keep_pulses) {\n      df_lad[[col]] &lt;- NULL  \n    }\n  }\n  \n  return(df_lad)\n}\n\n\n\n# method selection\nif (lad_method == \"beer\") {\n  message(\"✔ Using Beer–Lambert conversion LAD conversion...\")\n  df_lad &lt;- convert_to_LAD_beer(\n    vox_df,\n    grainsize = 1,\n    k = k_extinction,\n    scale_factor = 0.4,\n    lad_max = 2.5,\n    lad_min = 0.0\n  )\n} else if (lad_method == \"linear\") {\n  message(\"Using linear LAD conversion...\")\n  df_lad &lt;- convert_to_LAD(\n    vox_df,\n    grainsize = 1,\n    LADmax = 5.0\n  )\n} else {\n  stop(\"Unknown LAD conversion method: choose 'linear' or 'beer'\")\n}\n\n\n\n\n\nDT::datatable(head(df_lad, 5))\n\n\n\n\n\n\n\nRaster Stack Representation of 3D Vegetation (Voxel-Based)\nWe represent 3D vegetation using a voxel-based raster stack:\n\nSpace is divided into cubic voxels (e.g. 1 × 1 × 1 m).\nEach raster layer represents a height slice (e.g. 0–1 m, 1–2 m, …).\nVoxels store values like pulse counts or Leaf Area Density (LAD).\n\nThis 2D stack structure enables:\n\nVertical profiling of vegetation per XY column.\nLayer-wise analysis (e.g. median, entropy).\nIntegration with raster data like topography or irradiance.\nUse in raster-based ecological and microclimate models.\n\nIt supports both analysis and visualization of vertical structure with standard geospatial tools.\nENVI-met supports custom vegetation input via the SimplePlant method, which requires a vertical LAD profile per grid column. A raster stack derived from TLS data provides exactly this: each layer represents LAD in a specific height slice, and each XY cell corresponds to one vertical profile. This structure can be exported as CSV, ASCII rasters, or custom profile files.\nFor 3D vegetation parameterization in ENVI-met 5.8+, the raster stack enables preprocessing of spatially explicit LAD or LAI profiles, even if some reformatting is needed.\nThe raster stack also supports canopy clustering and prototyping. It allows classification of structural types, simplification of complex vegetation, and the creation of representative profiles for simulation.\n\n\nVisualization\n\nlibrary(terra)\n# In SpatRasterStack umwandeln\nxy &lt;- df_lad[, c(\"X\", \"Y\")]  \nlad_vals &lt;- df_lad[, grep(\"^lad_\", names(df_lad), value = TRUE)]  \n\nlad_raster &lt;- rast(cbind(xy, lad_vals), type = \"xyz\")  \nplot(lad_raster)\n\n\n\n\n\n\n\n\n\nLAD Profile Visualizations from TLS Data\nThe plot_lad_profiles() function visualizes vertical leaf area density (LAD) profiles derived from voxelized TLS (terrestrial laser scanning) data. LAD represents leaf surface area per unit volume (m²/m³). The function provides three main plot styles:\n\n\n1. XY Matrix Plot (plotstyle = \"each_median\")\n\nDisplays a grid of mini-profiles, each representing a 0.5 × 0.5 m (x/y) ground column.\nWithin each cell, a normalized vertical LAD profile is plotted:\n\nY-axis (height) is normalized from 0 to 1 per column.\nX-axis shows LAD values normalized relative to the global LAD maximum.\n\nUseful for comparing structural patterns across space.\n\n\n\n2. Overall Median Profile (plotstyle = \"all_median\")\n\nAggregates LAD values across all (x/y) locations by height bin.\nProduces a typical vertical profile using the median and smoothed with a moving average.\nHeight is shown in absolute units (e.g. meters).\nCaptures the dominant vertical canopy structure.\n\n\n\n3. Single Profile (plotstyle = \"single_profile\")\n\nExtracts and plots the LAD profile at a specific (x, y) coordinate.\nBoth LAD and height are shown in absolute units.\nPlots the true vertical structure at one location.\n\nThe matrix plot shows multiple vertical LAD profiles arranged in a grid, with each small plot corresponding to a specific spatial location. This allows the vertical vegetation structure to be viewed in relation to its position on the ground. To make the individual profiles comparable, both height and LAD values are normalized within the plot. A reference profile on the side shows the overall median LAD distribution by height, which helps interpret the scale and shape of the individual profiles.\n\n\n\n\n\n\nView Code\n\n\n\n\n\n\n# --- Reshape LAD data to long format ----------------------------------------\n\nlad_df &lt;- as.data.frame(lad_raster, xy = TRUE, na.rm = TRUE)     # Convert raster to data.frame\n\n# 1. Extract LAD columns and XY coordinates\npulse_cols &lt;- grep(\"^lad_\", names(lad_df), value = TRUE)\nxy_cols &lt;- c(\"x\", \"y\")  # Adjust to \"X\", \"Y\" if needed\n\n# 2. Reshape to long format (one row per LAD layer)\nlad_df &lt;- reshape(\n  data = lad_df[, c(xy_cols, pulse_cols)],\n  varying = pulse_cols,\n  v.names = \"LAD\",\n  timevar = \"layer\",\n  times = pulse_cols,\n  direction = \"long\"\n)\n\n# 3. Extract z-layer information from column names\nlad_df$z_low  &lt;- as.numeric(sub(\"lad_(\\\\d+)_.*\", \"\\\\1\", lad_df$layer))  \nlad_df$z_high &lt;- as.numeric(sub(\"lad_\\\\d+_(\\\\d+)m\", \"\\\\1\", lad_df$layer))  \n\n# 4. Compute mid-point height of each voxel layer\nlad_df$Height &lt;- (lad_df$z_low + lad_df$z_high) / 2  \n\n# 5. Round to whole meters to create height classes\nlad_df$Height_bin &lt;- round(lad_df$Height)  \n\n# --- Aggregate median LAD per 0.5 × 0.5 m column ----------------------------\nsetDT(lad_df)  # Use data.table for efficient aggregation\n\nlad_by_column &lt;- lad_df[  \n  , .(LAD_median = median(LAD, na.rm = TRUE)), \n  by = .(x, y, Height_bin)\n]\n\n# Convert back to regular data.frame\nlad_df &lt;- as.data.frame(lad_by_column)\n\nplot_lad_profiles &lt;- function(lad_df, plotstyle = c(\"each_median\", \"all_median\", \"single_profile\"),  \n                              single_coords = c(NA, NA)) {\n  plotstyle &lt;- match.arg(plotstyle)  \n  \n  # Combine x and y coordinates into a unique column ID\n  lad_df$col_id &lt;- paste(lad_df$x, lad_df$y, sep = \"_\")  \n  x_levels &lt;- sort(unique(lad_df$x))  \n  y_levels &lt;- sort(unique(lad_df$y))  \n  # Convert x/y coordinates to factor variables for matrix layout\n  lad_df$x_f &lt;- factor(lad_df$x, levels = x_levels)  \n  lad_df$y_f &lt;- factor(lad_df$y, levels = y_levels)  \n  n_x &lt;- length(x_levels)  \n  n_y &lt;- length(y_levels)  \n  \n  # Determine the maximum LAD value for relative normalization\n  lad_max &lt;- max(lad_df$LAD_median, na.rm = TRUE)  \n  height_range &lt;- range(lad_df$Height_bin, na.rm = TRUE)  \n  dx &lt;- 0.8  \n  dy &lt;- 0.8  \n  \n  par(mar = c(5, 5, 4, 5), xpd = TRUE)\n  \n \n\n  \n  # Differentiate by plot type: all profiles, overall profile, or single profile\n  if (plotstyle == \"each_median\") {\n # Load PNG legend\nlegend_img &lt;- png::readPNG(\"output.png\")\n\n# Define aspect-preserving image placement\nimg_height_units &lt;- 20\nimg_width_units &lt;- img_height_units * dim(legend_img)[2] / dim(legend_img)[1]  # preserve ratio\n\n# Define position\nimg_x_left &lt;- n_x + 1.5\nimg_x_right &lt;- img_x_left + img_width_units\nimg_y_bottom &lt;- 0\nimg_y_top &lt;- img_y_bottom + img_height_units\n\n# Begin plot\nplot(NA, xlim = c(1, n_x + img_width_units + 4), ylim = c(1, n_y),\n     type = \"n\", axes = FALSE, xlab = \"\", ylab = \"\",\n     main = \"Vertical LAD Profiles in XY Matrix\", asp = 1.2)\n\n\n# Draw all LAD profiles\nfor (i in seq_along(x_levels)) {\n  for (j in seq_along(y_levels)) {\n    profile &lt;- subset(lad_df, x == x_levels[i] & y == y_levels[j])\n    if (nrow(profile) == 0) next\n    lad_scaled &lt;- profile$LAD_median / lad_max\n    height_scaled &lt;- (profile$Height_bin - min(height_range)) / diff(height_range)\n    lines(x = lad_scaled * dx + i,\n          y = height_scaled * dy + j,\n          col = \"darkgreen\", lwd = 1)\n  }\n}\n\n# Axis labels for ground position\naxis(1, at = 1:n_x, labels = round(x_levels, 1), las = 2)\naxis(2, at = 1:n_y, labels = round(y_levels, 1), las = 2)\n\n# Add the image\nrasterImage(legend_img,\n            xleft = img_x_left,\n            xright = img_x_right,\n            ybottom = img_y_bottom,\n            ytop = img_y_top)\n\n    \n  } else if (plotstyle == \"all_median\") {\n    unique_heights &lt;- sort(unique(lad_df$Height_bin))  \n    lad_median &lt;- numeric(length(unique_heights))  \n    for (i in seq_along(unique_heights)) {\n      h &lt;- unique_heights[i]  \n      lad_median[i] &lt;- median(lad_df$LAD[lad_df$Height_bin == h], na.rm = TRUE)  \n    }\n    lad_smooth &lt;- stats::filter(lad_median, rep(1/3, 3), sides = 2)  \n    \n    plot(\n      lad_smooth, unique_heights,\n      type = \"l\",\n      col = \"darkgreen\",\n      lwd = 2,\n      xlab = \"Leaf Area Density (m²/m³)\",\n      ylab = \"Height (m)\",\n      main = \"Vertical LAD Profile (smoothed)\",\n      xlim = c(0, max(lad_smooth, na.rm = TRUE)),\n      ylim = range(unique_heights)\n    )\n    \n    text(\n      x = as.numeric(lad_smooth),\n      y = unique_heights,\n      labels = round(as.numeric(lad_smooth), 1),\n      pos = 4,\n      cex = 0.7,\n      col = \"black\"\n    )\n    grid()\n    \n    \n  } else if (plotstyle == \"single_profile\") {\n    x_target &lt;- single_coords[1]  \n    y_target &lt;- single_coords[2]  \n    tol &lt;- 1e-6  \n    \n    profile &lt;- subset(lad_df, abs(x - x_target) &lt; tol & abs(y - y_target) &lt; tol)  \n    \n    if (nrow(profile) == 0) {\n      # Show warning if no profile exists for selected coordinates\n      warning(\"No data for the selected coordinates.\")\n      plot.new()\n      title(main = paste(\"No profile at\", x_target, \"/\", y_target))\n      return(invisible(NULL))\n    }\n    \n    # Normalize height and LAD\n    height_range &lt;- range(profile$Height_bin, na.rm = TRUE)  \n    # Determine the maximum LAD value for relative normalization\n    lad_max &lt;- max(profile$LAD_median, na.rm = TRUE)  \n    \n    height_scaled &lt;- (profile$Height_bin - min(height_range)) / diff(height_range)  \n    height_unscaled &lt;- profile$Height_bin\n    # Determine the maximum LAD value for relative normalization\n    lad_scaled &lt;- profile$LAD_median / lad_max  \n    \n    plot(\n      x = lad_scaled,\n      y = height_unscaled, #height_scaled,\n      type = \"l\",\n      lwd = 2,\n      col = \"darkgreen\",\n      xlab = \"LAD (normalized)\",\n      ylab = \"Height (m)\",\n      main = paste(\"Profile at\", x_target, \"/\", y_target)\n    )\n  }\n}\n# --- Visualize LAD profiles -------------------------------------------------\n\n\n\n\n\n\n\nPlot the profiles\n\n# Option 1: Profile in each column\nplot_lad_profiles(lad_df, plotstyle = \"each_median\")\n\n\n\n\n\n\n\n# Option 2: Overall vertical LAD profile (median of all)\nplot_lad_profiles(lad_df, plotstyle = \"all_median\")\n\n\n\n\n\n\n\n# Option 3: Single profile at specified coordinates\nplot_lad_profiles(lad_df, plotstyle = \"single_profile\", single_coords = c(57.5, -94.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Pipe\n\n\n\n\n\n# Example: Run pipeline if script is sourced interactively\nif (interactive()) {\n  message(\"Running full voxelization pipeline...\")\n  las &lt;- lidR::readLAS(\"data/ALS/merged_output.las\")\n  las@data$Z &lt;- las@data$Z - min(las@data$Z, na.rm = TRUE)\n  zmax &lt;- 30  # example value, ensure it's defined\n  maxZ &lt;- min(floor(max(las@data$Z, na.rm = TRUE)), zmax)\n  las@data$Z[las@data$Z &gt; maxZ] &lt;- maxZ\n  \n  vox_out &lt;- preprocess_voxels(las, grain.size = 1, maxP = zmax)\n  message(\"Done.\")\n}\n\n\n\n\n\n\nView Code\n\n\n\n\n\n\nENVI-met 3D Tree Export\nThe next section describes more detailed how the key input values in the R function export_lad_to_envimet3d() are computed, derived or selected, and provides the rationale for each. The function converts a voxel-based Leaf Area Density (LAD) profile, typically obtained from Terrestrial Laser Scanning (TLS) data, into a structured XML file compatible with ENVI-met’s 3D tree model (.pld or PLANT3D).\nGiven the sensitivity of ENVI-met simulations to tree morphology and LAD distribution, the function ensures that the spatial dimensions, vertical layering and LAD intensity values are all correctly represented. Some parameters are optional, but can be derived from the data if not explicitly set.\nThe table below details each argument of the function, including its purpose, how it is determined and its necessity.\n\n\n\n\n\n\n\n\nCode Line\nMeaning\nReason\n\n\n\n\nlad_df &lt;-lad_df[!is.na(lad_df$LAD_median), ]\nRemoves entries with missing LAD values\nEnsures only valid data is used in the LAD calculation and XML export\n\n\nlad_df$i &lt;-as.integer(factor(lad_df$x))\nConverts x-coordinates to integer voxel column indices (i)\nRequired for ENVI-met LAD matrix indexing\n\n\nlad_df$j &lt;-as.integer(factor(lad_df$y))\nConverts y-coordinates to integer voxel row indices (j)\nSame as above, for the y-direction\n\n\nz_map &lt;-setNames( ...)\nMaps unique height bins to sequential vertical indices (k)\nTranslates height levels into voxel layers compatible with ENVI-met\n\n\nlad_df$k &lt;-z_map[as.character(lad_df$Height_bin)]\nApplies the vertical index to the LAD data\nAligns LAD values with ENVI-met vertical layer system\n\n\nlad_df$lad_value &lt;-round(lad_df$LAD_median * scale_factor, 5)\nScales LAD values and rounds to 5 digits\nBrings LAD values to a usable range for ENVI-met and ensures precision\n\n\ndataI &lt;-max(lad_df$i)\nGets the number of horizontal grid cells in i-direction (width)\nRequired as matrix size input for ENVI-met\n\n\ndataJ &lt;-max(lad_df$j)\nGets the number of horizontal grid cells in j-direction (depth)\nRequired as matrix size input for ENVI-met\n\n\nzlayers &lt;-max(lad_df$k)\nGets the number of vertical layers\nSets the height resolution of the LAD matrix\n\n\n\n\n\n\nAutomatic Grid Dimensions transformation\nCalculates the voxel grid dimensions in X, Y, and Z from the TLS-derived LAD profile.\nThe table below outlines how the core spatial and structural parameters of the tree model are computed from the input LAD_DF data frame. These derived values define the three-dimensional structure of the tree in terms of its horizontal extent, vertical layering and canopy dimensions.\nData I and data J represent the size of the voxel grid in the i and j dimensions, respectively, based on unique horizontal (x and y) and vertical (height bin) bins in the LAD profile.\n‘Width’ and ‘Depth’ describe the physical spread of the tree crown, inferred from the voxel grid extent if not manually set.\nHeight is computed by multiplying the number of vertical layers (zlayers) by the voxel resolution (cellSize), providing the total modelled height of the canopy.\nThese computed values are essential for correctly normalization and locating the 3D LAD matrix within the ENVI-met simulation domain to ensure visual and physiological realism.\n\n\n\n\n\n\n\n\nCode Line\nMeaning\nReason\n\n\n\n\nWidth  &lt;- if (is.null(Width)) dataI else Width\nUses the number of i-cells if Width is not provided\nAutomatically estimates tree width from voxel spread in x-direction\n\n\nDepth  &lt;- if (is.null(Depth)) dataJ else Depth\nUses the number of j-cells if Depth is not provided\nAutomatically estimates tree depth from voxel spread in y-direction\n\n\nHeight &lt;- zlayers * cellsize\nConverts number of vertical layers to metric height using cellsize\nComputes physical tree height in meters for ENVI-met\n\n\n\n\n# 1. Remove NA values from the LAD column\nlad_df &lt;- lad_df[!is.na(lad_df$LAD_median), ]\n\n# 2. Create discrete i and j indices for the horizontal position\n# (converts x and y coordinates into consecutive index values)\nlad_df$i &lt;- as.integer(factor(lad_df$x))\nlad_df$j &lt;- as.integer(factor(lad_df$y))\n\n# 3. Assign each Height_bin (z direction) a consecutive layer ID k\n# (z_map assigns an index layer to each unique height)\nz_map &lt;- setNames(seq_along(sort(unique(lad_df$Height_bin))), sort(unique(lad_df$Height_bin)))\nlad_df$k &lt;- z_map[as.character(lad_df$Height_bin)]\n\n# 4. Scale LAD values, e.g. to get from 0.02 to more realistic values such as 0.5–1.5\nlad_df$lad_value &lt;- round(lad_df$LAD_median * 1.2, 5)\n\n# 5. Calculate the maximum dimensions of the grid (for XML specifications)\ndataI &lt;- max(lad_df$i) # Width in cells (x-direction)\ndataJ &lt;- max(lad_df$j) # Depth in cells (y-direction)\nzlayers &lt;- max(lad_df$k) # Number of vertical layers (z-direction)\n\n\n\nTransmittance and Albedo\n\nAlbedo = 0.18\nTransmittance = 0.3\n\nAlbedo = 0.18: Albedo is the fraction of incoming solar radiation reflected by the canopy surface. For deciduous trees, values usually range between 0.15 and 0.20. 0.18 is a commonly used default for broadleaved species like Fagus sylvatica or Quercus robur in many ecological models (e.g., ENVI-met, MAESPA). It affects surface energy balance and radiation reflection in ENVI-met simulations.\nTransmittance = 0.3: Transmittance represents the proportion of shortwave radiation that passes through the canopy without being absorbed or reflected. Deciduous trees in full leaf have transmittance values between 0.1 and 0.4 depending on species and LAI. 0.3 reflects moderate canopy density, consistent with empirical observations for mid-summer crowns. It controls how much light reaches the ground and sub-canopy vegetation; affects microclimate and shading.\nBoth values can be adjusted to match field measurements or literature for specific species or leaf phenology. However you can use them as robust fallback defaults when exact species traits are unavailable.\n\n\nSeason-Profile\nDefines monthly LAD normalization.\nSeasonProfile = c(0.2, 0.2, 0.4, 0.7, 1.0, 1.0, 1.0, 0.8, 0.6, 0.3, 0.2, 0.2)\nThe SeasonProfile is a vector of 12 numeric values (one per month) weighting the relative Leaf Area Density (LAD) throughout the year. It models seasonal leaf development and senescence, controlling how much foliage is present in each month:\n\nValues range from 0.0 (no foliage) to 1.0 (full foliage).\nFor deciduous trees like Fagus sylvatica or Quercus robur, foliage develops in spring (April–May), peaks in summer (June–August), and declines in autumn (September–October).\n\nProfile Breakdown:\n\n\n\nMonths\nValue\nInterpretation\n\n\n\n\nJan–Feb, Nov–Dec\n0.2\nDormant / leafless\n\n\nMarch\n0.4\nBudburst begins\n\n\nApril\n0.7\nLeaf expansion\n\n\nMay–July\n1.0\nFull canopy\n\n\nAugust\n0.8\nLeaf maturity decline\n\n\nSeptember\n0.6\nSenescence onset\n\n\nOctober\n0.3\nStrong senescence\n\n\n\nThe SeasonProfile directly influences LAD in ENVI-met’s dynamic vegetation simulation — affecting transpiration, shading, and energy balance across the simulation year. Adjusting this vector allows tailoring of phenology to site-specific or species-specific data.\n\n\nL-SystemBased trees in ENVI-met (Experimetal)\nENVI-met optionally allows procedural generation of tree architecture using Lindenmayer Systems (L-Systems) — a formal grammar originally used to simulate plant growth patterns. When L-SystemBased = 1, the geometry of the tree is not derived from a static LAD matrix alone, but supplemented or replaced by rule-based 3D branching structures which supplement or replace the matrix. This is independent of the LAD profile but may affect shading and visualisation in the Albero interface of ENVI-met.\nL-SystemBased = 1\nAxiom = \"F(2)V\\V\\\\V/////B\"\nIterationDepth = 3\n\nExplanation of Key Parameters\n\n\n\n\n\n\n\nParameter\nMeaning\n\n\n\n\nL-SystemBased\nIf 1, enables L-system generation (uses rules to grow plant structure)\n\n\nAxiom\nStarting string (“seed”) for the L-system; defines base growth\n\n\nIterationDepth\nHow many times to apply production rules; higher means more detail\n\n\nTermLString\nOptional: Final symbol to be drawn/rendered (e.g. “L”)\n\n\nApplyTermLString\nIf 1, interprets the TermLString; otherwise, renders entire string\n\n\n\n\n\nDefault Settings\n\n\n\nL-System Branching as implemented by default\n\n\n&lt;L-SystemBased&gt;1&lt;/L-SystemBased&gt;\n&lt;Axiom&gt;F(2)V\\V\\\\V/////B&lt;/Axiom&gt;\n&lt;IterationDepth&gt;3&lt;/IterationDepth&gt;\n&lt;TermLString&gt;L&lt;/TermLString&gt;\n&lt;ApplyTermLString&gt;1&lt;/ApplyTermLString&gt;\n\nF(2): Move forward with length 2 (main trunk)\nV\\\\V/////B: Branching pattern with rotations (backslashes and slashes encode rotation commands); B may denote a terminal leaf or bud\nIterationDepth = 3: The production rules (if defined) will be applied 3 times to this axiom, generating a fractal-like tree structure.\n\n\nNote: In ENVI-met, the actual grammar rules are hard-coded and not customizable in .pld — only the axiom and iteration depth are user-defined. It is highly experimental and poorly documented\n\nUse L-SystemBased = 1 if:\n\nYou want visual structure added to otherwise sparse or low-resolution LAD matrices\nThe tree lacks realistic shape (for Albero visualization)\nUse L-SystemBased = 0 (default) if:\n\nYou already provide a dense voxel-based LAD (from TLS or similar)\nYou want strict control over the 3D structure via LAD profile only\n\n\n#| eval: false\n\n# --- Export final profile as Envi-met PLANT3D tree --------------------------\nexport_lad_to_envimet_p3d(\n  lad_df = lad_df,\n  ID = \"120312\",\n  Description = \"Fagus sylvatica TLS\",\n  AlternativeName = \"Fagus sylvatica\",\n  Albedo = 0.17,\n  Width = NULL,              # auto-detected\n  Depth = NULL,              # auto-detected\n  RootDiameter = 5.0,\n  cellsize = 1,\n  Transmittance = 0.3,\n  SeasonProfile = c(0.3, 0.3, 0.4, 0.6, 0.9, 1, 1, 1, 0.7, 0.4, 0.3, 0.3),\n  BlossomProfile = c(0, 0, 0.7, 0.1, 0, 0, 0, 0, 0, 0, 0, 0),\n  LSystem = TRUE,\n  scale_factor = 3,\n  file_out = output_envimet_tls_3d\n)\n\n#rstudioapi::navigateToFile(output_envimet_tls_3d)\n\n\nImport TLS-based .pld into ENVI-met via Albero Clipboard\nRequirements\n- ENVI-met 5.8+\n- .pld file (e.g. oak_tls_envimet.pld)\n- Albero editor (via Leonardo)\nSteps\n1. Open Albero\n→ Leonardo → Database → Plant Database\n2. Open Clipboard\n→ Click Clipboard (top-right)\n3. Import .pld\n→ Clipboard → Import → Load file\n4. Edit (optional)\n→ Adjust LAD, albedo, transmittance, name, etc.\n5. Send to Library\n→ Click “Send to Library”\n6. Use in ENVI-met\n→ In Leonardo/Spaces assign plant to your 3D model\nNotes\n- .pld contains LAD(z) values (m²/m³)\n- Use Advanced Settings to fine-tune visualization\n- Custom plants stored in your personal Albero library\n\n\n\nKey Benefits\n\nEfficient and scalable: The method avoids destructive sampling by using TLS return counts as proxies for leaf density. This makes it suitable for large-scale or repeated surveys without the need for time-consuming ground calibration.\nCaptures structural patterns: Normalizing the LAD values retains the vertical and spatial structure of vegetation, enabling meaningful comparison of crown shape, canopy layering, and vegetation density across space or time.\nDirectly usable in ENVI-met: The output is structured as a raster stack with height-specific layers, aligning with the input requirements of ENVI-met’s SimplePlant or 3D vegetation modules. This enables seamless integration into microclimate simulations.\n\n\n\nLimitations\n\nSimplified assumptions: The linear mapping of TLS returns to LAD assumes a proportional relationship, which simplifies the complex interaction between laser pulses and vegetation surfaces.\nScan geometry dependency: Occlusion, scan angle, and varying point densities can distort the return distribution, especially in dense or multi-layered vegetation.\nGeneric LAD normalization: The maximum LAD value used for normalization is taken from literature-based estimates rather than site-specific measurements, which can introduce bias in absolute LAD magnitudes.\n\n\n\nConclusion\nThis workflow offers a robust and accessible approach for analyzing vegetation structure and generating model-ready LAD profiles from TLS data. It is especially useful for relative comparisons and ecological modeling, but is not intended for absolute LAD quantification without additional calibration.\n\n\nReferences\n\nCalders et al. (2015). Nondestructive biomass estimation via TLS. Methods Ecol Evol, 6:198–208.https://doi.org/10.1111/2041-210X.12301\nChen et al. (2018): Estimation of LAI in open-canopy forests using TLS and path length models. Agric. For. Meteorol. 263, 323–333. https://doi.org/10.1016/j.agrformet.2018.09.006\nENVI-met PLANT3D specification: https://www.envi-met.net/documents/papers/overview30.pdf\nENVI-met Albero overview: https://envi-met.com/tutorials/albero-overview\nENVI-met KB – Obtaining Leaf Area Density: https://envi-met.info/doku.php?id=kb:lad#obtaining_leaf_area_density_data\nENVI-met dbmanager documentation: https://envi-met.info/doku.php?id=apps:dbmanager:start\nENVI-met Vegetation Tutorial (YouTube): https://www.youtube.com/watch?v=KGRLnXAXZds\nFlynn et al. (2023) – TLS-based vegetation index estimation; compares methods and highlights complexities in Mediterranean forest. Biogeosciences, 20(13), 2769–2784. doi:10.5194/bg-20-2769-2023\nHosoi & Omasa (2006). Voxel-based 3D tree modeling. IEEE TGRS, 44(12), 3610–3618. https://doi.org/10.1109/TGRS.2006.881743\nPrusinkiewicz & Lindenmayer (1990). The Algorithmic Beauty of Plants. Springer. https://doi.org/10.1007/978-1-4613-8476-2\nOshio & Asawa (2016). Solar transmittance of urban trees. IEEE TGRS, 54(9), 5483–5492. https://doi.org/10.1109/TGRS.2016.2565699\nSimon, Sinsel & Bruse (2020). Fractal trees in ENVI-met. Forests, 11(8), 869. https://doi.org/10.3390/f11080869\nWilkes et al. (2017). TLS acquisition strategies. Remote Sens Environ, 196, 140–153. https://doi.org/10.1016/j.rse.2017.04.030\nChen et al. (2018). LAI from TLS. Agr Forest Meteorol, 263, 323–333. https://doi.org/10.1016/j.agrformet.2018.09.006\nYin et al. (2019). Shading and thermal comfort. Sustainability, 11(5), 1355. https://doi.org/10.3390/su11051355\nZhang (2024). Green layouts in ENVI-met. Informatica, 48(23). https://doi.org/10.31449/inf.v48i23.6881 Certainly. Here’s the reference adapted to match your current compact style:",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#envi-met-3d-tree-export",
    "href": "doc/tls_v1_2.html#envi-met-3d-tree-export",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "ENVI-met 3D Tree Export",
    "text": "ENVI-met 3D Tree Export\nThe next section describes more detailed how the key input values in the R function export_lad_to_envimet3d() are computed, derived or selected, and provides the rationale for each. The function converts a voxel-based Leaf Area Density (LAD) profile, typically obtained from Terrestrial Laser Scanning (TLS) data, into a structured XML file compatible with ENVI-met’s 3D tree model (.pld or PLANT3D).\nGiven the sensitivity of ENVI-met simulations to tree morphology and LAD distribution, the function ensures that the spatial dimensions, vertical layering and LAD intensity values are all correctly represented. Some parameters are optional, but can be derived from the data if not explicitly set.\nThe table below details each argument of the function, including its purpose, how it is determined and its necessity.\n\n\n\n\n\n\n\n\nCode Line\nMeaning\nReason\n\n\n\n\nlad_df &lt;-lad_df[!is.na(lad_df$LAD_median), ]\nRemoves entries with missing LAD values\nEnsures only valid data is used in the LAD calculation and XML export\n\n\nlad_df$i &lt;-as.integer(factor(lad_df$x))\nConverts x-coordinates to integer voxel column indices (i)\nRequired for ENVI-met LAD matrix indexing\n\n\nlad_df$j &lt;-as.integer(factor(lad_df$y))\nConverts y-coordinates to integer voxel row indices (j)\nSame as above, for the y-direction\n\n\nz_map &lt;-setNames( ...)\nMaps unique height bins to sequential vertical indices (k)\nTranslates height levels into voxel layers compatible with ENVI-met\n\n\nlad_df$k &lt;-z_map[as.character(lad_df$Height_bin)]\nApplies the vertical index to the LAD data\nAligns LAD values with ENVI-met vertical layer system\n\n\nlad_df$lad_value &lt;-round(lad_df$LAD_median * scale_factor, 5)\nScales LAD values and rounds to 5 digits\nBrings LAD values to a usable range for ENVI-met and ensures precision\n\n\ndataI &lt;-max(lad_df$i)\nGets the number of horizontal grid cells in i-direction (width)\nRequired as matrix size input for ENVI-met\n\n\ndataJ &lt;-max(lad_df$j)\nGets the number of horizontal grid cells in j-direction (depth)\nRequired as matrix size input for ENVI-met\n\n\nzlayers &lt;-max(lad_df$k)\nGets the number of vertical layers\nSets the height resolution of the LAD matrix",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#automatic-grid-dimensions-transformation",
    "href": "doc/tls_v1_2.html#automatic-grid-dimensions-transformation",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Automatic Grid Dimensions transformation",
    "text": "Automatic Grid Dimensions transformation\nCalculates the voxel grid dimensions in X, Y, and Z from the TLS-derived LAD profile.\nThe table below outlines how the core spatial and structural parameters of the tree model are computed from the input LAD_DF data frame. These derived values define the three-dimensional structure of the tree in terms of its horizontal extent, vertical layering and canopy dimensions.\nData I and data J represent the size of the voxel grid in the i and j dimensions, respectively, based on unique horizontal (x and y) and vertical (height bin) bins in the LAD profile.\n‘Width’ and ‘Depth’ describe the physical spread of the tree crown, inferred from the voxel grid extent if not manually set.\nHeight is computed by multiplying the number of vertical layers (zlayers) by the voxel resolution (cellSize), providing the total modelled height of the canopy.\nThese computed values are essential for correctly normalization and locating the 3D LAD matrix within the ENVI-met simulation domain to ensure visual and physiological realism.\n\n\n\n\n\n\n\n\nCode Line\nMeaning\nReason\n\n\n\n\nWidth  &lt;- if (is.null(Width)) dataI else Width\nUses the number of i-cells if Width is not provided\nAutomatically estimates tree width from voxel spread in x-direction\n\n\nDepth  &lt;- if (is.null(Depth)) dataJ else Depth\nUses the number of j-cells if Depth is not provided\nAutomatically estimates tree depth from voxel spread in y-direction\n\n\nHeight &lt;- zlayers * cellsize\nConverts number of vertical layers to metric height using cellsize\nComputes physical tree height in meters for ENVI-met\n\n\n\n\n# 1. Remove NA values from the LAD column\nlad_df &lt;- lad_df[!is.na(lad_df$LAD_median), ]\n\n# 2. Create discrete i and j indices for the horizontal position\n# (converts x and y coordinates into consecutive index values)\nlad_df$i &lt;- as.integer(factor(lad_df$x))\nlad_df$j &lt;- as.integer(factor(lad_df$y))\n\n# 3. Assign each Height_bin (z direction) a consecutive layer ID k\n# (z_map assigns an index layer to each unique height)\nz_map &lt;- setNames(seq_along(sort(unique(lad_df$Height_bin))), sort(unique(lad_df$Height_bin)))\nlad_df$k &lt;- z_map[as.character(lad_df$Height_bin)]\n\n# 4. Scale LAD values, e.g. to get from 0.02 to more realistic values such as 0.5–1.5\nlad_df$lad_value &lt;- round(lad_df$LAD_median * 1.2, 5)\n\n# 5. Calculate the maximum dimensions of the grid (for XML specifications)\ndataI &lt;- max(lad_df$i) # Width in cells (x-direction)\ndataJ &lt;- max(lad_df$j) # Depth in cells (y-direction)\nzlayers &lt;- max(lad_df$k) # Number of vertical layers (z-direction)",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#transmittance-and-albedo",
    "href": "doc/tls_v1_2.html#transmittance-and-albedo",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Transmittance and Albedo",
    "text": "Transmittance and Albedo\n\nAlbedo = 0.18\nTransmittance = 0.3\n\nAlbedo = 0.18: Albedo is the fraction of incoming solar radiation reflected by the canopy surface. For deciduous trees, values usually range between 0.15 and 0.20. 0.18 is a commonly used default for broadleaved species like Fagus sylvatica or Quercus robur in many ecological models (e.g., ENVI-met, MAESPA). It affects surface energy balance and radiation reflection in ENVI-met simulations.\nTransmittance = 0.3: Transmittance represents the proportion of shortwave radiation that passes through the canopy without being absorbed or reflected. Deciduous trees in full leaf have transmittance values between 0.1 and 0.4 depending on species and LAI. 0.3 reflects moderate canopy density, consistent with empirical observations for mid-summer crowns. It controls how much light reaches the ground and sub-canopy vegetation; affects microclimate and shading.\nBoth values can be adjusted to match field measurements or literature for specific species or leaf phenology. However you can use them as robust fallback defaults when exact species traits are unavailable.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#season-profile",
    "href": "doc/tls_v1_2.html#season-profile",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Season-Profile",
    "text": "Season-Profile\nDefines monthly LAD normalization.\nSeasonProfile = c(0.2, 0.2, 0.4, 0.7, 1.0, 1.0, 1.0, 0.8, 0.6, 0.3, 0.2, 0.2)\nThe SeasonProfile is a vector of 12 numeric values (one per month) weighting the relative Leaf Area Density (LAD) throughout the year. It models seasonal leaf development and senescence, controlling how much foliage is present in each month:\n\nValues range from 0.0 (no foliage) to 1.0 (full foliage).\nFor deciduous trees like Fagus sylvatica or Quercus robur, foliage develops in spring (April–May), peaks in summer (June–August), and declines in autumn (September–October).\n\nProfile Breakdown:\n\n\n\nMonths\nValue\nInterpretation\n\n\n\n\nJan–Feb, Nov–Dec\n0.2\nDormant / leafless\n\n\nMarch\n0.4\nBudburst begins\n\n\nApril\n0.7\nLeaf expansion\n\n\nMay–July\n1.0\nFull canopy\n\n\nAugust\n0.8\nLeaf maturity decline\n\n\nSeptember\n0.6\nSenescence onset\n\n\nOctober\n0.3\nStrong senescence\n\n\n\nThe SeasonProfile directly influences LAD in ENVI-met’s dynamic vegetation simulation — affecting transpiration, shading, and energy balance across the simulation year. Adjusting this vector allows tailoring of phenology to site-specific or species-specific data.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#l-systembased-trees-in-envi-met-experimetal",
    "href": "doc/tls_v1_2.html#l-systembased-trees-in-envi-met-experimetal",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "L-SystemBased trees in ENVI-met (Experimetal)",
    "text": "L-SystemBased trees in ENVI-met (Experimetal)\nENVI-met optionally allows procedural generation of tree architecture using Lindenmayer Systems (L-Systems) — a formal grammar originally used to simulate plant growth patterns. When L-SystemBased = 1, the geometry of the tree is not derived from a static LAD matrix alone, but supplemented or replaced by rule-based 3D branching structures which supplement or replace the matrix. This is independent of the LAD profile but may affect shading and visualisation in the Albero interface of ENVI-met.\nL-SystemBased = 1\nAxiom = \"F(2)V\\V\\\\V/////B\"\nIterationDepth = 3\n\nExplanation of Key Parameters\n\n\n\n\n\n\n\nParameter\nMeaning\n\n\n\n\nL-SystemBased\nIf 1, enables L-system generation (uses rules to grow plant structure)\n\n\nAxiom\nStarting string (“seed”) for the L-system; defines base growth\n\n\nIterationDepth\nHow many times to apply production rules; higher means more detail\n\n\nTermLString\nOptional: Final symbol to be drawn/rendered (e.g. “L”)\n\n\nApplyTermLString\nIf 1, interprets the TermLString; otherwise, renders entire string\n\n\n\n\n\nDefault Settings\n\n\n\nL-System Branching as implemented by default\n\n\n&lt;L-SystemBased&gt;1&lt;/L-SystemBased&gt;\n&lt;Axiom&gt;F(2)V\\V\\\\V/////B&lt;/Axiom&gt;\n&lt;IterationDepth&gt;3&lt;/IterationDepth&gt;\n&lt;TermLString&gt;L&lt;/TermLString&gt;\n&lt;ApplyTermLString&gt;1&lt;/ApplyTermLString&gt;\n\nF(2): Move forward with length 2 (main trunk)\nV\\\\V/////B: Branching pattern with rotations (backslashes and slashes encode rotation commands); B may denote a terminal leaf or bud\nIterationDepth = 3: The production rules (if defined) will be applied 3 times to this axiom, generating a fractal-like tree structure.\n\n\nNote: In ENVI-met, the actual grammar rules are hard-coded and not customizable in .pld — only the axiom and iteration depth are user-defined. It is highly experimental and poorly documented\n\nUse L-SystemBased = 1 if:\n\nYou want visual structure added to otherwise sparse or low-resolution LAD matrices\nThe tree lacks realistic shape (for Albero visualization)\nUse L-SystemBased = 0 (default) if:\n\nYou already provide a dense voxel-based LAD (from TLS or similar)\nYou want strict control over the 3D structure via LAD profile only\n\n\n#| eval: false\n\n# --- Export final profile as Envi-met PLANT3D tree --------------------------\nexport_lad_to_envimet_p3d(\n  lad_df = lad_df,\n  ID = \"120312\",\n  Description = \"Fagus sylvatica TLS\",\n  AlternativeName = \"Fagus sylvatica\",\n  Albedo = 0.17,\n  Width = NULL,              # auto-detected\n  Depth = NULL,              # auto-detected\n  RootDiameter = 5.0,\n  cellsize = 1,\n  Transmittance = 0.3,\n  SeasonProfile = c(0.3, 0.3, 0.4, 0.6, 0.9, 1, 1, 1, 0.7, 0.4, 0.3, 0.3),\n  BlossomProfile = c(0, 0, 0.7, 0.1, 0, 0, 0, 0, 0, 0, 0, 0),\n  LSystem = TRUE,\n  scale_factor = 3,\n  file_out = output_envimet_tls_3d\n)\n\n#rstudioapi::navigateToFile(output_envimet_tls_3d)\n\n\nImport TLS-based .pld into ENVI-met via Albero Clipboard\nRequirements\n- ENVI-met 5.8+\n- .pld file (e.g. oak_tls_envimet.pld)\n- Albero editor (via Leonardo)\nSteps\n1. Open Albero\n→ Leonardo → Database → Plant Database\n2. Open Clipboard\n→ Click Clipboard (top-right)\n3. Import .pld\n→ Clipboard → Import → Load file\n4. Edit (optional)\n→ Adjust LAD, albedo, transmittance, name, etc.\n5. Send to Library\n→ Click “Send to Library”\n6. Use in ENVI-met\n→ In Leonardo/Spaces assign plant to your 3D model\nNotes\n- .pld contains LAD(z) values (m²/m³)\n- Use Advanced Settings to fine-tune visualization\n- Custom plants stored in your personal Albero library",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#key-benefits",
    "href": "doc/tls_v1_2.html#key-benefits",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Key Benefits",
    "text": "Key Benefits\n\nEfficient and scalable: The method avoids destructive sampling by using TLS return counts as proxies for leaf density. This makes it suitable for large-scale or repeated surveys without the need for time-consuming ground calibration.\nCaptures structural patterns: Normalizing the LAD values retains the vertical and spatial structure of vegetation, enabling meaningful comparison of crown shape, canopy layering, and vegetation density across space or time.\nDirectly usable in ENVI-met: The output is structured as a raster stack with height-specific layers, aligning with the input requirements of ENVI-met’s SimplePlant or 3D vegetation modules. This enables seamless integration into microclimate simulations.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#limitations",
    "href": "doc/tls_v1_2.html#limitations",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Limitations",
    "text": "Limitations\n\nSimplified assumptions: The linear mapping of TLS returns to LAD assumes a proportional relationship, which simplifies the complex interaction between laser pulses and vegetation surfaces.\nScan geometry dependency: Occlusion, scan angle, and varying point densities can distort the return distribution, especially in dense or multi-layered vegetation.\nGeneric LAD normalization: The maximum LAD value used for normalization is taken from literature-based estimates rather than site-specific measurements, which can introduce bias in absolute LAD magnitudes.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#conclusion",
    "href": "doc/tls_v1_2.html#conclusion",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Conclusion",
    "text": "Conclusion\nThis workflow offers a robust and accessible approach for analyzing vegetation structure and generating model-ready LAD profiles from TLS data. It is especially useful for relative comparisons and ecological modeling, but is not intended for absolute LAD quantification without additional calibration.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#references",
    "href": "doc/tls_v1_2.html#references",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "References",
    "text": "References\n\nCalders et al. (2015). Nondestructive biomass estimation via TLS. Methods Ecol Evol, 6:198–208.https://doi.org/10.1111/2041-210X.12301\nChen et al. (2018): Estimation of LAI in open-canopy forests using TLS and path length models. Agric. For. Meteorol. 263, 323–333. https://doi.org/10.1016/j.agrformet.2018.09.006\nENVI-met PLANT3D specification: https://www.envi-met.net/documents/papers/overview30.pdf\nENVI-met Albero overview: https://envi-met.com/tutorials/albero-overview\nENVI-met KB – Obtaining Leaf Area Density: https://envi-met.info/doku.php?id=kb:lad#obtaining_leaf_area_density_data\nENVI-met dbmanager documentation: https://envi-met.info/doku.php?id=apps:dbmanager:start\nENVI-met Vegetation Tutorial (YouTube): https://www.youtube.com/watch?v=KGRLnXAXZds\nFlynn et al. (2023) – TLS-based vegetation index estimation; compares methods and highlights complexities in Mediterranean forest. Biogeosciences, 20(13), 2769–2784. doi:10.5194/bg-20-2769-2023\nHosoi & Omasa (2006). Voxel-based 3D tree modeling. IEEE TGRS, 44(12), 3610–3618. https://doi.org/10.1109/TGRS.2006.881743\nPrusinkiewicz & Lindenmayer (1990). The Algorithmic Beauty of Plants. Springer. https://doi.org/10.1007/978-1-4613-8476-2\nOshio & Asawa (2016). Solar transmittance of urban trees. IEEE TGRS, 54(9), 5483–5492. https://doi.org/10.1109/TGRS.2016.2565699\nSimon, Sinsel & Bruse (2020). Fractal trees in ENVI-met. Forests, 11(8), 869. https://doi.org/10.3390/f11080869\nWilkes et al. (2017). TLS acquisition strategies. Remote Sens Environ, 196, 140–153. https://doi.org/10.1016/j.rse.2017.04.030\nChen et al. (2018). LAI from TLS. Agr Forest Meteorol, 263, 323–333. https://doi.org/10.1016/j.agrformet.2018.09.006\nYin et al. (2019). Shading and thermal comfort. Sustainability, 11(5), 1355. https://doi.org/10.3390/su11051355\nZhang (2024). Green layouts in ENVI-met. Informatica, 48(23). https://doi.org/10.31449/inf.v48i23.6881 Certainly. Here’s the reference adapted to match your current compact style:",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "modeling/script-00-interpolation.html",
    "href": "modeling/script-00-interpolation.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Script: Spatial Interpolation\n\n#------------------------------------------------------------------------------\n# Name: FR_soilmoist.R\n# Type: control script \n# Author: Chris Reudenbach, creuden@gmail.com\n# Description:  calculates the soil moisture from Lacanau point data\n# Copyright:GPL (&gt;= 3) \n# Date: 2022-11-10 \n# V-2022-11-12; \n#------------------------------------------------------------------------------\n# 0 - project setup\n#------------------------------------------------------------------------------\n# geoAI course basic setup\n# Type: script\n# Name: geoAI_setup.R\n# Author: Chris Reudenbach, creuden@gmail.com\n# Description:  create/read project folder structure and returns pathes as list\n#               load all necessary packages \n#               sources all functions in a defined function folder\n# Dependencies:   \n# Output: list containing the folder strings as shortcuts\n# Copyright: Chris Reudenbach, thomas Nauss 2019-2021, GPL (&gt;= 3)\n# git clone https://github.com/gisma-courses/geoAI-scripts.git\n#------------------------------------------------------------------------------\n\n\n\n# basic packages\nlibrary(\"mapview\")\nlibrary(\"tmap\")\nlibrary(\"tmaptools\")\nlibrary(\"raster\")\nlibrary(\"terra\")\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"lidR\")\nlibrary(\"future\")\nlibrary(\"lwgeom\")\nlibrary(\"tmap\")\nlibrary(\"mapview\")\nlibrary(rprojroot)\n\nroot_folder = find_rstudio_root_file()\n#root_folder = getwd()\nndvi.col = function(n) {\n  rev(colorspace::sequential_hcl(n, \"Green-Yellow\"))\n}\n\nano.col = colorspace::diverging_hcl(7, palette = \"Red-Green\",  register = \"rg\")\n\n\n\n\n# # suppres gdal warnings\n# rgdal::set_thin_PROJ6_warnings(TRUE)\n# \n# \n# \n# # workaround subfolder\n# loc_name = \"harz\"\n# \n# # harz\n# epsg=25833\n# \n# attributename = c(\"Moisture_1_17Nov\",\"Moisture_2_17Nov\",\"Moisture_1_19Nov\",\"Moisture_2_19Nov\")\n# varname = c(\"soilmoist2022_08_17\",\"soilmoist2022_08_19\")\n# fnDTM = \"DTM_v3.vrt\"\n# fnsm_data = \"lacanau_moisture_measurements.csv\"\n# fnpos_data= \"ltrees.gpkg\"\n# \n# # read DTM\n# DTM = terra::rast(fnDTM)\n# # cast to SpatialPixelsDataFrame\n# DTM.spdf &lt;- as(raster(DTM),\n#                        'SpatialPixelsDataFrame')\n# colnames(DTM.spdf@data) &lt;- \"altitude\"\n# # read moist data \n# sm=read.csv2(fnsm_data,sep = \",\")\n# # read tree data\n# pos=st_read(fnpos_data)\n# # merge\n# sm$Point = paste0(\"TREE\",str_split_fixed(sm$TargetID, \"_\", 3)[,3])\n# m=merge(pos,sm)\n# \n# # extract altitudes for positions\n# em= exactextractr::exact_extract(DTM,st_buffer(m,1),\"mean\")\n# m$altitude=em\n# \n# # start kriging \n# for (i in 1:length(varname) ){\n#   z=i*2\n#   # mean\n#   m$var = (as.numeric(m[[attributename[z-1]]]) + as.numeric(m[[attributename[z]]]))/2\n#   # to sp\n#   m2 = as(m,\"Spatial\")    \n#   tm2 = spTransform(m2,\n#                     crs(\"+proj=utm +zone=30 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\"))\n# \n#   # autofit variogramm for kriging \n#   vm.auto = automap::autofitVariogram(formula = as.formula(paste(\"var\", \"~ 1\")),\n#                                       input_data = tm2)\n#   plot(vm.auto)\n#   \n#   # kriging   \n#   print(paste0(\"kriging \", varname[i]))\n#   var.pred &lt;- gstat::krige(formula = as.formula(paste(\"var\", \"~ altitude\")),\n#                            locations = tm2,\n#                            newdata = DTM.spdf,\n#                            model = vm.auto$var_model,\n#                            debug.level=0,)\n#   \n#   r=rasterFromXYZ(as.data.frame(var.pred)[, c(\"x\", \"y\", \"var1.pred\")])\n#   \n#   # reclassify erratic values \n#   reclass_df &lt;- c(-Inf, 0, NA)\n#   # reshape the object into a matrix with columns and rows\n#   reclass_m &lt;- matrix(reclass_df,\n#                       ncol = 3,\n#                       byrow = TRUE)\n#   r_c &lt;- reclassify(r,reclass_m)\n# \n#   plot(r_c)\n#   # re assign crs\n#   crs(r_c) = crs(paste0(\"EPSG:\",epsg))\n#   raster::writeRaster(r_c,paste0(\"data/gis/France_Lacanau_PP_Gis/data_lev0/\",varname[i],\".tif\"),overwrite=TRUE)\n#   \n# }"
  },
  {
    "objectID": "modeling/qgis-tutorials.html",
    "href": "modeling/qgis-tutorials.html",
    "title": "Data and Software",
    "section": "",
    "text": "Please find all Data Downloads at theCourse Data Server Data folder for any file exchange and data related purposes."
  },
  {
    "objectID": "modeling/qgis-tutorials.html#data-set-for-training-purposes",
    "href": "modeling/qgis-tutorials.html#data-set-for-training-purposes",
    "title": "Data and Software",
    "section": "",
    "text": "Please find all Data Downloads at theCourse Data Server Data folder for any file exchange and data related purposes."
  },
  {
    "objectID": "modeling/qgis-tutorials.html#specific-modeling-software",
    "href": "modeling/qgis-tutorials.html#specific-modeling-software",
    "title": "Data and Software",
    "section": "Specific modeling software",
    "text": "Specific modeling software\nPlease find all Downloads according to ENVI-met at the ENVI-met landing page"
  },
  {
    "objectID": "modeling/qgis-tutorials.html#common-software",
    "href": "modeling/qgis-tutorials.html#common-software",
    "title": "Data and Software",
    "section": "Common Software",
    "text": "Common Software\nShell — any command line environment will do for the exercises. For Linux we recommend the bash shell. For Windows the Windows command line can be used.\n\nQGIS has become one of the most promising and most integrative open source GIS systems over the last years. Through the processing plugin, it additionally integrates modules from the other leading free GIS solutions. We will need it (if necessary) to prepare or manipulate some of the data.\n\nRegarding installation, for Ubuntu Linux, the Ubuntu GIS package is a good choice. For Windows, we strongly recommend installing everything via the OSGeo4W environment and not the standalone QGIS installation tool."
  },
  {
    "objectID": "modeling/qgis-tutorials.html#additional-data-sources",
    "href": "modeling/qgis-tutorials.html#additional-data-sources",
    "title": "Data and Software",
    "section": "Additional data sources",
    "text": "Additional data sources"
  },
  {
    "objectID": "base/faq.html",
    "href": "base/faq.html",
    "title": "Frequently asked Questions",
    "section": "",
    "text": "This is a senseless question to meet a meaningfull answer\n\n\n\n\n\n\n\n\n\nThis is a meaningful answer to a senseless question\n\n\n\n\n\n\n\n\n\nLearn More…\n\n\n\n\n\nThis is a even more meaningful answer to a senseless question"
  },
  {
    "objectID": "base/faq.html#make-sense-topic",
    "href": "base/faq.html#make-sense-topic",
    "title": "Frequently asked Questions",
    "section": "",
    "text": "This is a senseless question to meet a meaningfull answer\n\n\n\n\n\n\n\n\n\nThis is a meaningful answer to a senseless question\n\n\n\n\n\n\n\n\n\nLearn More…\n\n\n\n\n\nThis is a even more meaningful answer to a senseless question"
  },
  {
    "objectID": "assessment/slidelist.html",
    "href": "assessment/slidelist.html",
    "title": "Assessments",
    "section": "",
    "text": "Basic Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit, GitHub & Rstudio [DE]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit, GitHub & Rstudio [EN]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "doc/helper_functions.html",
    "href": "doc/helper_functions.html",
    "title": "Helper Functions for Microclimate Predictor Stack",
    "section": "",
    "text": "1 Introduction\nThis document explains the custom helper functions used in the microclimate_predictor_stack.R script for preprocessing and analyzing LiDAR data in R. The functions support pixel-level metrics computation, raster template creation, VRT mosaicking, and tree hull extraction.\n\n\n\n2 .stdmetrics()\n#' @title .stdmetrics\n#' @description Berechnet Standardmetriken für LiDAR Rasterzellen\n.stdmetrics &lt;- function(z, i, ...) {\n  return(list(\n    zmax = max(z, na.rm = TRUE),            # Maximum height\n    zmean = mean(z, na.rm = TRUE),          # Mean height\n    zsd = sd(z, na.rm = TRUE),              # Standard deviation of heights\n    zkurto = moments::kurtosis(z, na.rm = TRUE), # Kurtosis (peakedness of distribution)\n    zskew = moments::skewness(z, na.rm = TRUE),  # Skewness (asymmetry)\n    zq25 = quantile(z, 0.25, na.rm = TRUE), # 25th percentile\n    zq50 = quantile(z, 0.5, na.rm = TRUE),  # Median height\n    zq75 = quantile(z, 0.75, na.rm = TRUE), # 75th percentile\n    zpulse = length(z)                      # Number of returns (pulse count)\n  ))\n}\nUsed to derive standard height-based metrics from LiDAR returns per raster cell using pixel_metrics().\n\n\n\n3 get_vrt_img()\n#' @title get_vrt_img\n#' @description Creates a VRT from multiple GeoTIFF files in a directory\nget_vrt_img &lt;- function(name, path, pattern) {\n  tifs &lt;- list.files(path = path, pattern = paste0(pattern, \".tif$\"), full.names = TRUE)\n  vrt &lt;- file.path(path, paste0(name, \".vrt\"))\n  if (file.exists(vrt)) file.remove(vrt)\n  gdal_utils(util = \"buildvrt\", source = tifs, destination = vrt)\n  return(vrt)\n}\nUsed to dynamically generate a VRT (virtual raster stack) from multiple .tif files with a matching pattern, e.g. \"lad_metrics\".\n\n\n\n4 tree_fn()\n#' @title tree_fn\n#' @description Creates convex hulls from segmented trees in LAS catalogs\ntree_fn &lt;- function(las, ...) {\n  if (is.empty(las)) return(NULL)                   # Skip if empty\n  las &lt;- filter_poi(las, !is.na(treeID))            # Keep only trees\n  if (npoints(las) == 0) return(NULL)               # Skip if no points\n  dt &lt;- data.table::as.data.table(las@data)\n  dt &lt;- dt[, .(X = mean(X), Y = mean(Y)), by = treeID]  # Mean location per tree\n  points_sf &lt;- st_as_sf(dt, coords = c(\"X\", \"Y\"), crs = sf::st_crs(las))\n  hulls &lt;- st_convex_hull(st_union(points_sf))      # Create unified convex hull\n  return(hulls)\n}\nUsed with catalog_apply() to derive convex hull geometries from segmented tree point clouds.\n\n\n\n5 template_raster()\n#' @title template_raster\n#' @description Creates an empty raster template based on bounding box and resolution\ntemplate_raster &lt;- function(bbox, crs, res = 1.0) {\n  if (inherits(bbox, \"sf\")) bbox &lt;- st_bbox(bbox)\n  r &lt;- terra::rast(xmin = bbox[\"xmin\"], xmax = bbox[\"xmax\"],\n                   ymin = bbox[\"ymin\"], ymax = bbox[\"ymax\"],\n                   resolution = res, crs = crs)\n  return(r)\n}\nGenerates a blank terra::rast object for rasterizing vector geometries such as LAD polygons or tree hulls."
  },
  {
    "objectID": "doc/microclimate_predictor_stack_commented.html",
    "href": "doc/microclimate_predictor_stack_commented.html",
    "title": "Microclimate Predictor Stack Tutorial",
    "section": "",
    "text": "1 Introduction\nThis tutorial documents the modular processing chain for deriving microclimate-relevant predictors from ALS (Airborne Laser Scanning) data.\nIt is based on the script 20_microclimate_predictor_stack.R, which builds a raster predictor stack used in microclimate or ecological modeling.\n\n\n\n2 1. Overall Workflow Diagram\n\n\n\n\n\nflowchart TD\n    LAS[\"LAS Input Data\"]\n    DEM[\"Normalize & Create DEM/DSM/CHM\"]\n    PM[\"Pixel-Level Metrics\"]\n    SEG[\"Tree Segmentation\"]\n\n    TOPO[\"Topographic Variables\"]\n    VOX[\"Voxel Metrics: VCI, LAD, Entropy\"]\n    LAD[\"LAD Profiles\"]\n    CLU[\"Tree Cluster Analysis\"]\n\n    MERGE[\"Merge: Predictor Stack\"]\n    OUT[\"Final Raster Predictor Stack\"]\n\n    LAS --&gt; DEM\n    LAS --&gt; PM\n    LAS --&gt; SEG\n\n    DEM --&gt; TOPO\n    PM --&gt; VOX\n    SEG --&gt; LAD\n    LAD --&gt; CLU\n\n    TOPO --&gt; MERGE\n    VOX --&gt; MERGE\n    CLU --&gt; MERGE\n\n    MERGE --&gt; OUT\n\n\n\n\n\n\nThis diagram shows the data flow:\n\nThe LAS file is used in 3 parallel branches.\nTopographic, voxel, and tree-based metrics are computed independently.\nFinally, all are merged into one raster predictor stack.\n\n\n\n\n3 2. Project Setup\n# Load required packages and environment\nrequire(envimaR)\nrequire(rprojroot)\n\n# Determine root directory of project (requires .Rproj or .here file)\nroot_folder &lt;- find_rstudio_root_file()\n\n# Load envrmt list with all folder paths and EPSG settings\nsource(file.path(root_folder, \"src/000-rspatial-setup.R\"), echo = TRUE)\n\nenvimaR handles dynamic folder structures.\nenvrmt contains paths like path_lidar_raster, path_topo, etc.\nepsg_number, bbox and other global spatial variables are set here.\n\n\n\n\n4 3. Normalizing the LAS Catalog\nctg &lt;- readLAScatalog(las_fileFN)\nctg_base &lt;- normalize_height(ctg, knnidw(k = 6L, p = 2))\n\nA LAS catalog is loaded and normalized.\nGround points are removed to prepare for CHM and DSM creation.\n\n\n\n\n5 4. Terrain Models\ndem &lt;- rasterize_terrain(ctg, res = 1, knnidw(k = 6L, p = 2))\ndsm &lt;- rasterize_canopy(ctg, res = 1, algorithm = pitfree())\nchm &lt;- rasterize_canopy(ctg_base, res = 1, pitfree(c(0,2,5,10,15)))\n\nDEM (Digital Elevation Model) is created from ground returns.\nDSM (Surface Model) and CHM (Canopy Height Model) from canopy points.\n\n\n\n\n6 5. Topographic Derivatives\nslope &lt;- terrain(dem, \"slope\")\naspect &lt;- terrain(dem, \"aspect\")\nTPI &lt;- terrain(dsm, \"TPI\")\n\nDerived terrain parameters used for modeling light, moisture, and temperature.\n\n\n\n\n7 6. Pixel-Level Metrics\npixel_stdmetrics &lt;- pixel_metrics(ctg_base, .stdmetrics, res = 1)\npixel_LAD &lt;- pixel_metrics(ctg_base, ~as.numeric(cv(LAD(Z, dz = 1, k = 0.87)$lad)), res = 1)\npixel_entropy &lt;- pixel_metrics(ctg_base, ~as.numeric(entropy(Z, by = 1.0)), res = 1)\npixel_VCI &lt;- pixel_metrics(ctg_base, ~as.numeric(VCI(Z, zmax = 40, by = 1.0)), res = 1)\nThese voxel-based metrics represent vertical structure:\n\nLAD = Leaf Area Density\nVCI = Vertical Complexity Index\nEntropy = point height diversity\nipground = intensity of ground points (optional)\n\n\n\n\n8 7. Tree Segmentation and Metrics\nctg_seg &lt;- segment_trees(ctg_base, li2012())\nhulls &lt;- catalog_apply(ctg_seg, tree_fn)\nlad_vox &lt;- lad.voxels(ctg_base, grain.size = 1, k = 0.87, maxP = 40)\n\nTrees are segmented using the Li et al. (2012) method.\ntree_fn generates convex hulls or crown shapes.\nLAD profiles are voxelized and linked to hulls.\n\n\n\n\n9 8. Clustering Tree Profiles\nclust_model &lt;- KMeans_arma(data_clust, clusters = 10, n_iter = 500)\ntrees_lad$cluster &lt;- predict_KMeans(data_clust, clust_model)\n\nLAD metrics are dimensionally reduced (PCA or manually).\nClustering assigns structural class per tree.\nResult is written as vector layer and rasterized.\n\n\n\n\n10 9. Predictor Stack Creation\nforest_structure_metrics &lt;- c(rast(topoFN), rast(pmetricsFN), rast(tree_clus_rasFN))\nwriteRaster(forest_structure_metrics, predstack_forest_metricsFN, overwrite = TRUE)\n\nCombines topography, pixel metrics, and clusters into one multiband raster.\n\n\n\n\n11 10. Optional: Solar Irradiance via GRASS\nlinkGRASS7(dem, gisdbase = root_folder, location = \"MOF2\")\nexecGRASS(\"r.sun.hourly\", parameters = list(...))\n\nOptionally runs r.sun.hourly from GRASS to model solar radiation.\nResulting hourly radiation maps can be included in predictor stacks.\n\n\n\n\n12 Output Summary\n\n\n\n\n\n\n\n\nLayer\nType\nDescription\n\n\n\n\ntopo.tif\nRaster\nTerrain-derived variables\n\n\nall_pixel_metrics.tif\nRaster\nStructural voxel statistics\n\n\nlad_hull_raster.tif\nRaster\nLAD metrics aggregated to tree hulls\n\n\ntree_cluster.tif\nRaster\nCluster class per tree segment\n\n\npred_forest_structure.tif\nRaster\nFull predictor stack for modeling\n\n\ntrees_lad_clean.rds\nDataFrame\nTree-level statistics for analysis\n\n\n\n\n\n\n13 Questions or Extensions\n\nAdd modeling scripts (e.g. Random Forest, GLM, XGBoost)\nVisualize clusters with tmap or leaflet\nCombine with microclimate sensors or UAV data"
  },
  {
    "objectID": "doc/treespecies.html",
    "href": "doc/treespecies.html",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "",
    "text": "Purpose of Tree Data for ENVI-met Modeling This workflow prepares classified tree species data as a basis for generating individual tree objects for use in ENVI-met’s 3DPLANT module. Each tree location is linked to a simplified vertical LAD profile and assigned to a species class (e.g. Fagus sylvatica, Quercus robur, Pseudotsuga menziesii), which defines its interaction with ENVI-met’s radiation and vegetation modules.\nThe underlying classification raster originates from official, state-level aerial RGB orthophotos with a spatial resolution of 0.3 m. These orthophotos provide sufficient detail to allow object-based species classification at the level of individual tree crowns.\nSpecies prediction was performed using a leave-location-out forward feature selection approach implemented via the CAST package in R. This ensures that classification results generalize across spatially distinct regions by avoiding overfitting to local spectral conditions.\nBefore assigning vegetation objects to the ENVI-met model domain, species maps are despeckled, aggregated, and contextually corrected to remove isolated or misclassified tree crowns (e.g. Douglas-fir pixels in beech-dominated stands). This ensures that each synthetic ENVI-met tree is placed in a semantically and structurally consistent vegetation context.",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#setup-and-environment",
    "href": "doc/treespecies.html#setup-and-environment",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "1. Setup and Environment",
    "text": "1. Setup and Environment\nManual Setup of OTB Environment for use with link2GI\nThe R package link2GI provides wrapper functions to connect R with external geospatial software like Orfeo Toolbox (OTB), GRASS GIS, and QGIS. The function linkOTB() is used to locate OTB binaries and configure the R session to allow calling OTB applications via command-line interface (CLI) from R.\nHowever, in many modern setups—especially on Linux or in manually installed environments (e.g., extracted zip files)—the required environment variables are not set globally, and linkOTB() alone is not sufficient. This typically leads to errors like:\n\n“Application not found”\n“No XML application descriptors”\n“Could not find CLI tools”\n\nTo fix this, two critical environment variables need to be explicitly set after calling linkOTB():\n\nOTB_APPLICATION_PATH: This must point to the directory lib/otb/applications, where all XML definitions of the OTB applications are stored. These XML files describe how to call each OTB tool from the command line.\nPATH: This must include the directory where OTB binaries like otbcli_BandMath are stored (typically bin/). Without this, system calls from R to OTB will fail.\n\nExample for Linux:\n\notb &lt;- link2GI::linkOTB(searchLocation = \"~/apps/OTB-9.1.0-Linux/\")\nSys.setenv(OTB_APPLICATION_PATH = file.path(dirname(as.character(otb$pathOTB)), \"lib/otb/applications\"))\nSys.setenv(PATH = paste(otb$pathOTB, Sys.getenv(\"PATH\"), sep = \":\"))\n\nExample for Windows:\n\notb &lt;- link2GI::linkOTB(searchLocation = \"C:/OTB-9.1.0-Win64/\")\nSys.setenv(OTB_APPLICATION_PATH = \"C:/OTB-9.1.0-Win64/lib/otb/applications\")\nSys.setenv(PATH = paste(\"C:/OTB-9.1.0-Win64/bin\", Sys.getenv(\"PATH\"), sep = \";\"))\n\nNote:\n\nOn Windows, use forward slashes / in the path.\nThe PATH separator is ; on Windows and : on Unix-based systems.\n\nThis workaround is often necessary in portable, containerized, or research setups where full system integration (e.g., PATH exports, registry entries) is not available or not desired. It ensures that link2GI can still function as intended by emulating the expected environment internally within R.\n\n# Load libraries\nlibrary(terra)\nlibrary(RColorBrewer)\nlibrary(link2GI)\nlibrary(tools)\nlibrary(mapview)\nlibrary(dplyr)\n\n# Project root and OTB environment\nroot_folder &lt;- rprojroot::find_rstudio_root_file()\notb &lt;- link2GI::linkOTB(searchLocation = \"~/apps/OTB-9.1.0-Linux/\")\nSys.setenv(OTB_APPLICATION_PATH = file.path(dirname(as.character(otb$pathOTB)), \"lib/otb/applications\"))\nSys.setenv(PATH = paste(otb$pathOTB, Sys.getenv(\"PATH\"), sep = \":\"))",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#parameters-and-class-legend",
    "href": "doc/treespecies.html#parameters-and-class-legend",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "2. Parameters and Class Legend",
    "text": "2. Parameters and Class Legend\n\ntarget_res &lt;- 1\nfn &lt;- \"5-25_MOF_rgb\"\nepsg &lt;- 25832\nsapflow_ext &lt;- raster::extent(477500, 478218, 5631730, 5632500)\n\nts &lt;- data.frame(\n  ID = 1:12,\n  value = c(\"agriculture\", \"alder\", \"ash\", \"beech\", \"douglas_fir\", \"larch\",\n            \"oak\", \"pastures\", \"roads\", \"settlements\", \"spruce\", \"water\")\n)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#rationale-why-despeckle-first",
    "href": "doc/treespecies.html#rationale-why-despeckle-first",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "3. Rationale: Why Despeckle First?",
    "text": "3. Rationale: Why Despeckle First?\n\n🔍 Why do we despeckle at original resolution before aggregation and contextual filtering?\n\nPreserve spatial detail: High-frequency noise (e.g., misclassified single pixels) must be removed before they get averaged into larger grid cells.\nAvoid error propagation: Aggregating first would carry speckle artifacts into the coarser grid.\nEnable ecologically meaningful correction: Focal filtering (e.g., Douglas-fir to Oak) should be applied on ~1 m resolution where “dominance” of classes has meaning.\nStep order summary:\n\nClassificationMapRegularization: Clean noise at 0.2 m\naggregate(): Smooth to 1 m (e.g., crown scale)\nfocal(): Replace ecologically implausible patches",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#load-and-preprocess-species-classification",
    "href": "doc/treespecies.html#load-and-preprocess-species-classification",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "4. Load and Preprocess Species Classification",
    "text": "4. Load and Preprocess Species Classification\n\nsapflow_species &lt;- readRDS(\"../data/aerial/sfprediction_ffs_5-25_MOF_rgb.rds\")\nraster::writeRaster(sapflow_species, \"../data/aerial/prediction_ffs.tif\", overwrite = TRUE)\nsapflow_species &lt;- raster::crop(sapflow_species, sapflow_ext)\nraster::writeRaster(sapflow_species, \"../data/aerial/prediction_ffs_cut.tif\", overwrite = TRUE)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#majority-filtering-otb-despeckle",
    "href": "doc/treespecies.html#majority-filtering-otb-despeckle",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "5. Majority Filtering (OTB Despeckle)",
    "text": "5. Majority Filtering (OTB Despeckle)\n\ncmr &lt;- parseOTBFunction(\"ClassificationMapRegularization\", otb)\ncmr$io.in &lt;- \"../data/aerial/prediction_ffs.tif\"\ncmr$io.out &lt;- \"../data/aerial/majority_out.tif\"\ncmr$ip.radius &lt;- \"1\"\ncmr$progress &lt;- \"true\"\nfilter_treespecies &lt;- runOTB(cmr, gili = otb$pathOTB, quiet = FALSE, retRaster = TRUE)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#aggregate-to-1-m-resolution",
    "href": "doc/treespecies.html#aggregate-to-1-m-resolution",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "6. Aggregate to 1 m Resolution",
    "text": "6. Aggregate to 1 m Resolution\n\nr &lt;- rast(\"../data/aerial/majority_out.tif\")\ncur_res &lt;- res(r)[1]\nfact &lt;- round(target_res / cur_res)\nif (target_res &lt;= cur_res) stop(\"Target resolution is lower than input resolution.\")\nr_agg &lt;- aggregate(r, fact = fact, fun = median, na.rm = TRUE)\noutfile &lt;- sprintf(\"../data/aerial/%s_%sm.tif\", tools::file_path_sans_ext(basename(\"../data/aerial/aggregate.tif\")), target_res)\nwriteRaster(r_agg, outfile, overwrite = TRUE)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#contextual-correction-douglas-beechoak",
    "href": "doc/treespecies.html#contextual-correction-douglas-beechoak",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "7. Contextual Correction (Douglas → Beech/Oak)",
    "text": "7. Contextual Correction (Douglas → Beech/Oak)\n\nreplace_douglas_in_buche_eiche &lt;- function(rast_input,\n                                           window_size = 5,\n                                           douglas_value = 5,\n                                           target_values = c(4, 7),\n                                           target_res = 1.0) {\n  if (!inherits(rast_input, \"SpatRaster\")) stop(\"Input must be SpatRaster\")\n  if (window_size %% 2 == 0) stop(\"window_size must be odd\")\n  w &lt;- matrix(1, nrow = window_size, ncol = window_size)\n  r_mode &lt;- focal(rast_input, w = w, fun = modal, na.policy = \"omit\", na.rm = TRUE, progress = \"text\")\n  is_douglas &lt;- rast_input == douglas_value\n  is_oak_beech_mode &lt;- r_mode %in% target_values\n  replace_mask &lt;- is_douglas & is_oak_beech_mode\n  r_new &lt;- rast_input\n  r_new[replace_mask] &lt;- r_mode[replace_mask]\n  writeRaster(r_new, sprintf(\"../data/aerial/%s_%sm.tif\", \"agg_cleand\", target_res), overwrite = TRUE)\n  return(r_new)\n}\nspecies_cleaned &lt;- replace_douglas_in_buche_eiche(r_agg, window_size = 5)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#complete-code",
    "href": "doc/treespecies.html#complete-code",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "Complete Code",
    "text": "Complete Code\n\n#------------------------------------------------------------------------------\n# Script Type: Processing script\n# Script Name: 30_filter_classified_species_map.R\n# Author: Chris Reudenbach, creuden@gmail.com\n#\n# Description:\n#   - Loads a classified raster map of tree species\n#   - Applies spatial smoothing using OTB ClassificationMapRegularization\n#   - Aggregates to 1 m resolution using median filtering\n#   - Performs contextual correction: replaces isolated Douglas-fir pixels\n#     with Beech or Oak if those are dominant in the local neighborhood\n#\n# Input:\n#   - RDS and GeoTIFF classification of tree species (0.2 m resolution)\n#\n# Output:\n#   - Cleaned and aggregated species raster (1 m resolution)\n#\n# Dependencies:\n#   - OTB 9.1+ with PATH and OTB_APPLICATION_PATH correctly set\n#\n# Copyright: Chris Reudenbach 2021, GPL (&gt;= 3)\n# Git: https://github.com/gisma-courses/microclimate.git\n#\n# Commentary:\n#   This workflow separates two crucial but distinct steps in map cleaning:\n#   1. **Noise reduction (smoothing):** Applied via OTB's ClassificationMapRegularization.\n#      - It performs a fast majority-based smoothing using a local moving window (e.g., 3x3).\n#      - This step removes small speckles or misclassified pixels in homogeneous areas.\n#      - It is computationally efficient due to OTB's C++-based implementation.\n#\n#   2. **Semantic filtering:** Performed in R via a contextual reclassification function.\n#      - Specifically targets ecologically unlikely or isolated Douglas-fir pixels.\n#      - These are replaced with surrounding Beech or Oak pixels if they dominate locally.\n#      - Allows flexible, rule-based filtering that OTB cannot natively perform.\n#\n#   ➤ Both steps are technically possible in R using terra::focal(), but:\n#     - Smoothing with `focal()` is **much slower** on large rasters (single-threaded).\n#     - OTB is highly recommended for performance.\n#\n#   ➤ The R-based semantic filtering step is **required** if logical replacement\n#     rules (like Douglas-fir substitution) are needed. This goes beyond statistical smoothing.\n#------------------------------------------------------------------------------\n\n\n# === Libraries ===\nlibrary(terra)           # raster handling\n\nterra 1.8.60\n\nlibrary(RColorBrewer)    # color palettes\nlibrary(link2GI)         # OTB integration\nlibrary(rprojroot)\nlibrary(tools)           # file name tools\nlibrary(mapview)         # interactive maps\nlibrary(dplyr)           # data manipulation\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:terra':\n\n    intersect, union\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# === Environment and paths ===\nroot_folder &lt;- find_rstudio_root_file()\n\n# Set up OTB environment\notb &lt;- link2GI::linkOTB(searchLocation = \"~/apps/OTB-9.1.0-Linux/\")\nSys.setenv(OTB_APPLICATION_PATH = file.path(dirname(as.character(otb$pathOTB)), \"lib/otb/applications\"))\nSys.setenv(PATH = paste(otb$pathOTB, Sys.getenv(\"PATH\"), sep = \":\"))\n\n# === Parameters ===\ntarget_res &lt;- 1                # desired resolution in meters\nmin_tree_height &lt;- 2           # (not used yet)\nfn &lt;- \"5-25_MOF_rgb\"           # image stem\nepsg &lt;- 25832                  # UTM32N\nsapflow_ext &lt;- raster::extent(477500, 478218, 5631730, 5632500)  # area of interest\n\n# === Class ID legend ===\nts &lt;- data.frame(\n  ID = 1:12,\n  value = c(\n    \"agriculture\",\n    \"alder\",\n    \"ash\",\n    \"beech\",\n    \"douglas_fir\",\n    \"larch\",\n    \"oak\",\n    \"pastures\",\n    \"roads\",\n    \"settlements\",\n    \"spruce\",\n    \"water\"\n  )\n)\n\n#------------------------------------------------------------------------------\n# FUNCTION: Replace isolated Douglas-fir with Beech or Oak if dominant around\n#------------------------------------------------------------------------------\nreplace_douglas_in_buche_eiche &lt;- function(rast_input,\n                                           window_size = 5,\n                                           douglas_value = 5,\n                                           target_values = c(4, 7),\n                                           target_res = 1.0) {\n  if (window_size %% 2 == 0)\n    stop(\"window_size must be odd\")\n  \n  # Focal window matrix (square)\n  w &lt;- matrix(1, nrow = window_size, ncol = window_size)\n  \n  # Run OTB ClassificationMapRegularization to compute local mode\n  cmr &lt;- parseOTBFunction(\"ClassificationMapRegularization\", otb)\n  cmr$io.in &lt;- sprintf(\"../data/aerial/%s_%sm.tif\",\n                       tools::file_path_sans_ext(basename(\"../data/aerial/aggregate.tif\")),\n                       target_res)\n  cmr$io.out &lt;- sprintf(\"../data/aerial/%s_%sm.tif\",\n                        tools::file_path_sans_ext(basename(\"../data/aerial/aggregate_mode.tif\")),\n                        window_size)\n  cmr$ip.radius &lt;- as.character((window_size - 1) / 2)  # for 5x5 window: radius = (5 - 1)/2 = 2\n  cmr$progress &lt;- \"true\"\n  \n  runOTB(cmr, gili = otb$pathOTB, quiet = FALSE)\n\n  # Identify Douglas-fir pixels and surrounding Beech/Oak dominance\n  r_mode = rast(cmr$io.out)\n  rast_input = rast(cmr$io.in)\n  is_douglas &lt;- rast_input == douglas_value\n  is_oak_beech_mode &lt;- r_mode %in% target_values\n  replace_mask &lt;- is_douglas & is_oak_beech_mode\n  \n  # Replace Douglas-fir where Beech or Oak dominate\n  r_new &lt;- rast_input\n  r_new[replace_mask] &lt;- r_mode[replace_mask]\n  \n  # Construct output path \n  outname &lt;- paste0(\"../data/aerial/\",\n                    \"agg_cleand_\",\n                    as.character(target_res),\n                    \"m.tif\")\n  writeRaster(r_new, outname,overwrite = TRUE)\n  \n  return(r_new)\n}\n\n#------------------------------------------------------------------------------\n# STEP 1: Read tree species classification from RDS\n#------------------------------------------------------------------------------\nsapflow_species &lt;- readRDS(\"../data/aerial/sfprediction_ffs_5-25_MOF_rgb.rds\")\n\n# Write to GeoTIFF for further processing\nraster::writeRaster(\n  sapflow_species,\n  \"../data/aerial/prediction_ffs.tif\",\n  progress = \"text\",\n  overwrite = TRUE\n)\n\n# Crop to sapflow test area\nsapflow_species &lt;- raster::crop(sapflow_species, sapflow_ext)\nraster::writeRaster(\n  sapflow_species,\n  \"../data/aerial/prediction_ffs_cut.tif\",\n  progress = \"text\",\n  overwrite = TRUE\n)\n\n#------------------------------------------------------------------------------\n# STEP 2: Run OTB ClassificationMapRegularization (majority filter)\n#------------------------------------------------------------------------------\ncmr &lt;- parseOTBFunction(\"ClassificationMapRegularization\", otb)\ncmr$io.in &lt;- \"../data/aerial/prediction_ffs.tif\"\ncmr$io.out &lt;- \"../data/aerial/majority_out.tif\"\ncmr$progress &lt;- \"true\"\ncmr$ip.radius &lt;- \"1\"\n\nfilter_treespecies &lt;- runOTB(cmr,\n                             gili = otb$pathOTB,\n                             quiet = FALSE,\n                             retRaster = TRUE)\n\notbcli_ClassificationMapRegularization  -io.in ../data/aerial/prediction_ffs.tif -io.out /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/majority_out.tif -ip.radius 1 -ip.suvbool false -ip.nodatalabel 0 -ip.undecidedlabel 0 -ip.onlyisolatedpixels false -ip.isolatedthreshold 1 -ram 256 -progress true\n\n\n[1] \"2025-07-29 14:52:41 (INFO) ClassificationMapRegularization: Default RAM limit for OTB is 256 MB\"\n[1] \"2025-07-29 14:52:41 (INFO) ClassificationMapRegularization: GDAL maximum cache size is 4814 MB\"\n[1] \"2025-07-29 14:52:41 (INFO) ClassificationMapRegularization: OTB will use at most 32 threads\"\n[1] \"2025-07-29 14:52:41 (INFO): Loading metadata from official product\"\n[1] \"2025-07-29 14:52:41 (INFO): Estimated memory for full processing: 1052.34MB (avail.: 256 MB), optimal image partitioning: 5 blocks\"\n[1] \"2025-07-29 14:52:41 (INFO): File /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/majority_out.tif will be written in 6 blocks of 14798x2044 pixels\"\n[1] \"Writing /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/majority_out.tif...: 100% [**************************************************] (2s)\"\n\n#------------------------------------------------------------------------------\n# STEP 3: Aggregate to 1 m resolution using median\n#------------------------------------------------------------------------------\nr &lt;- rast(\"../data/aerial/majority_out.tif\")\ncur_res &lt;- res(r)[1]\nfact &lt;- round(target_res / cur_res)\n\nif (target_res &lt;= cur_res)\n  stop(\"Zielauflösung ist kleiner als aktuelle.\")\n\nr_agg &lt;- aggregate(r,\n                   fact = fact,\n                   fun = median,\n                   na.rm = TRUE)\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n# Build automatic filename\noutfile &lt;- sprintf(\"../data/aerial/%s_%sm.tif\",\n                   tools::file_path_sans_ext(basename(\"../data/aerial/aggregate.tif\")),\n                   target_res)\n\n# Save aggregated raster\nwriteRaster(r_agg, outfile, overwrite = TRUE)\n\n#------------------------------------------------------------------------------\n# STEP 4: Clean Douglas-fir patches contextually\n#------------------------------------------------------------------------------\nspecies_cleaned &lt;- replace_douglas_in_buche_eiche(window_size = 9)\n\notbcli_ClassificationMapRegularization  -io.in ../data/aerial/aggregate_1m.tif -io.out /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/aggregate_mode_9m.tif -ip.radius 4 -ip.suvbool false -ip.nodatalabel 0 -ip.undecidedlabel 0 -ip.onlyisolatedpixels false -ip.isolatedthreshold 1 -ram 256 -progress true\n\n\n[1] \"2025-07-29 14:52:46 (INFO) ClassificationMapRegularization: Default RAM limit for OTB is 256 MB\"\n[1] \"2025-07-29 14:52:47 (INFO) ClassificationMapRegularization: GDAL maximum cache size is 4814 MB\"\n[1] \"2025-07-29 14:52:47 (INFO) ClassificationMapRegularization: OTB will use at most 32 threads\"\n[1] \"2025-07-29 14:52:47 (INFO): Loading metadata from official product\"\n[1] \"2025-07-29 14:52:47 (INFO): Estimated memory for full processing: 43.842MB (avail.: 256 MB), optimal image partitioning: 1 blocks\"\n[1] \"2025-07-29 14:52:47 (INFO): File /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/aggregate_mode_9m.tif will be written in 1 blocks of 2960x2453 pixels\"\n[1] \"Writing /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/aggregate_mode_9m.tif...: 100% [**************************************************] (0s)\"\n\n#------------------------------------------------------------------------------\n# STEP 5: Visualize intermediate steps (interactive)\n#------------------------------------------------------------------------------\n\nlibrary(mapview)\nlibrary(leafsync)\nlibrary(htmlwidgets)\nlibrary(terra)\nlibrary(RColorBrewer)\n\n# Define common parameters\npalette &lt;- brewer.pal(12, \"Paired\")\nzoom_center &lt;- list(lng = 8.68443, lat = 50.84089, zoom = 18)\n\n# -- Map 1: Raw species classification (0.2 m)\nm1 &lt;- mapview(\n  terra::crop(sapflow_species, sapflow_ext),\n  col.regions = palette,\n  at = ts$ID,\n  layer.name = \"Species 0.2m\"\n)\n\nWarning in rasterCheckSize(x, maxpixels = maxpixels): maximum number of pixels for Raster* viewing is 5e+05 ; \nthe supplied Raster* has 13821500 \n ... decreasing Raster* resolution to 5e+05 pixels\n to view full resolution set 'maxpixels =  13821500 '\n\n# -- Map 2: OTB 3×3 modal smoothing\nm2 &lt;- mapview(\n  terra::crop(filter_treespecies, sapflow_ext),\n  col.regions = palette,\n  at = ts$ID,\n\n  layer.name = \"3x3 modal_filt\"\n)\n\nNumber of pixels is above 5e+05.Only about 5e+05 pixels will be shown.\nYou can increase the value of `maxpixels` to 13821500 to avoid this.\n\n# -- Map 3: Aggregated to 1 m resolution\nm3 &lt;- mapview(\n  terra::crop(r_agg, sapflow_ext),\n  col.regions = palette,\n  at = ts$ID,\n  layer.name = \"Aggregated 1m\"\n)\n\nNumber of pixels is above 5e+05.Only about 5e+05 pixels will be shown.\nYou can increase the value of `maxpixels` to 552860 to avoid this.\n\n\nWarning: Found more colors (12) than zcol values (11)! \nTrimming colors to match number of zcol values.\n\n# -- Map 4: Douglas-fir replaced by contextual rules\nm4 &lt;- mapview(\n  terra::crop(species_cleaned, sapflow_ext),\n  col.regions = palette,\n\n  fgb = TRUE,\n  layer.name = \"Douglas out 1m\"\n)\n\nNumber of pixels is above 5e+05.Only about 5e+05 pixels will be shown.\nYou can increase the value of `maxpixels` to 552860 to avoid this.\n\n\nWarning: Found more colors (12) than zcol values (11)! \nTrimming colors to match number of zcol values.\n\n# Convert to leaflet and apply zoom center\nlm1 &lt;- m1@map %&gt;% leaflet::setView(zoom_center$lng, zoom_center$lat, zoom_center$zoom)\nlm2 &lt;- m2@map %&gt;% leaflet::setView(zoom_center$lng, zoom_center$lat, zoom_center$zoom)\nlm3 &lt;- m3@map %&gt;% leaflet::setView(zoom_center$lng, zoom_center$lat, zoom_center$zoom)\nlm4 &lt;- m4@map %&gt;% leaflet::setView(zoom_center$lng, zoom_center$lat, zoom_center$zoom)\n\n# Synchronize maps side-by-side\n\nout &lt;- sync(lm1, lm2, lm3, lm4)\n\nout",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "templates/journals-examples-default.html",
    "href": "templates/journals-examples-default.html",
    "title": "Scales in Geography and Ecology\nA neverending story",
    "section": "",
    "text": "Ecology is the study of how organisms interact with each other and their environment. Scales and processes are two fundamental concepts in ecology that are used to understand ecological patterns and dynamics.\nScales in ecology refer to the spatial and temporal dimensions of ecological phenomena. Ecological phenomena occur at different scales, ranging from individual organisms to entire ecosystems and from seconds to millennia. Understanding the appropriate scale is crucial for understanding the ecological processes that occur within that scale. The selection of an appropriate scale also influences the accuracy and precision of ecological data and the interpretation of ecological patterns.\nProcesses in ecology refer to the biological and physical mechanisms that underlie ecological phenomena. Ecological processes occur at different scales, ranging from the molecular level to the landscape level. Ecological processes include biotic interactions, such as competition, predation, and mutualism, as well as abiotic interactions, such as climate, nutrient cycling, and disturbances. Understanding the underlying processes is essential for predicting how ecological systems will respond to changing conditions and for developing effective conservation and management strategies.\nOverall, scales and processes are essential concepts in ecology that are used to understand the complex interactions between organisms and their environment."
  },
  {
    "objectID": "templates/journals-examples-default.html#subsection",
    "href": "templates/journals-examples-default.html#subsection",
    "title": "Scales in Geography and Ecology\nA neverending story",
    "section": "Subsection",
    "text": "Subsection\nAny study that examines the effects of area-based attributes on individual behaviors or outcomes faces another fundamental methodological problem besides the modifiable areal unit problem (MAUP). It is the problem that results about these effects can be affected by how contextual units or neighborhoods are geographically delineated and the extent to which these areal units deviate from the true geographic context. The problem arises because of the spatial uncertainty in the actual areas that exert the contextual influences under study and the temporal uncertainty in the timing and duration in which individuals experienced these contextual influences. Using neighborhood effects and environmental health research as a point of departure, this article clarifies the nature and sources of this problem, which is referred to as the uncertain geographic context problem (UGCoP). It highlights some of the inferential errors that the UGCoP might cause and discusses some means for mitigating the problem. It reviews recent studies to show that both contextual variables and research findings are sensitive to different delineations of contextual units. The article argues that the UGCoP is a problem as fundamental as the MAUP but is a different kind of problem. Future research needs to pay explicit attention to its potential confounding effects on research results and to methods for mitigating the problem (Kwan 2012).\n\n\n\n\n\n\n\n\nFigure 1: There are four lights"
  },
  {
    "objectID": "templates/journals-examples-default.html#subsection-1",
    "href": "templates/journals-examples-default.html#subsection-1",
    "title": "Scales in Geography and Ecology\nA neverending story",
    "section": "subsection",
    "text": "subsection\nTime can have a significant effect on scale in geography. This is because spatial relationships and patterns can change over time, and the appropriate scale to study a phenomenon may also change as a result. For example, the appropriate scale to study a natural disaster such as a hurricane may change as the storm approaches and intensifies, and then again as it makes landfall and moves inland. Similarly, the appropriate scale to study urban growth may change over time as the city expands and new neighborhoods or suburbs emerge.\nIn addition, the temporal scale at which data is collected and analyzed can also impact the understanding of geographic phenomena. For example, studying population changes over a decade may reveal different patterns and trends than studying changes over a single year.\nOverall, time is an important consideration in determining the appropriate scale to study a phenomenon and in interpreting the results of geographic analyses. It is essential to consider how spatial patterns and relationships change over time, and to collect and analyze data at appropriate temporal scales in order to gain a comprehensive understanding of geographic phenomena.\n\n3rd level\nYes, there are several theories in geography that relate to the effects of time on scale. One of the most well-known is the concept of temporal scale developed by geographer David Harvey. According to Harvey, temporal scale refers to the length of time over which a phenomenon can be observed, and it is an important consideration in understanding the spatial relationships and patterns that exist within a particular geographic context. In addition, Harvey argues that the temporal scale at which data is collected and analyzed can have a significant impact on the conclusions that can be drawn from the data.\nAnother theory related to the effects of time on scale is the concept of time-space compression developed by geographer David Harvey and others. This theory suggests that advances in technology and communication have led to a compression of time and space, making the world feel smaller and more connected. As a result, geographic phenomena may be influenced by factors that exist at different spatial and temporal scales, and it is important to consider these factors in analyzing and interpreting geographic data.\nOverall, the theories related to the effects of time on scale in geography highlight the complex and dynamic relationships between spatial and temporal phenomena, and the importance of considering these relationships in geographic analyses (Harvey 1996).\n\n4th level\nTime-space compression is a concept that has been developed and elaborated upon by several geographers, including David Harvey, Doreen Massey, and Henri Lefebvre. At its core, time-space compression refers to the idea that advances in transportation and communication technologies have created a sense of “shrinking” in terms of the time and space required for social and economic interactions.\nOne way in which time-space compression is manifested is through the increasing speed and ease of transportation and communication. For example, air travel, high-speed rail, and the internet have all made it possible to move goods, people, and information across vast distances in relatively short periods of time. This has led to a blurring of traditional spatial boundaries, and the emergence of global networks of exchange and interaction.\nHowever, time-space compression is not a neutral or uniform process, and its effects are felt differently by different people and in different places. For example, the increased speed of global trade may benefit some individuals and communities while harming others, and the intensification of global networks can lead to a sense of dislocation or disorientation for some people.\nOverall, time-space compression is an important concept in geography because it helps to explain how the spatial relationships and patterns that exist within a particular geographic context are shaped and transformed by technological change and globalization."
  },
  {
    "objectID": "doc/p3D.html",
    "href": "doc/p3D.html",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "",
    "text": "This tutorial demonstrates a full workflow to derive simplified ENVI-met-compatible 3DPLANT trees from airborne laser scanning (ALS) data. The process includes:\n\nVoxelization of ALS point cloud data\nLeaf Area Density (LAD) calculation per voxel column\nClustering of LAD profiles to define representative tree types\nExport of tree locations as GIS vector points\nExport of LAD profiles as ENVI-met .pld 3DPLANT definitions"
  },
  {
    "objectID": "doc/p3D.html#overview",
    "href": "doc/p3D.html#overview",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "",
    "text": "This tutorial demonstrates a full workflow to derive simplified ENVI-met-compatible 3DPLANT trees from airborne laser scanning (ALS) data. The process includes:\n\nVoxelization of ALS point cloud data\nLeaf Area Density (LAD) calculation per voxel column\nClustering of LAD profiles to define representative tree types\nExport of tree locations as GIS vector points\nExport of LAD profiles as ENVI-met .pld 3DPLANT definitions"
  },
  {
    "objectID": "doc/p3D.html#load-required-libraries",
    "href": "doc/p3D.html#load-required-libraries",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Load required libraries",
    "text": "Load required libraries\nlibrary(lidR)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(here)\nlibrary(XML)\nlibrary(stats)\nlibrary(tibble)\nlibrary(rprojroot)"
  },
  {
    "objectID": "doc/p3D.html#set-global-parameters",
    "href": "doc/p3D.html#set-global-parameters",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Set global parameters",
    "text": "Set global parameters\nlas_file &lt;- here(\"data/ALS/tiles/\")\nres_xy &lt;- 1\nres_z  &lt;- 1\nk      &lt;- 0.3\nscale_factor &lt;- 1.2\ncrs_code &lt;- 25832\noutput_gpkg &lt;- \"data/envimet/envimet_p3dtree_points.gpkg\"\nxml_output_file &lt;- \"data/envimet/als_envimet_trees.pld\"\nspecies_raster &lt;- rast(sprintf(\"data/aerial/%s_%sm.tif\", \"agg_cleand\", res_xy))\nn_clusters &lt;- 100"
  },
  {
    "objectID": "doc/p3D.html#step-1-read-and-preprocess-las-data",
    "href": "doc/p3D.html#step-1-read-and-preprocess-las-data",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 1: Read and preprocess LAS data",
    "text": "Step 1: Read and preprocess LAS data\nlas = merge_las_tiles(\n  tile_dir = las_file,\n  output_file = \"data/ALS/merged_output.laz\",\n  chunk_size = 10000,\n  workers = 6\n)\n\ncrs(las) &lt;- \"EPSG:25832\"\nlas &lt;- normalize_height(las, knnidw(k = 6, p = 2))\nlas &lt;- filter_poi(las, Z &gt; 0)"
  },
  {
    "objectID": "doc/p3D.html#step-2-voxelization",
    "href": "doc/p3D.html#step-2-voxelization",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 2: Voxelization",
    "text": "Step 2: Voxelization\nvoxels &lt;- voxel_metrics(las, ~length(Z), res = res_xy, dz = res_z)"
  },
  {
    "objectID": "doc/p3D.html#step-3-lad-profile-calculation",
    "href": "doc/p3D.html#step-3-lad-profile-calculation",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 3: LAD profile calculation",
    "text": "Step 3: LAD profile calculation\nlad_df &lt;- convert_voxel_lad_long(voxels, res_z = res_z, k = k, scale_factor = scale_factor)"
  },
  {
    "objectID": "doc/p3D.html#step-4-add-tree-species-class",
    "href": "doc/p3D.html#step-4-add-tree-species-class",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 4: Add tree species class",
    "text": "Step 4: Add tree species class\nspecies_at_xy &lt;- extract(species_raster, lad_df[, c(\"x\", \"y\")])\nlad_df$species_class &lt;- species_at_xy[, 2]"
  },
  {
    "objectID": "doc/p3D.html#step-5-clustering-lad-profiles",
    "href": "doc/p3D.html#step-5-clustering-lad-profiles",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 5: Clustering LAD profiles",
    "text": "Step 5: Clustering LAD profiles\nlad_df$xy_key &lt;- paste(lad_df$x, lad_df$y)\nlad_matrix &lt;- lad_df %&gt;% \n  tidyr::pivot_wider(names_from = z, values_from = lad, values_fill = 0) %&gt;%\n  column_to_rownames(\"xy_key\") %&gt;%\n  as.matrix()\n\nclustering &lt;- kmeans(lad_matrix, centers = n_clusters, nstart = 100)\nlad_df$cluster &lt;- clustering$cluster[match(lad_df$xy_key, rownames(lad_matrix))]"
  },
  {
    "objectID": "doc/p3D.html#step-6-assign-envi-met-compatible-ids",
    "href": "doc/p3D.html#step-6-assign-envi-met-compatible-ids",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 6: Assign ENVI-met compatible IDs",
    "text": "Step 6: Assign ENVI-met compatible IDs\ncluster_ids &lt;- unique(lad_df$cluster)\ncluster_mapping &lt;- data.frame(\n  cluster = cluster_ids,\n  ENVIMET_ID = sapply(cluster_ids, int_to_base36)\n)\nlad_df &lt;- left_join(lad_df, cluster_mapping, by = \"cluster\")"
  },
  {
    "objectID": "doc/p3D.html#step-7-export-spatial-locations",
    "href": "doc/p3D.html#step-7-export-spatial-locations",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 7: Export spatial locations",
    "text": "Step 7: Export spatial locations\npoint_df &lt;- lad_df[!duplicated(lad_df$xy_key), c(\"x\", \"y\", \"ENVIMET_ID\")]\nsf_points &lt;- st_as_sf(point_df, coords = c(\"x\", \"y\"), crs = crs_code)\nst_write(sf_points, output_gpkg, delete_layer = TRUE)"
  },
  {
    "objectID": "doc/p3D.html#step-8-export-xml-based-plant3d-definitions",
    "href": "doc/p3D.html#step-8-export-xml-based-plant3d-definitions",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 8: Export XML-based PLANT3D definitions",
    "text": "Step 8: Export XML-based PLANT3D definitions\nexport_lad_to_envimetp3d(lad_df, file_out = xml_output_file)"
  },
  {
    "objectID": "doc/p3D.html#result",
    "href": "doc/p3D.html#result",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Result",
    "text": "Result\n\nA geopackage containing tree positions and ENVIMET_IDs used for ENVI-met domain placement.\nA .pld file with clustered LAD profiles, ready for use in ENVI-met 3DPLANT simulations.\n\nThis method allows a realistic but computationally manageable vegetation representation in ENVI-met using TLS/ALS-derived structure."
  },
  {
    "objectID": "doc/tls_v1_3.html",
    "href": "doc/tls_v1_3.html",
    "title": "Untitled",
    "section": "",
    "text": "For a realitic sourounding we need voxelized ALS (Airborne Laser Scanning) data which needs to be usable as ENVI-met compatible 3DPLANT profiles. It includes LAD (Leaf Area Density) computation, profile clustering, and export to both GIS and XML formats for ENVI-met integration.\nTo provide a realistic but computationally tractable vegetation structure, we apply a pseudo-3D columnar tree approach: each voxel column of ALS returns is interpreted as a simplified vertical tree profile. These profiles are clustered to reduce complexity, then exported as ENVI-met 3DPLANT objects.\n\n\nUnlike TLS (Terrestrial Laser Scanning), which scans from the bottom-up and suffers from occlusion in upper layers, ALS samples vegetation top-down. This means:\n\nALS oversamples upper canopy layers\nALS undersamples lower canopy due to occlusion\n\nTo correct for this sampling bias, we estimate LAD using a modified form of Beer’s Law, based on the normalized proportion of hits per voxel layer. The key difference lies in the way “gap probability” is estimated: rather than tracking cumulative occlusion, ALS uses the maximum return count per column as a proxy for full canopy closure.\n\n\n\nWe model LAD using:\n\\[\nLAD = -\\frac{\\ln(1 - p)}{k \\cdot dz}\n\\]\nWhere: - ( p ) is the normalized proportion of hits per voxel column (( 0 &lt; p &lt; 1 )) - ( k ) is the light extinction coefficient - ( dz ) is the vertical resolution (voxel height)\nIn our script, we set: - ( k = 0.3 ) (typical value) - LAD values are scaled using a multiplicative factor (default 1.2)\n\n\nIn TLS-based LAD estimation, we assume that the LiDAR sensor is located near ground level and that returns are accumulated from bottom to top. In this setup, each voxel’s return count ( N_i ) is interpreted as contributing to the cumulative transmittance through the canopy.\nThe Beer–Lambert law is applied as:\n\\[\n\\text{LAD}_i = -\\frac{\\ln\\left(1 - \\frac{N_i}{N_{\\max}}\\right)}{k \\cdot \\Delta z}\n\\]\nHere: - ( N_i ): number of returns in voxel layer ( i ) - ( N_{} ): maximum number of returns in any voxel in the column (used to normalize return density) - ( z ): voxel height - ( k ): extinction coefficient\n\n\nThe ratio ( ) estimates the fraction of light intercepted at layer ( i ), assuming the densest layer represents near-total occlusion. Thus, the term ( 1 - ) represents the gap fraction — i.e., the probability that a beam of light traveling from the ground upward has not yet been occluded by vegetation up to that layer.\nThis interpretation fits the TLS scanning geometry, where lower layers are sampled first and occlusion increases with height.\n\n\n\n\nWe assume that the highest return count in the column corresponds to full canopy closure (i.e., near-zero gap fraction). This allows us to use the maximum as a local normalization factor:\n\n( p_i = )\n( LAD_i = -(1 - p_i) / (k dz) )\n\nThis does not model occlusion directly, but gives a consistent LAD profile for column-wise clustering.\n\n\n\n\n\nlibrary(lidR)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(here)\nlibrary(XML)\nlibrary(stats)\nlibrary(tibble)\nlibrary(data.table)\nlibrary(rprojroot)\n\nzmax &lt;- 40  \ngrain.size &lt;- 1  \noutput_merged_las &lt;- here(\"data/ALS/merged_output.las\")  \n\noutput_envimet_als_3d &lt;- here(\"data/envimet/als_envimet_trees.pld\")  \nsource(\"../src/utils.R\")\n\n\n\n\nlas_file &lt;- here(\"data/ALS/tiles\")  # Path to ALS point cloud file \n\n las_fn = merge_las_tiles(\n    tile_dir = las_file,\n    output_file = output_merged_las,\n    chunk_size = 10000,\n    workers = 6\n  )\n\nlas &lt;- readLAS(las_fn)\nlas &lt;- normalize_height(las, knnidw(k = 6, p = 2))\nlas &lt;- filter_poi(las, Z &gt; 0)\n\n\n\n\n\nvoxels &lt;- voxel_metrics(las, ~length(Z), res = res_xy, dz = res_z)\n\n\n\n\n\nlad_df &lt;- convert_voxel_lad_long(voxels, res_z = res_z, k = k, scale_factor = scale_factor)\n\n\n\n\nWe reduce the number of ENVI-met profiles by grouping similar LAD profiles using k-means clustering. LAD profiles are pivoted to a wide matrix (z-layers as columns):\n\n# Create a unique key per voxel column (x/y location)\nlad_df$xy_key &lt;- paste(lad_df$x, lad_df$y)\n\n# Reshape the long-format LAD data to a wide matrix:\n# each row = 1 (x, y) location (column), each column = height bin (z), cell = LAD value\nlad_matrix &lt;- lad_df %&gt;% \n  tidyr::pivot_wider(\n    names_from = z,            # use height (z) as new column names\n    values_from = lad,         # fill cells with LAD values\n    values_fill = 0            # fill missing voxel heights with 0 (no LAD)\n  ) %&gt;%\n  column_to_rownames(\"xy_key\") %&gt;%  # set xy_key as row names\n  as.matrix()                       # convert to numeric matrix for clustering\n\n# Cluster LAD profiles using k-means\nclustering &lt;- kmeans(lad_matrix, centers = n_clusters, nstart = 100, iter.max = 200)\nlad_df$cluster &lt;- clustering$cluster[match(lad_df$xy_key, rownames(lad_matrix))]\n\n\n\n\nEach LAD cluster is assigned a unique identifier that begins with “S” and uses base36 encoding (0–9, A–Z):\n\n#' Convert an integer to a 6-character alphanumeric ENVIMET ID (Base36)\n#'\n#' Converts an integer into a 6-character string using base-36 encoding (digits + uppercase letters),\n#' padded with leading zeros and prefixed with `\"S\"`. This is useful for assigning unique `ENVIMET_ID`s\n#' in 3D plant libraries for ENVI-met.\n#'\n#' @param n An integer (or vector of integers) to convert to alphanumeric IDs.\n#' @param width The number of base-36 digits to use (default: `5`, resulting in IDs like `\"S0000A\"`).\n#'\n#' @return A character vector of base-36 encoded IDs (with `\"S\"` prefix).\n#' @export\n#'\n#' @examples\n#' int_to_base36(1)       # \"S00001\"\n#' int_to_base36(35)      # \"S0000Z\"\n#' int_to_base36(36)      # \"S00010\"\n#' int_to_base36(123456)  # \"S02N9S\"\n#' int_to_base36(1:3)     # \"S00001\" \"S00002\" \"S00003\"\nint_to_base36 &lt;- function(n, width = 5) {\n  chars &lt;- c(0:9, LETTERS)         # Base-36 character set\n  base &lt;- length(chars)\n  result &lt;- character()\n  \n  while (n &gt; 0) {\n    result &lt;- c(chars[(n %% base) + 1], result)\n    n &lt;- n %/% base\n  }\n  \n  result &lt;- paste(result, collapse = \"\")\n  \n  # Pad with leading zeros if necessary\n  padded &lt;- sprintf(paste0(\"%0\", width, \"s\"), result)\n  \n  # Replace any spaces with \"0\" and prefix with \"S\"\n  paste0(\"S\", substr(gsub(\" \", \"0\", padded), 1, width))\n}\n\n\n\n\nEach unique LAD column becomes a point in a GeoPackage, tagged with its ENVIMET_ID.\n\n# Assign ENVIMET_IDs per cluster\ncluster_ids &lt;- unique(lad_df$cluster)\ncluster_mapping &lt;- data.frame(\n  cluster = cluster_ids,\n  ENVIMET_ID = sapply(cluster_ids, int_to_base36)\n)\nlad_df &lt;- left_join(lad_df, cluster_mapping, by = \"cluster\")\n\nsf_points &lt;- st_as_sf(point_df, coords = c(\"x\", \"y\"), crs = crs_code)\nst_write(sf_points, output_gpkg, delete_layer = TRUE)\n\n\n\n\nThe LAD profile per cluster is exported to a .pld file using XML.\n\nexport_lad_to_envimet_p3d(\n  lad_df = lad_clustered, \n  file_out = output_envimet_als_3d\n)\n\n\n\n\n\nEach clustered LAD profile is interpreted as a pseudo-3D vegetation column. These are not derived from segmented individual trees but represent aggregated vertical structure typical for a 2 × 2 m area.\nThis approach provides a balance between realism and simplicity:\n\nIt allows realistic vertical vegetation profiles from ALS\nReduces complexity through clustering\nProvides efficient integration into ENVI-met via both:\n\nGIS point layers with ENVIMET_ID\nXML-based 3DPLANT definitions\n\n\nPseudo-3D trees enable realistic microclimate domains with vegetation heterogeneity without requiring full 3D reconstruction.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nTLS\nALS\n\n\n\n\nView Direction\nBottom-up\nTop-down\n\n\nOcclusion Bias\nUndersamples upper canopy\nUndersamples lower canopy\n\n\nLAD Estimation\nCumulative bottom-up (Beer)\nNormalized per column (max count)\n\n\nTypical Use Case\nDetailed single tree analysis\nLarge-area structure sampling\n\n\n\n\n\n\nThis pipeline offers an efficient method to integrate voxelized ALS data into ENVI-met’s 3DPLANT framework by:\n\nEstimating LAD profiles via a Beer–Lambert-based approximation\nClustering voxel columns into representative pseudo-3D vegetation types\nExporting both point geometries and XML-based plant profiles\n\nAdvantages: - Scalable to large ALS datasets - Preserves key structural heterogeneity - Compatible with ENVI-met simulation domains\nLimitations: - Assumes that the maximum voxel return represents full canopy cover, which may not hold in sparse stands - LAD estimation is empirical; it does not model true light attenuation or occlusion - The pseudo-3D approach does not represent individual trees or crown geometry - Clustering may smooth out fine-scale vertical variability\nFuture improvements could include stratified LAD normalization, occlusion-aware corrections, or hybrid ALS-TLS fusion for enhanced realism.\n\n\n\n\nBéland, M., et al. (2014). Remote Sensing of Environment\nCalders, K., et al. (2015). Methods in Ecology and Evolution\nJupp, D. L. B., et al. (2009). Remote Sensing of Environment\n\n\n\n\nsource(\"src/microclimate_ALS.R\")\nThis source contains the complete processing workflow from voxel metrics to XML generation."
  },
  {
    "objectID": "doc/tls_v1_3.html#why-als-requires-a-specific-lad-approach",
    "href": "doc/tls_v1_3.html#why-als-requires-a-specific-lad-approach",
    "title": "Untitled",
    "section": "",
    "text": "Unlike TLS (Terrestrial Laser Scanning), which scans from the bottom-up and suffers from occlusion in upper layers, ALS samples vegetation top-down. This means:\n\nALS oversamples upper canopy layers\nALS undersamples lower canopy due to occlusion\n\nTo correct for this sampling bias, we estimate LAD using a modified form of Beer’s Law, based on the normalized proportion of hits per voxel layer. The key difference lies in the way “gap probability” is estimated: rather than tracking cumulative occlusion, ALS uses the maximum return count per column as a proxy for full canopy closure."
  },
  {
    "objectID": "doc/tls_v1_3.html#lad-estimation-using-beers-law",
    "href": "doc/tls_v1_3.html#lad-estimation-using-beers-law",
    "title": "Untitled",
    "section": "",
    "text": "We model LAD using:\n\\[\nLAD = -\\frac{\\ln(1 - p)}{k \\cdot dz}\n\\]\nWhere: - ( p ) is the normalized proportion of hits per voxel column (( 0 &lt; p &lt; 1 )) - ( k ) is the light extinction coefficient - ( dz ) is the vertical resolution (voxel height)\nIn our script, we set: - ( k = 0.3 ) (typical value) - LAD values are scaled using a multiplicative factor (default 1.2)\n\n\nIn TLS-based LAD estimation, we assume that the LiDAR sensor is located near ground level and that returns are accumulated from bottom to top. In this setup, each voxel’s return count ( N_i ) is interpreted as contributing to the cumulative transmittance through the canopy.\nThe Beer–Lambert law is applied as:\n\\[\n\\text{LAD}_i = -\\frac{\\ln\\left(1 - \\frac{N_i}{N_{\\max}}\\right)}{k \\cdot \\Delta z}\n\\]\nHere: - ( N_i ): number of returns in voxel layer ( i ) - ( N_{} ): maximum number of returns in any voxel in the column (used to normalize return density) - ( z ): voxel height - ( k ): extinction coefficient\n\n\nThe ratio ( ) estimates the fraction of light intercepted at layer ( i ), assuming the densest layer represents near-total occlusion. Thus, the term ( 1 - ) represents the gap fraction — i.e., the probability that a beam of light traveling from the ground upward has not yet been occluded by vegetation up to that layer.\nThis interpretation fits the TLS scanning geometry, where lower layers are sampled first and occlusion increases with height.\n\n\n\n\nWe assume that the highest return count in the column corresponds to full canopy closure (i.e., near-zero gap fraction). This allows us to use the maximum as a local normalization factor:\n\n( p_i = )\n( LAD_i = -(1 - p_i) / (k dz) )\n\nThis does not model occlusion directly, but gives a consistent LAD profile for column-wise clustering."
  },
  {
    "objectID": "doc/tls_v1_3.html#full-workflow-voxelization-to-envi-met-3d-trees",
    "href": "doc/tls_v1_3.html#full-workflow-voxelization-to-envi-met-3d-trees",
    "title": "Untitled",
    "section": "",
    "text": "library(lidR)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(here)\nlibrary(XML)\nlibrary(stats)\nlibrary(tibble)\nlibrary(data.table)\nlibrary(rprojroot)\n\nzmax &lt;- 40  \ngrain.size &lt;- 1  \noutput_merged_las &lt;- here(\"data/ALS/merged_output.las\")  \n\noutput_envimet_als_3d &lt;- here(\"data/envimet/als_envimet_trees.pld\")  \nsource(\"../src/utils.R\")\n\n\n\n\nlas_file &lt;- here(\"data/ALS/tiles\")  # Path to ALS point cloud file \n\n las_fn = merge_las_tiles(\n    tile_dir = las_file,\n    output_file = output_merged_las,\n    chunk_size = 10000,\n    workers = 6\n  )\n\nlas &lt;- readLAS(las_fn)\nlas &lt;- normalize_height(las, knnidw(k = 6, p = 2))\nlas &lt;- filter_poi(las, Z &gt; 0)\n\n\n\n\n\nvoxels &lt;- voxel_metrics(las, ~length(Z), res = res_xy, dz = res_z)\n\n\n\n\n\nlad_df &lt;- convert_voxel_lad_long(voxels, res_z = res_z, k = k, scale_factor = scale_factor)\n\n\n\n\nWe reduce the number of ENVI-met profiles by grouping similar LAD profiles using k-means clustering. LAD profiles are pivoted to a wide matrix (z-layers as columns):\n\n# Create a unique key per voxel column (x/y location)\nlad_df$xy_key &lt;- paste(lad_df$x, lad_df$y)\n\n# Reshape the long-format LAD data to a wide matrix:\n# each row = 1 (x, y) location (column), each column = height bin (z), cell = LAD value\nlad_matrix &lt;- lad_df %&gt;% \n  tidyr::pivot_wider(\n    names_from = z,            # use height (z) as new column names\n    values_from = lad,         # fill cells with LAD values\n    values_fill = 0            # fill missing voxel heights with 0 (no LAD)\n  ) %&gt;%\n  column_to_rownames(\"xy_key\") %&gt;%  # set xy_key as row names\n  as.matrix()                       # convert to numeric matrix for clustering\n\n# Cluster LAD profiles using k-means\nclustering &lt;- kmeans(lad_matrix, centers = n_clusters, nstart = 100, iter.max = 200)\nlad_df$cluster &lt;- clustering$cluster[match(lad_df$xy_key, rownames(lad_matrix))]\n\n\n\n\nEach LAD cluster is assigned a unique identifier that begins with “S” and uses base36 encoding (0–9, A–Z):\n\n#' Convert an integer to a 6-character alphanumeric ENVIMET ID (Base36)\n#'\n#' Converts an integer into a 6-character string using base-36 encoding (digits + uppercase letters),\n#' padded with leading zeros and prefixed with `\"S\"`. This is useful for assigning unique `ENVIMET_ID`s\n#' in 3D plant libraries for ENVI-met.\n#'\n#' @param n An integer (or vector of integers) to convert to alphanumeric IDs.\n#' @param width The number of base-36 digits to use (default: `5`, resulting in IDs like `\"S0000A\"`).\n#'\n#' @return A character vector of base-36 encoded IDs (with `\"S\"` prefix).\n#' @export\n#'\n#' @examples\n#' int_to_base36(1)       # \"S00001\"\n#' int_to_base36(35)      # \"S0000Z\"\n#' int_to_base36(36)      # \"S00010\"\n#' int_to_base36(123456)  # \"S02N9S\"\n#' int_to_base36(1:3)     # \"S00001\" \"S00002\" \"S00003\"\nint_to_base36 &lt;- function(n, width = 5) {\n  chars &lt;- c(0:9, LETTERS)         # Base-36 character set\n  base &lt;- length(chars)\n  result &lt;- character()\n  \n  while (n &gt; 0) {\n    result &lt;- c(chars[(n %% base) + 1], result)\n    n &lt;- n %/% base\n  }\n  \n  result &lt;- paste(result, collapse = \"\")\n  \n  # Pad with leading zeros if necessary\n  padded &lt;- sprintf(paste0(\"%0\", width, \"s\"), result)\n  \n  # Replace any spaces with \"0\" and prefix with \"S\"\n  paste0(\"S\", substr(gsub(\" \", \"0\", padded), 1, width))\n}\n\n\n\n\nEach unique LAD column becomes a point in a GeoPackage, tagged with its ENVIMET_ID.\n\n# Assign ENVIMET_IDs per cluster\ncluster_ids &lt;- unique(lad_df$cluster)\ncluster_mapping &lt;- data.frame(\n  cluster = cluster_ids,\n  ENVIMET_ID = sapply(cluster_ids, int_to_base36)\n)\nlad_df &lt;- left_join(lad_df, cluster_mapping, by = \"cluster\")\n\nsf_points &lt;- st_as_sf(point_df, coords = c(\"x\", \"y\"), crs = crs_code)\nst_write(sf_points, output_gpkg, delete_layer = TRUE)\n\n\n\n\nThe LAD profile per cluster is exported to a .pld file using XML.\n\nexport_lad_to_envimet_p3d(\n  lad_df = lad_clustered, \n  file_out = output_envimet_als_3d\n)"
  },
  {
    "objectID": "doc/tls_v1_3.html#concept-of-pseudo-3d-tree-columns",
    "href": "doc/tls_v1_3.html#concept-of-pseudo-3d-tree-columns",
    "title": "Untitled",
    "section": "",
    "text": "Each clustered LAD profile is interpreted as a pseudo-3D vegetation column. These are not derived from segmented individual trees but represent aggregated vertical structure typical for a 2 × 2 m area.\nThis approach provides a balance between realism and simplicity:\n\nIt allows realistic vertical vegetation profiles from ALS\nReduces complexity through clustering\nProvides efficient integration into ENVI-met via both:\n\nGIS point layers with ENVIMET_ID\nXML-based 3DPLANT definitions\n\n\nPseudo-3D trees enable realistic microclimate domains with vegetation heterogeneity without requiring full 3D reconstruction."
  },
  {
    "objectID": "doc/tls_v1_3.html#tls-vs-als-lad-computation-summary",
    "href": "doc/tls_v1_3.html#tls-vs-als-lad-computation-summary",
    "title": "Untitled",
    "section": "",
    "text": "Aspect\nTLS\nALS\n\n\n\n\nView Direction\nBottom-up\nTop-down\n\n\nOcclusion Bias\nUndersamples upper canopy\nUndersamples lower canopy\n\n\nLAD Estimation\nCumulative bottom-up (Beer)\nNormalized per column (max count)\n\n\nTypical Use Case\nDetailed single tree analysis\nLarge-area structure sampling"
  },
  {
    "objectID": "doc/tls_v1_3.html#conclusion-and-limitations",
    "href": "doc/tls_v1_3.html#conclusion-and-limitations",
    "title": "Untitled",
    "section": "",
    "text": "This pipeline offers an efficient method to integrate voxelized ALS data into ENVI-met’s 3DPLANT framework by:\n\nEstimating LAD profiles via a Beer–Lambert-based approximation\nClustering voxel columns into representative pseudo-3D vegetation types\nExporting both point geometries and XML-based plant profiles\n\nAdvantages: - Scalable to large ALS datasets - Preserves key structural heterogeneity - Compatible with ENVI-met simulation domains\nLimitations: - Assumes that the maximum voxel return represents full canopy cover, which may not hold in sparse stands - LAD estimation is empirical; it does not model true light attenuation or occlusion - The pseudo-3D approach does not represent individual trees or crown geometry - Clustering may smooth out fine-scale vertical variability\nFuture improvements could include stratified LAD normalization, occlusion-aware corrections, or hybrid ALS-TLS fusion for enhanced realism."
  },
  {
    "objectID": "doc/tls_v1_3.html#references",
    "href": "doc/tls_v1_3.html#references",
    "title": "Untitled",
    "section": "",
    "text": "Béland, M., et al. (2014). Remote Sensing of Environment\nCalders, K., et al. (2015). Methods in Ecology and Evolution\nJupp, D. L. B., et al. (2009). Remote Sensing of Environment"
  },
  {
    "objectID": "doc/tls_v1_3.html#script-reference",
    "href": "doc/tls_v1_3.html#script-reference",
    "title": "Untitled",
    "section": "",
    "text": "source(\"src/microclimate_ALS.R\")\nThis source contains the complete processing workflow from voxel metrics to XML generation."
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html",
    "href": "doc/microclimate_ALS_tc_v4.html",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "",
    "text": "This tutorial guides you through the complete workflow to derive vegetation structure information from Airborne Laser Scanning (ALS) data and export it into a format compatible with ENVI-met’s 3DPLANT system.\nWe cover:\n\nMerging and preprocessing ALS point clouds\nGenerating digital terrain and surface models\nVoxelizing ALS data and calculating Leaf Area Density (LAD)\nDeriving ecological and structural metrics\nClustering tree profiles into generalized types\nExporting synthetic 3D tree definitions (.pld XML) and their locations (.gpkg)",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#introduction",
    "href": "doc/microclimate_ALS_tc_v4.html#introduction",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "",
    "text": "This tutorial guides you through the complete workflow to derive vegetation structure information from Airborne Laser Scanning (ALS) data and export it into a format compatible with ENVI-met’s 3DPLANT system.\nWe cover:\n\nMerging and preprocessing ALS point clouds\nGenerating digital terrain and surface models\nVoxelizing ALS data and calculating Leaf Area Density (LAD)\nDeriving ecological and structural metrics\nClustering tree profiles into generalized types\nExporting synthetic 3D tree definitions (.pld XML) and their locations (.gpkg)",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#setup-and-configuration",
    "href": "doc/microclimate_ALS_tc_v4.html#setup-and-configuration",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "1. Setup and Configuration",
    "text": "1. Setup and Configuration\nWe begin by loading all required libraries and defining key input/output paths and parameters.\n# Load necessary R packages\nlibrary(lidR)       # for LiDAR handling\nlibrary(terra)      # for raster operations\nlibrary(dplyr)      # data wrangling\nlibrary(tidyr)      # reshaping\nlibrary(sf)         # spatial data (simple features)\nlibrary(here)       # file path handling\nlibrary(XML)        # export to ENVI-met XML\nlibrary(clusternomics) # clustering\n...\nWe also define which species classes are considered valid and load a species raster map for later assignment.",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#als-preprocessing-and-normalization",
    "href": "doc/microclimate_ALS_tc_v4.html#als-preprocessing-and-normalization",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "2. ALS Preprocessing and Normalization",
    "text": "2. ALS Preprocessing and Normalization\nFirst, the LAS tiles are merged and their heights normalized using a DEM derived from the ground classification.\nlas_fn &lt;- merge_las_tiles(...)  # Merges all LAS tiles into one\nlas &lt;- readLAS(las_fn)\nlas &lt;- classify_ground(las, csf(...))  # Adaptive Cloth Simulation Filter\n...\nThe point cloud is then normalized to height above ground using normalize_height().",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#terrain-and-canopy-metrics",
    "href": "doc/microclimate_ALS_tc_v4.html#terrain-and-canopy-metrics",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "3. Terrain and Canopy Metrics",
    "text": "3. Terrain and Canopy Metrics\nWe generate various raster layers:\n\nDEM – Digital Elevation Model\nDSM – Digital Surface Model (top of canopy)\nCHM – Canopy Height Model (DSM – DEM)\nSlope, Aspect, and TPI – Topographic descriptors\n\nThese are later used to enrich each voxel with its spatial context.\nslope &lt;- terrain(dem, \"slope\")\nCHM &lt;- DSM - DEM\n...",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#voxelization-and-lad-calculation",
    "href": "doc/microclimate_ALS_tc_v4.html#voxelization-and-lad-calculation",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "4. Voxelization and LAD Calculation",
    "text": "4. Voxelization and LAD Calculation\nThe normalized ALS cloud is converted to voxels and LAD profiles are derived using a Beer–Lambert approach:\nvoxels &lt;- preprocess_voxels(las_norm, res_xy, res_z)\nlad_df &lt;- convert_to_LAD_beer(voxels, grainsize = res_z, k = 0.3)\nExplanation: The Beer–Lambert law models LAD from voxel pulse counts as an exponential decay of light through vegetation. Each voxel contributes to the vertical profile based on its pulse return density and assumed extinction coefficient.\n\n⚙️ Advanced Implementation Details\nThis pipeline uses a highly optimized voxel engine that pre-aggregates point returns into vertical bins based on user-defined voxel height (res_z).\nThe convert_to_LAD_beer() function applies:\n\nNormalization by the maximum point count per column (to estimate occlusion probability)\nBeer–Lambert transformation:\n\\[\\text{LAD}_i = -\\frac{\\ln(1 - \\frac{N_i}{N_{\\max}})}{k \\cdot \\Delta z}\\]\nWhere:\n\n\\(N_i\\) = point returns in voxel \\(i\\)\n\\(N_{\\max}\\) = max returns in vertical column\n\\(k\\) = extinction coefficient (typically 0.3–0.5)\n\\(\\Delta z\\) = voxel height (e.g. 2 m)\n\nClipping to physical LAD limits: optional thresholds (e.g., LAD_min = 0.05, LAD_max = 3.0)\nOptional scaling: using a correction factor (scale_factor) for vegetation type\n\nThis approach is robust across canopy densities and avoids bias from heterogeneous ALS sampling.",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#spatial-enrichment",
    "href": "doc/microclimate_ALS_tc_v4.html#spatial-enrichment",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "5. Spatial Enrichment",
    "text": "5. Spatial Enrichment\nEach voxel column is enriched with values from the terrain metrics (elevation, slope, CHM, etc.) and species raster via buffered spatial extraction using the exactextractr package.\nlad_df$elev &lt;- exact_extract(dem, st_buffer(...))\n...\nNote: We use small buffers to ensure overlap with raster cells, especially for coarse resolutions or near-boundary voxels.",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#structural-and-ecological-metrics",
    "href": "doc/microclimate_ALS_tc_v4.html#structural-and-ecological-metrics",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "6. Structural and Ecological Metrics",
    "text": "6. Structural and Ecological Metrics\nFrom the LAD profiles, we derive additional vegetation structure descriptors:\n\nLAD_mean / LAD_max – Central tendency\nSkewness / Kurtosis – Shape of vertical distribution\nEntropy – Evenness / complexity of LAD\nCanopy Cover, Gap Fraction – Occupancy of upper and lower canopy\nVertical Evenness – Shannon diversity of vertical distribution\n\nlad_df$LAD_mean &lt;- matrixStats::rowMeans2(lad_matrix, na.rm = TRUE)\n...\nInterpretation: These metrics help characterize different plant types and are key inputs to the clustering process.",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#clustering-and-profile-aggregation",
    "href": "doc/microclimate_ALS_tc_v4.html#clustering-and-profile-aggregation",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "7. Clustering and Profile Aggregation",
    "text": "7. Clustering and Profile Aggregation\nWe sample the LAD space, reduce dimensionality via Principal Component Analysis (PCA), and use NbClust to find the optimal number of clusters. These clusters represent generalized tree types for ENVI-met.\npca_res &lt;- prcomp(sample_data, scale. = TRUE)\nnb &lt;- NbClust(...)\nkm &lt;- KMeans_arma(...)\n\n🔍 PCA Concept and Purpose\nPCA transforms the original high-dimensional LAD profile into a set of orthogonal axes (principal components) that capture the greatest variance in the data. This has several advantages:\n\nNoise reduction: Removes minor variance and unstable features\nInterpretability: Principal axes often reflect dominant structural traits\nEfficiency: Downsamples high-dimensional LAD vectors to fewer axes (e.g., 5–10 PCs)\nClustering stability: Improves separation of groups in Euclidean space\n\nWe determine the number of components by examining explained variance and selecting a cutoff (e.g., 80%). NbClust is then applied to the PCA-reduced data to identify the optimal number of vegetation types (clusters).",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#trait-assignment-and-xml-export",
    "href": "doc/microclimate_ALS_tc_v4.html#trait-assignment-and-xml-export",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "8. Trait Assignment and XML Export",
    "text": "8. Trait Assignment and XML Export\nFor each cluster, we assign:\n\nSpecies name via majority vote\nLeafThickness via lookup table\nLAI, Crown height, Max LAD, Roughness length via structural stats\n\nThen export to .pld (ENVI-met XML format):\nexport_lad_to_envimet_p3d(lad_df, file_out = ..., trait_df = ...)\nNote: Ensure that ENVIMET_ID is uniquely assigned and used for mapping in both XML and spatial files.",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#export-tree-locations",
    "href": "doc/microclimate_ALS_tc_v4.html#export-tree-locations",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "9. Export Tree Locations",
    "text": "9. Export Tree Locations\nThe x/y locations of all LAD columns are exported to a GeoPackage with their cluster ID (ENVIMET_ID) for domain placement in ENVI-met.\nst_write(sf_points, output_gpkg)\nTip: These point geometries can be directly imported into ENVI-met Spaces or Generator.",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#summary-and-recommendations",
    "href": "doc/microclimate_ALS_tc_v4.html#summary-and-recommendations",
    "title": "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data",
    "section": "Summary and Recommendations",
    "text": "Summary and Recommendations\nThis pipeline generates highly realistic, location-aware synthetic vegetation objects based on ALS-derived LAD profiles. The final .pld and .gpkg can be directly used for the QGIS ENVI-met plugin.",
    "crumbs": [
      "UPDATE ENVI-met 3DPLANT Tutorial from ALS Data"
    ]
  },
  {
    "objectID": "base/about.html",
    "href": "base/about.html",
    "title": "About this site",
    "section": "",
    "text": "About this site\nThis page summarizes the essential workflows , basic literature and web resources from the distributed course systems , documents and field protocols into a knowledge base.\nAlthough the web space is topic-centered any keyword can be searched using the full text search.\nThe creation of new pages, the editing of existing pages can be triggered directly via the right column online.\nOffline there are several visual editors and full integration with Rstudio etc."
  },
  {
    "objectID": "base/impressum.html#content-responsibility",
    "href": "base/impressum.html#content-responsibility",
    "title": "Impressum",
    "section": "Content Responsibility",
    "text": "Content Responsibility\nThe responsibility for the content rests with the instructors. Statements, opinions and/or conclusions are the ones from the instructors and do not necessarily reflect the opinion of the representatives of Marburg University."
  },
  {
    "objectID": "base/impressum.html#content-license",
    "href": "base/impressum.html#content-license",
    "title": "Impressum",
    "section": "Content License",
    "text": "Content License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\nPrivacy Policy\n\n\nAs of 21. October 2021\n\n\nIntroduction\n\n\nWith the following data protection declaration, we would like to inform you about the types of your personal data (hereinafter also referred to as “data” for short) that we process, for what purposes and to what extent. The privacy policy applies to all processing of personal data carried out by us, both in the context of the provision of our services and in particular on our websites, in mobile applications and within external online presences, such as our social media profiles (hereinafter collectively referred to as “Online Offerings”).\n\n\nThe terms used are not gender-specific.\n\n\nResponsible\n\n\nDr Christoph ReudenbachDeutschhaustr 1035037 Marburg\n\n\nEmail address: reudenbach@uni-marburg.de.\n\n\nImprint: https://www.uni-marburg.de/de/impressum.\n\n\nOverview of Processing\n\n\nThe following overview summarizes the types of data processed and the purposes of their processing, and refers to the data subjects.\n\n\nTypes of Data Processed\n\n\n\nContent data (e.g. input in online forms).\n\n\nContact data (e.g. email, phone numbers).\n\n\nMeta/communication data (e.g. device information, IP addresses).\n\n\nUse data (e.g. websites visited, interest in content, access times).\n\n\n\nCategories of data subjects\n\n\n\nCommunication partners.\n\n\nUsers (e.g.. Website visitors, users of online services).\n\n\n\nPurposes of processing\n\n\n\nDirect marketing (e.g., by email or postal mail).\n\n\nContact requests and communications.\n\n\n\nRelevant legal basis\n\n\nThe following is an overview of the legal basis of the GDPR on the basis of which we process personal data. Please note that in addition to the provisions of the GDPR, national data protection regulations may apply in your or our country of residence or domicile. Furthermore, should more specific legal bases be decisive in individual cases, we will inform you of these in the data protection declaration.\n\n \n\n\nConsent (Art. 6 para. 1 p. 1 lit. a. DSGVO) - The data subject has given his or her consent to the processing of personal data concerning him or her for a specific purpose or purposes.\n\n\nRegistered interests (Art. 6 para. 1 p. 1 lit. f. DSGVO) - Processing is necessary to protect the legitimate interests of the controller or a third party, unless such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require the protection of personal data.\n\n\n\nNational data protection regulations in Germany: In addition to the data protection regulations of the General Data Protection Regulation, national regulations on data protection apply in Germany. These include, in particular, the Act on Protection against Misuse of Personal Data in Data Processing (Federal Data Protection Act - BDSG). In particular, the BDSG contains special regulations on the right to information, the right to erasure, the right to object, the processing of special categories of personal data, processing for other purposes and transmission, as well as automated decision-making in individual cases, including profiling. Furthermore, it regulates data processing for employment purposes (Section 26 BDSG), in particular with regard to the establishment, implementation or termination of employment relationships as well as the consent of employees. Furthermore, state data protection laws of the individual federal states may apply.\n\n \n\nSecurity measures\n\n\nWe take appropriate technical and organizational measures in accordance with the legal requirements, taking into account the state of the art, the implementation costs and the nature, scope, circumstances and purposes of the processing, as well as the different probabilities of occurrence and the extent of the threat to the rights and freedoms of natural persons, in order to ensure a level of protection appropriate to the risk.\n\n.\n\nMeasures include, in particular, ensuring the confidentiality, integrity, and availability of data by controlling physical and electronic access to data as well as access to, entry into, disclosure of, assurance of availability of, and segregation of data concerning them. Furthermore, we have established procedures to ensure the exercise of data subjects’ rights, the deletion of data, and responses to data compromise. Furthermore, we take the protection of personal data into account as early as the development or selection of hardware, software as well as procedures in accordance with the principle of data protection, through technology design and through data protection-friendly default settings.\n\n \n\nDeletion of data\n\n\nThe data processed by us will be deleted in accordance with legal requirements as soon as their consents permitted for processing are revoked or other permissions cease to apply (e.g. if the purpose of processing this data has ceased to apply or it is not necessary for the purpose).\n\n \n\nIf the data are not deleted because they are required for other and legally permissible purposes, their processing will be limited to these purposes. That is, the data will be blocked and not processed for other purposes. This applies, for example, to data that must be retained for reasons of commercial or tax law or whose storage is necessary for the assertion, exercise or defense of legal claims or for the protection of the rights of another natural person or legal entity.\n\n \n\nOur privacy notices may also include further information on the retention and deletion of data that takes precedence for the processing operations in question.\n\n \n\nUse of cookies\n\n\nCookies are text files that contain data from websites or domains visited and are stored by a browser on the user’s computer. The primary purpose of a cookie is to store information about a user during or after their visit within an online site. Stored information may include, for example, language settings on a website, login status, a shopping cart, or where a video was watched. We further include in the term cookies other technologies that perform the same functions as cookies (e.g., when user details are stored using pseudonymous online identifiers, also referred to as “user IDs”)\n\n.\n\nThe following cookie types and functions are distinguished:\n\n\n\nTemporary cookies (also: session or session cookies): Temporary cookies are deleted at the latest after a user has left an online offer and closed his browser.\n\n\nPermanent cookies: Permanent cookies remain stored even after closing the browser. For example, the login status can be saved or preferred content can be displayed directly when the user revisits a website. Likewise, the interests of users used for range measurement or marketing purposes can be stored in such a cookie.\n\n\nFirst-party cookies: First-party cookies are set by ourselves.\n\n\nThird-party cookies (also: third-party cookies): Third-party cookies are mainly used by advertisers (so-called third parties) to process user information.\n\n\nNecessary (also: essential or absolutely necessary) cookies: Cookies may be absolutely necessary for the operation of a website (e.g. to store logins or other user input or for security reasons).\n\n\nStatistics, marketing and personalization cookies: Furthermore, cookies are usually also used in the context of range measurement and when the interests of a user or his behavior (e.g. viewing certain content, use of functions, etc.) on individual web pages are stored in a user profile. Such profiles are used, for example, to show users content that matches their potential interests. This process is also referred to as “tracking”, i.e., tracking the potential interests of users. Insofar as we use cookies or “tracking” technologies, we will inform you separately in our privacy policy or in the context of obtaining consent.\n\n\n\nNotes on legal bases: On which legal basis we process your personal data using cookies depends on whether we ask you for consent. If this is the case and you consent to the use of cookies, the legal basis for the processing of your data is the declared consent. Otherwise, the data processed with the help of cookies is processed on the basis of our legitimate interests (e.g. in a business operation of our online offer and its improvement) or, if the use of cookies is necessary to fulfill our contractual obligations.\n\n.\n\nDuration of storage: If we do not provide you with explicit information about the storage period of permanent cookies (e.g. in the context of a so-called cookie opt-in), please assume that the storage period can be up to two years.\n\n.\n\nGeneral information on revocation and objection (opt-out):  Depending on whether the processing is based on consent or legal permission, you have the option at any time to revoke any consent given or to object to the processing of your data by cookie technologies (collectively referred to as “opt-out”). You can initially declare your objection by means of your browser settings, e.g. by deactivating the use of cookies (whereby this may also restrict the functionality of our online offer). An objection to the use of cookies for online marketing purposes can also be declared by means of a variety of services, especially in the case of tracking, via the websites https://optout.aboutads.info and https://www.youronlinechoices.com/. In addition, you can receive further objection notices in the context of the information on the service providers and cookies used.\n\n.\n\nProcessing of cookie data on the basis of consent: We use a cookie consent management procedure, in the context of which the consent of users to the use of cookies, or the processing and providers mentioned in the cookie consent management procedure can be obtained and managed and revoked by users. Here, the declaration of consent is stored in order not to have to repeat its query and to be able to prove the consent in accordance with the legal obligation. The storage can take place on the server side and/or in a cookie (so-called opt-in cookie, or with the help of comparable technologies), in order to be able to assign the consent to a user or their device. Subject to individual information on the providers of cookie management services, the following information applies: The duration of the storage of consent can be up to two years. Here, a pseudonymous user identifier is formed and stored with the time of consent, information on the scope of consent (e.g., which categories of cookies and/or service providers) as well as the browser, system and end device used.\n\n.\n\n\nTypes of data processed: Usage data (e.g. websites visited, interest in content, access times), meta/communication data (e.g. device information, IP addresses).\n\n\nPersons concerned: Users (e.g. website visitors, users of online services).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nSurveys and polls\n\n\nThe surveys and polls (hereinafter “surveys”) conducted by us are evaluated anonymously. Personal data is only processed insofar as this is necessary for the provision and technical implementation of the surveys (e.g. processing of the IP address to display the survey in the user’s browser or to enable a resumption of the survey with the help of a temporary cookie (session cookie)) or users have consented.\n\n.\n\nNotes on legal basis: If we ask participants for consent to process their data, this is the legal basis of the processing, otherwise the processing of participants’ data is based on our legitimate interests in conducting an objective survey.\n\n \n\n\nTypes of data processed: Contact data (e.g. email, phone numbers), content data (e.g. input in online forms), usage data (e.g. web pages visited, interest in content, access times), meta/communication data (e.g. device information, IP addresses).\n\n\nParticipants concerned: Communication partners.\n\n\nPurposes of processing: Contact requests and communication, direct marketing (e.g. by e-mail or postal mail).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nChange and Update Privacy Policy\n\n\nWe encourage you to periodically review the contents of our Privacy Policy. We adapt the Privacy Policy as soon as the changes in the data processing activities we carry out make it necessary. We will inform you as soon as the changes require an act of cooperation on your part (e.g. consent) or other individual notification.\n\n.\n\nWhere we provide addresses and contact information for companies and organizations in this Privacy Policy, please note that addresses may change over time and please check the information before contacting us.\n\n.\n\nRights of data subjects\n\n\nAs a data subject, you are entitled to various rights under the GDPR, which arise in particular from Art. 15 to 21 DSGVO:\n\n\n\nRight to object: You have the right to object at any time, on grounds relating to your particular situation, to the processing of personal data relating to you which is carried out on the basis of Art. 6(1)(e) or (f) DSGVO; this also applies to profiling based on these provisions. If the personal data concerning you is processed for the purpose of direct marketing, you have the right to object at any time to the processing of personal data concerning you for the purpose of such marketing; this also applies to profiling, insofar as it is associated with such direct marketing.\n\n\nRight of withdrawal in the case of consent: You have the right to withdraw any consent you have given at any time.\n\n\nRight of access: You have the right to request confirmation as to whether data in question is being processed and to information about this data, as well as further information and copy of the data in accordance with the legal requirements.\n\n\nRight of rectification: You have the right, in accordance with the legal requirements, to request the completion of the data concerning you or the correction of incorrect data concerning you.\n\n\nRight to erasure and restriction of processing: You have, in accordance with the law, the right to request that data concerning you be erased without undue delay, or alternatively, in accordance with the law, to request restriction of the processing of the data.\n\n\nRight to data portability: You have the right to receive data concerning you, which you have provided to us, in a structured, common and machine-readable format in accordance with the legal requirements, or to demand its transfer to another responsible party.\n\n\nComplaint to supervisory authority: Without prejudice to any other administrative or judicial remedy, you have the right to lodge a complaint with a supervisory authority, in particular in the Member State of your habitual residence, place of work or the place of the alleged infringement, if you consider that the processing of personal data concerning you infringes the requirements of the GDPR.\n\n\n.\n\nDefinitions of Terms\n\n\nThis section provides you with an overview of the terms used in this Privacy Policy. Many of the terms are taken from the law and defined primarily in Article 4 of the GDPR. The legal definitions are binding. The following explanations, on the other hand, are primarily intended to aid understanding. The terms are sorted alphabetically.\n\n \n\n\nPersonal data: “Personal data” means any information relating to an identified or identifiable natural person (hereinafter “data subject”); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier (eg. e.g. cookie) or to one or more special characteristics that are an expression of the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.\n\n\nController: The “controller” is the natural or legal person, public authority, agency or other body which alone or jointly with others determines the purposes and means of the processing of personal data.\n\n\nProcessing: “Processing” means any operation or set of operations which is performed upon personal data, whether or not by automatic means. The term is broad and includes virtually any handling of data, whether collecting, evaluating, storing, transmitting or deleting.\n\n\n\nCreated with free Datenschutz-Generator.de by Dr. Thomas Schwenke"
  },
  {
    "objectID": "base/impressum.html#comments-suggestions",
    "href": "base/impressum.html#comments-suggestions",
    "title": "Impressum",
    "section": "Comments & Suggestions",
    "text": "Comments & Suggestions"
  },
  {
    "objectID": "modeling/modeling-material.html",
    "href": "modeling/modeling-material.html",
    "title": "Data and Software",
    "section": "",
    "text": "Please find all Data Downloads at the Course Data Server Data folder for any file exchange and data related purposes."
  },
  {
    "objectID": "modeling/modeling-material.html#data-set-for-training-purposes",
    "href": "modeling/modeling-material.html#data-set-for-training-purposes",
    "title": "Data and Software",
    "section": "",
    "text": "Please find all Data Downloads at the Course Data Server Data folder for any file exchange and data related purposes."
  },
  {
    "objectID": "modeling/modeling-material.html#specific-modeling-software",
    "href": "modeling/modeling-material.html#specific-modeling-software",
    "title": "Data and Software",
    "section": "Specific modeling software",
    "text": "Specific modeling software\nPlease find all Downloads according to ENVI-met at the ENVI-met landing page"
  },
  {
    "objectID": "modeling/modeling-material.html#common-software",
    "href": "modeling/modeling-material.html#common-software",
    "title": "Data and Software",
    "section": "Common Software",
    "text": "Common Software\nShell — any command line environment will do for the exercises. For Linux we recommend the bash shell. For Windows the Windows command line can be used.\n\nQGIS has become one of the most promising and most integrative open source GIS systems over the last years. Through the processing plugin, it additionally integrates modules from the other leading free GIS solutions. We will need it (if necessary) to prepare or manipulate some of the data.\n\nRegarding installation, for Ubuntu Linux, the Ubuntu GIS package is a good choice. For Windows, we strongly recommend installing everything via the OSGeo4W environment and not the standalone QGIS installation tool.\nFor most of the data pre and postprocessing, we will use the statistical scripting language R and we highly recommend the Rstudio integrated developing environment.\nPlease follow the instructions according to your operating system."
  },
  {
    "objectID": "modeling/modeling-material.html#additional-data-sources",
    "href": "modeling/modeling-material.html#additional-data-sources",
    "title": "Data and Software",
    "section": "Additional data sources",
    "text": "Additional data sources\nYou can find information about the “Zukunftsquartier Hasenkopf” in various places:\n\nCity of Marburg Zukunftsquartier Hasenkopf\nSlides including the winning design\nCitizens’ initiative wirsindhasenkopf Flyer and here “We argue”\nEnvi-met Hasenkopf GIS and Modeldata\nDigital Elevation Model DEM and Digital Surface Model DSM files relevant for Marburg can be downloaded from the GDS website of the Hessian Administration for Land Management and Geoinformation\nFor downloading the OSM data it is recommended to use the OSMDownloader extension to QGIS. It simply provides the ability to draw a rectangle and download the complete and currently available OSM data to a file named hasenkopf.osm.\nIf the data has to be digitised manually, it is advisable to use an up-to-date aerial photograph from Bing or Google. These can be easily integrated via XYZ tiles\nThe planning data for the development and sealing were taken from page 23 of the presentation of the winning design via screenshot."
  }
]