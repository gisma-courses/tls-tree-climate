[
  {
    "objectID": "modeling/script-00-interpolation.html",
    "href": "modeling/script-00-interpolation.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Script: Spatial Interpolation\n\n#------------------------------------------------------------------------------\n# Name: FR_soilmoist.R\n# Type: control script \n# Author: Chris Reudenbach, creuden@gmail.com\n# Description:  calculates the soil moisture from Lacanau point data\n# Copyright:GPL (&gt;= 3) \n# Date: 2022-11-10 \n# V-2022-11-12; \n#------------------------------------------------------------------------------\n# 0 - project setup\n#------------------------------------------------------------------------------\n# geoAI course basic setup\n# Type: script\n# Name: geoAI_setup.R\n# Author: Chris Reudenbach, creuden@gmail.com\n# Description:  create/read project folder structure and returns pathes as list\n#               load all necessary packages \n#               sources all functions in a defined function folder\n# Dependencies:   \n# Output: list containing the folder strings as shortcuts\n# Copyright: Chris Reudenbach, thomas Nauss 2019-2021, GPL (&gt;= 3)\n# git clone https://github.com/gisma-courses/geoAI-scripts.git\n#------------------------------------------------------------------------------\n\n\n\n# basic packages\nlibrary(\"mapview\")\nlibrary(\"tmap\")\nlibrary(\"tmaptools\")\nlibrary(\"raster\")\nlibrary(\"terra\")\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"lidR\")\nlibrary(\"future\")\nlibrary(\"lwgeom\")\nlibrary(\"tmap\")\nlibrary(\"mapview\")\nlibrary(rprojroot)\n\nroot_folder = find_rstudio_root_file()\n#root_folder = getwd()\nndvi.col = function(n) {\n  rev(colorspace::sequential_hcl(n, \"Green-Yellow\"))\n}\n\nano.col = colorspace::diverging_hcl(7, palette = \"Red-Green\",  register = \"rg\")\n\n\n\n\n# # suppres gdal warnings\n# rgdal::set_thin_PROJ6_warnings(TRUE)\n# \n# \n# \n# # workaround subfolder\n# loc_name = \"harz\"\n# \n# # harz\n# epsg=25833\n# \n# attributename = c(\"Moisture_1_17Nov\",\"Moisture_2_17Nov\",\"Moisture_1_19Nov\",\"Moisture_2_19Nov\")\n# varname = c(\"soilmoist2022_08_17\",\"soilmoist2022_08_19\")\n# fnDTM = \"DTM_v3.vrt\"\n# fnsm_data = \"lacanau_moisture_measurements.csv\"\n# fnpos_data= \"ltrees.gpkg\"\n# \n# # read DTM\n# DTM = terra::rast(fnDTM)\n# # cast to SpatialPixelsDataFrame\n# DTM.spdf &lt;- as(raster(DTM),\n#                        'SpatialPixelsDataFrame')\n# colnames(DTM.spdf@data) &lt;- \"altitude\"\n# # read moist data \n# sm=read.csv2(fnsm_data,sep = \",\")\n# # read tree data\n# pos=st_read(fnpos_data)\n# # merge\n# sm$Point = paste0(\"TREE\",str_split_fixed(sm$TargetID, \"_\", 3)[,3])\n# m=merge(pos,sm)\n# \n# # extract altitudes for positions\n# em= exactextractr::exact_extract(DTM,st_buffer(m,1),\"mean\")\n# m$altitude=em\n# \n# # start kriging \n# for (i in 1:length(varname) ){\n#   z=i*2\n#   # mean\n#   m$var = (as.numeric(m[[attributename[z-1]]]) + as.numeric(m[[attributename[z]]]))/2\n#   # to sp\n#   m2 = as(m,\"Spatial\")    \n#   tm2 = spTransform(m2,\n#                     crs(\"+proj=utm +zone=30 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\"))\n# \n#   # autofit variogramm for kriging \n#   vm.auto = automap::autofitVariogram(formula = as.formula(paste(\"var\", \"~ 1\")),\n#                                       input_data = tm2)\n#   plot(vm.auto)\n#   \n#   # kriging   \n#   print(paste0(\"kriging \", varname[i]))\n#   var.pred &lt;- gstat::krige(formula = as.formula(paste(\"var\", \"~ altitude\")),\n#                            locations = tm2,\n#                            newdata = DTM.spdf,\n#                            model = vm.auto$var_model,\n#                            debug.level=0,)\n#   \n#   r=rasterFromXYZ(as.data.frame(var.pred)[, c(\"x\", \"y\", \"var1.pred\")])\n#   \n#   # reclassify erratic values \n#   reclass_df &lt;- c(-Inf, 0, NA)\n#   # reshape the object into a matrix with columns and rows\n#   reclass_m &lt;- matrix(reclass_df,\n#                       ncol = 3,\n#                       byrow = TRUE)\n#   r_c &lt;- reclassify(r,reclass_m)\n# \n#   plot(r_c)\n#   # re assign crs\n#   crs(r_c) = crs(paste0(\"EPSG:\",epsg))\n#   raster::writeRaster(r_c,paste0(\"data/gis/France_Lacanau_PP_Gis/data_lev0/\",varname[i],\".tif\"),overwrite=TRUE)\n#   \n# }"
  },
  {
    "objectID": "modeling/qgis-tutorials.html",
    "href": "modeling/qgis-tutorials.html",
    "title": "Data and Software",
    "section": "",
    "text": "Please find all Data Downloads at theCourse Data Server Data folder for any file exchange and data related purposes."
  },
  {
    "objectID": "modeling/qgis-tutorials.html#data-set-for-training-purposes",
    "href": "modeling/qgis-tutorials.html#data-set-for-training-purposes",
    "title": "Data and Software",
    "section": "",
    "text": "Please find all Data Downloads at theCourse Data Server Data folder for any file exchange and data related purposes."
  },
  {
    "objectID": "modeling/qgis-tutorials.html#specific-modeling-software",
    "href": "modeling/qgis-tutorials.html#specific-modeling-software",
    "title": "Data and Software",
    "section": "Specific modeling software",
    "text": "Specific modeling software\nPlease find all Downloads according to ENVI-met at the ENVI-met landing page"
  },
  {
    "objectID": "modeling/qgis-tutorials.html#common-software",
    "href": "modeling/qgis-tutorials.html#common-software",
    "title": "Data and Software",
    "section": "Common Software",
    "text": "Common Software\nShell — any command line environment will do for the exercises. For Linux we recommend the bash shell. For Windows the Windows command line can be used.\n\nQGIS has become one of the most promising and most integrative open source GIS systems over the last years. Through the processing plugin, it additionally integrates modules from the other leading free GIS solutions. We will need it (if necessary) to prepare or manipulate some of the data.\n\nRegarding installation, for Ubuntu Linux, the Ubuntu GIS package is a good choice. For Windows, we strongly recommend installing everything via the OSGeo4W environment and not the standalone QGIS installation tool."
  },
  {
    "objectID": "modeling/qgis-tutorials.html#additional-data-sources",
    "href": "modeling/qgis-tutorials.html#additional-data-sources",
    "title": "Data and Software",
    "section": "Additional data sources",
    "text": "Additional data sources"
  },
  {
    "objectID": "base/faq.html",
    "href": "base/faq.html",
    "title": "Frequently asked Questions",
    "section": "",
    "text": "This is a senseless question to meet a meaningfull answer\n\n\n\n\n\n\n\n\n\nThis is a meaningful answer to a senseless question\n\n\n\n\n\n\n\n\n\nLearn More…\n\n\n\n\n\nThis is a even more meaningful answer to a senseless question"
  },
  {
    "objectID": "base/faq.html#make-sense-topic",
    "href": "base/faq.html#make-sense-topic",
    "title": "Frequently asked Questions",
    "section": "",
    "text": "This is a senseless question to meet a meaningfull answer\n\n\n\n\n\n\n\n\n\nThis is a meaningful answer to a senseless question\n\n\n\n\n\n\n\n\n\nLearn More…\n\n\n\n\n\nThis is a even more meaningful answer to a senseless question"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Goals",
    "section": "",
    "text": "Evaluate atmosphere–vegetation models and their structure feedbacks\nAnalyze surface energy fluxes to understand vertical climate profiles\nExtract tree structure from TLS and ALS point clouds\nClassify and correct species distributions\nTransform OSM maps into simulation-ready domains\n\n\n\n\nThis site uses utterances for comments via GitHub. To leave a comment, simply sign in with your GitHub account. If you don’t have one, you can create a free account here.\nYour comment will be publicly stored and appear directly below. No trackers or third-party cookies are used.\nNeed help? What is utterances and how does it work?"
  },
  {
    "objectID": "index.html#this-course-explores-how-trees-shape-climate-through-structure-species-and-energy-flow.-youll-work-with-laser-scans-land-data-and-physical-models-to-reveal-patterns-beneath-the-canopy",
    "href": "index.html#this-course-explores-how-trees-shape-climate-through-structure-species-and-energy-flow.-youll-work-with-laser-scans-land-data-and-physical-models-to-reveal-patterns-beneath-the-canopy",
    "title": "Goals",
    "section": "",
    "text": "Evaluate atmosphere–vegetation models and their structure feedbacks\nAnalyze surface energy fluxes to understand vertical climate profiles\nExtract tree structure from TLS and ALS point clouds\nClassify and correct species distributions\nTransform OSM maps into simulation-ready domains\n\n\n\n\nThis site uses utterances for comments via GitHub. To leave a comment, simply sign in with your GitHub account. If you don’t have one, you can create a free account here.\nYour comment will be publicly stored and appear directly below. No trackers or third-party cookies are used.\nNeed help? What is utterances and how does it work?"
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html",
    "href": "doc/microclimate_ALS_tc_v4.html",
    "title": "ENVI-met 3DPLANTs from ALS Data",
    "section": "",
    "text": "This tutorial guides you through the complete workflow to derive vegetation structure information from Airborne Laser Scanning (ALS) data and export it into a format compatible with ENVI-met’s 3DPLANT system.\nWe cover:\n\nMerging and preprocessing ALS point clouds\nGenerating digital terrain and surface models\nVoxelizing ALS data and calculating Leaf Area Density (LAD)\nDeriving ecological and structural metrics\nClustering tree profiles into generalized types\nExporting synthetic 3D tree definitions (.pld XML) and their locations (.gpkg)",
    "crumbs": [
      "ENVI-met 3DPLANTs from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#introduction",
    "href": "doc/microclimate_ALS_tc_v4.html#introduction",
    "title": "ENVI-met 3DPLANTs from ALS Data",
    "section": "",
    "text": "This tutorial guides you through the complete workflow to derive vegetation structure information from Airborne Laser Scanning (ALS) data and export it into a format compatible with ENVI-met’s 3DPLANT system.\nWe cover:\n\nMerging and preprocessing ALS point clouds\nGenerating digital terrain and surface models\nVoxelizing ALS data and calculating Leaf Area Density (LAD)\nDeriving ecological and structural metrics\nClustering tree profiles into generalized types\nExporting synthetic 3D tree definitions (.pld XML) and their locations (.gpkg)",
    "crumbs": [
      "ENVI-met 3DPLANTs from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#workflow-overview",
    "href": "doc/microclimate_ALS_tc_v4.html#workflow-overview",
    "title": "ENVI-met 3DPLANTs from ALS Data",
    "section": "Workflow Overview",
    "text": "Workflow Overview\n\n\n\n\n\nflowchart TD\n  A[LAS tiles] --&gt; B[Merge + Normalize]\n  B --&gt; C[DEM, CHM, DSM]\n  C --&gt; D[Topographic indices]\n  B --&gt; E[Voxelize + LAD via Beer–Lambert]\n  D --&gt; F[Add species + topography to LAD]\n  E --&gt; F\n  F --&gt; G[PCA + NbClust]\n  G --&gt; H[KMeans clustering]\n  H --&gt; I[Aggregate profiles]\n  I --&gt; J[Compute traits]\n  J --&gt; K[Export .pld XML]\n  J --&gt; L[Export GPKG positions]\n\n\n\n\n\n\n\nSetup and Configuration\nWe begin by loading all required libraries and defining key input/output paths and parameters.\n# Load necessary R packages\nlibrary(lidR)       # for LiDAR handling\nlibrary(terra)      # for raster operations\nlibrary(dplyr)      # data wrangling\nlibrary(tidyr)      # reshaping\nlibrary(sf)         # spatial data (simple features)\nlibrary(here)       # file path handling\nlibrary(XML)        # export to ENVI-met XML\nlibrary(clusternomics) # clustering\n...\nWe also define which species classes are considered valid and load a species raster map for later assignment.\n\n\nALS Preprocessing and Normalization\nFirst, the LAS tiles are merged and their heights normalized using a DEM derived from the ground classification.\nlas_fn &lt;- merge_las_tiles(...)  # Merges all LAS tiles into one\nlas &lt;- readLAS(las_fn)\nlas &lt;- classify_ground(las, csf(...))  # Adaptive Cloth Simulation Filter\n...\nThe point cloud is then normalized to height above ground using normalize_height().\n\n\nTerrain and Canopy Metrics\nWe generate various raster layers:\n\nDEM – Digital Elevation Model\nDSM – Digital Surface Model (top of canopy)\nCHM – Canopy Height Model (DSM – DEM)\nSlope, Aspect, and TPI – Topographic descriptors\n\nThese are later used to enrich each voxel with its spatial context.\nslope &lt;- terrain(dem, \"slope\")\nCHM &lt;- DSM - DEM\n...\n\n\nVoxelization and LAD Calculation\nThe normalized ALS cloud is converted to voxels and LAD profiles are derived using a Beer–Lambert approach:\nvoxels &lt;- preprocess_voxels(las_norm, res_xy, res_z)\nlad_df &lt;- convert_to_LAD_beer(voxels, grainsize = res_z, k = 0.3)\nExplanation: The Beer–Lambert law models LAD from voxel pulse counts as an exponential decay of light through vegetation. Each voxel contributes to the vertical profile based on its pulse return density and assumed extinction coefficient.\n\nAdvanced Implementation Details\nThis pipeline uses a highly optimized voxel engine that pre-aggregates point returns into vertical bins based on user-defined voxel height (res_z).\nThe convert_to_LAD_beer() function applies:\n\nNormalization by the maximum point count per column (to estimate occlusion probability)\nBeer–Lambert transformation:\n\\[\\text{LAD}_i = -\\frac{\\ln(1 - \\frac{N_i}{N_{\\max}})}{k \\cdot \\Delta z}\\]\nWhere:\n\n\\(N_i\\) = point returns in voxel \\(i\\)\n\\(N_{\\max}\\) = max returns in vertical column\n\\(k\\) = extinction coefficient (typically 0.3–0.5)\n\\(\\Delta z\\) = voxel height (e.g. 2 m)\n\nClipping to physical LAD limits: optional thresholds (e.g., LAD_min = 0.05, LAD_max = 3.0)\nOptional scaling: using a correction factor (scale_factor) for vegetation type\n\nThis approach is robust across canopy densities and avoids bias from heterogeneous ALS sampling.\n\n\nSpatial Enrichment\nEach voxel column is enriched with values from the terrain metrics (elevation, slope, CHM, etc.) and species raster via buffered spatial extraction using the exactextractr package.\nlad_df$elev &lt;- exact_extract(dem, st_buffer(...))\n...\nNote: We use small buffers to ensure overlap with raster cells, especially for coarse resolutions or near-boundary voxels.",
    "crumbs": [
      "ENVI-met 3DPLANTs from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#structural-and-ecological-metrics",
    "href": "doc/microclimate_ALS_tc_v4.html#structural-and-ecological-metrics",
    "title": "ENVI-met 3DPLANTs from ALS Data",
    "section": "Structural and Ecological Metrics",
    "text": "Structural and Ecological Metrics\nFrom the LAD profiles, we derive additional vegetation structure descriptors:\n\nLAD_mean / LAD_max – Central tendency\nSkewness / Kurtosis – Shape of vertical distribution\nEntropy – Evenness / complexity of LAD\nCanopy Cover, Gap Fraction – Occupancy of upper and lower canopy\nVertical Evenness – Shannon diversity of vertical distribution\n\nlad_df$LAD_mean &lt;- matrixStats::rowMeans2(lad_matrix, na.rm = TRUE)\n...\nInterpretation: These metrics help characterize different plant types and are key inputs to the clustering process.\n\nClustering and Profile Aggregation\nWe sample the LAD space, reduce dimensionality via Principal Component Analysis (PCA), and use NbClust to find the optimal number of clusters. These clusters represent generalized tree types for ENVI-met.\npca_res &lt;- prcomp(sample_data, scale. = TRUE)\nnb &lt;- NbClust(...)\nkm &lt;- KMeans_arma(...)\n\nPCA Concept and Purpose\nPCA transforms the original high-dimensional LAD profile into a set of orthogonal axes (principal components) that capture the greatest variance in the data. This has several advantages:\n\nNoise reduction: Removes minor variance and unstable features\nInterpretability: Principal axes often reflect dominant structural traits\nEfficiency: Downsamples high-dimensional LAD vectors to fewer axes (e.g., 5–10 PCs)\nClustering stability: Improves separation of groups in Euclidean space\n\nWe determine the number of components by examining explained variance and selecting a cutoff (e.g., 80%). NbClust is then applied to the PCA-reduced data to identify the optimal number of vegetation types (clusters).",
    "crumbs": [
      "ENVI-met 3DPLANTs from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#trait-assignment-and-xml-export",
    "href": "doc/microclimate_ALS_tc_v4.html#trait-assignment-and-xml-export",
    "title": "ENVI-met 3DPLANTs from ALS Data",
    "section": "Trait Assignment and XML Export",
    "text": "Trait Assignment and XML Export\nFor each cluster, we assign:\n\nSpecies name via majority vote\nLeafThickness via lookup table\nLAI, Crown height, Max LAD, Roughness length via structural stats\n\nThen export to .pld (ENVI-met XML format):\nexport_lad_to_envimet_p3d(lad_df, file_out = ..., trait_df = ...)\nNote: Ensure that ENVIMET_ID is uniquely assigned and used for mapping in both XML and spatial files.",
    "crumbs": [
      "ENVI-met 3DPLANTs from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#export-tree-locations",
    "href": "doc/microclimate_ALS_tc_v4.html#export-tree-locations",
    "title": "ENVI-met 3DPLANTs from ALS Data",
    "section": "Export Tree Locations",
    "text": "Export Tree Locations\nThe x/y locations of all LAD columns are exported to a GeoPackage with their cluster ID (ENVIMET_ID) for domain placement in ENVI-met.\nst_write(sf_points, output_gpkg)\nTip: These point geometries can be directly imported into ENVI-met Spaces or Generator.",
    "crumbs": [
      "ENVI-met 3DPLANTs from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#summary-and-recommendations",
    "href": "doc/microclimate_ALS_tc_v4.html#summary-and-recommendations",
    "title": "ENVI-met 3DPLANTs from ALS Data",
    "section": "Summary and Recommendations",
    "text": "Summary and Recommendations\nThis pipeline generates highly realistic, location-aware synthetic vegetation objects based on ALS-derived LAD profiles. The final .pld and .gpkg can be directly used for the QGIS ENVI-met plugin.",
    "crumbs": [
      "ENVI-met 3DPLANTs from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#functions-from-new_utils.r",
    "href": "doc/microclimate_ALS_tc_v4.html#functions-from-new_utils.r",
    "title": "ENVI-met 3DPLANTs from ALS Data",
    "section": "Functions from new_utils.R",
    "text": "Functions from new_utils.R\n #' Suggest Optimal Number of Principal Components\n#'\n#' This function analyzes a PCA object to determine the recommended number of \n#' principal components (PCs) to retain, using three common criteria:\n#' - Cumulative explained variance (threshold-based)\n#' - Kaiser criterion (eigenvalue &gt; 1)\n#' - Elbow method (first minimum of successive variance drops)\n#' \n#' Optionally, a diagnostic plot (scree and cumulative variance) is shown.\n#'\n#' @param pca_obj A PCA object returned by [prcomp()] or similar. Must contain `$sdev`.\n#' @param variance_cutoff Numeric; cumulative variance threshold for selecting PCs \n#'                        (default is 0.8 = 80% explained variance).\n#' @param plot Logical; if `TRUE`, a scree plot and cumulative variance plot are displayed.\n#'\n#' @return A list containing:\n#' \\describe{\n#'   \\item{n_pcs}{Number of PCs recommended based on the variance threshold.}\n#'   \\item{explained_variance}{Number of PCs needed to reach the variance cutoff.}\n#'   \\item{kaiser}{Number of PCs with eigenvalue &gt; 1 (Kaiser criterion).}\n#'   \\item{elbow}{Position of elbow point (first minimal drop in explained variance).}\n#'   \\item{info_table}{A summary table showing the values for each criterion.}\n#' }\n#'\n#' @details\n#' - **Cumulative Variance Threshold**: Retain the smallest number of components such that\n#'   the cumulative proportion of explained variance meets or exceeds `variance_cutoff`.\n#'\n#' - **Kaiser Criterion**: Retain all components with eigenvalues greater than 1. Assumes\n#'   data has been standardized (mean-centered and scaled). See Kaiser (1960).\n#'\n#' - **Elbow Method**: Finds the index where the decrease in explained variance is smallest,\n#'   i.e., where the \"knee\" or \"elbow\" appears in the scree plot.\n#'\n#' @references\n#' - Jolliffe, I. T. (2002). *Principal Component Analysis*. Springer Series in Statistics.\n#' - Kaiser, H. F. (1960). The application of electronic computers to factor analysis.\n#'   *Educational and Psychological Measurement*, 20(1), 141–151.\n#' - Cattell, R. B. (1966). The scree test for the number of factors. *Multivariate Behavioral Research*, 1(2), 245–276.\n#'\n#' @examples\n#' pca &lt;- prcomp(USArrests, scale. = TRUE)\n#' suggest_n_pcs(pca, variance_cutoff = 0.9)\n#'\n#' @export\nsuggest_n_pcs &lt;- function(pca_obj, variance_cutoff = 0.8, plot = TRUE) {\n  # Extract standard deviations of the principal components\n  std_dev &lt;- pca_obj$sdev\n  \n  # Compute proportion of variance explained by each PC\n  var_explained &lt;- std_dev^2 / sum(std_dev^2)\n  \n  # Compute cumulative explained variance\n  cum_var_explained &lt;- cumsum(var_explained)\n  \n  # Criterion 1: Number of components needed to reach variance_cutoff\n  n_var &lt;- which(cum_var_explained &gt;= variance_cutoff)[1]\n  \n  # Criterion 2: Kaiser criterion – eigenvalue &gt; 1\n  eigenvalues &lt;- std_dev^2\n  n_kaiser &lt;- sum(eigenvalues &gt; 1)\n  \n  # Criterion 3: Elbow method – where decrease in explained variance flattens\n  diffs &lt;- diff(var_explained)\n  elbow &lt;- which.min(diffs)[1]\n  \n  # Assemble criterion comparison table\n  info_table &lt;- data.frame(\n    Criterion = c(\"Variance Cutoff\", \"Kaiser Criterion\", \"Elbow Method\"),\n    Num_Components = c(n_var, n_kaiser, elbow),\n    Cumulative_Explained_Variance = c(\n      round(cum_var_explained[n_var], 3),\n      round(cum_var_explained[n_kaiser], 3),\n      round(cum_var_explained[elbow], 3)\n    )\n  )\n  \n  # Final recommendation is based on variance_cutoff only (can be modified as needed)\n  n_final &lt;- n_var\n  \n  # Print summary\n  cat(\"📊 Summary of PCA Component Selection Criteria:\\n\")\n  print(info_table, row.names = FALSE)\n  cat(\"\\n✅ Recommended number of PCs (based on variance_cutoff =\", variance_cutoff, \"):\", n_final, \"\\n\")\n  \n  # Optional plots\n  if (plot) {\n    par(mfrow = c(1, 2))\n    \n    # Scree plot: individual variance explained\n    plot(var_explained, type = \"b\", pch = 19, col = \"steelblue\",\n         xlab = \"Component\", ylab = \"Explained Variance\",\n         main = \"Scree Plot\")\n    abline(h = 1, col = \"red\", lty = 2)     # Kaiser line\n    abline(v = elbow, col = \"darkgreen\", lty = 3)  # Elbow marker\n    legend(\"topright\", legend = c(\"Kaiser (λ &gt; 1)\", \"Elbow\"),\n           col = c(\"red\", \"darkgreen\"), lty = c(2, 3), bty = \"n\")\n    \n    # Cumulative variance plot\n    plot(cum_var_explained, type = \"b\", pch = 19, col = \"darkorange\",\n         xlab = \"Component\", ylab = \"Cumulative Variance\",\n         main = \"Cumulative Explained Variance\")\n    abline(h = variance_cutoff, col = \"red\", lty = 2)\n    abline(v = n_var, col = \"blue\", lty = 3)\n    legend(\"bottomright\", legend = c(\"Cutoff\", \"Selected Components\"),\n           col = c(\"red\", \"blue\"), lty = c(2, 3), bty = \"n\")\n    \n    par(mfrow = c(1, 1))  # reset plotting layout\n  }\n  \n  # Return results silently for use in pipelines\n  invisible(list(\n    n_pcs = n_final,\n    explained_variance = n_var,\n    kaiser = n_kaiser,\n    elbow = elbow,\n    info_table = info_table\n  ))\n}\n\n\n\n#' Split Z Coordinates into Vertical Slices and Count Points per Slice\n#'\n#' This function takes a vector of Z-coordinates (heights) and bins them into\n#' 1-meter horizontal slices. It returns the count of points in each slice, ensuring that\n#' all slices from 0 to `maxZ` are represented, even if some slices have zero points.\n#'\n#' @param Z A numeric vector of Z coordinates (e.g., heights of LiDAR points in meters).\n#' @param maxZ Integer; the maximum height to consider (defines the highest slice boundary).\n#'\n#' @return A named list containing point counts per 1-meter height slice. The names are\n#' formatted for clarity (e.g., `\"ground_0_1m\"`, `\"pulses_1_2m\"`, …).\n#'\n#' @details\n#' - This is a foundational step in computing vertical vegetation structure such as\n#'   Leaf Area Density (LAD) profiles.\n#' - The slicing assumes a 1-meter vertical resolution and bins by floor(Z).\n#' - Empty slices (no points) are included with count 0 to preserve structure for later matrix assembly.\n#'\n#' @examples\n#' z_vals &lt;- runif(1000, 0, 20)\n#' pointsByZSlice(z_vals, maxZ = 20)\n#'\n#' @export\npointsByZSlice &lt;- function(Z, maxZ) {\n  # Floor Z-values to get integer bin index (0-based)\n  heightSlices &lt;- as.integer(Z)\n  \n  # Create data.table for potential grouping (not used further here)\n  zSlice &lt;- data.table::data.table(Z = Z, heightSlices = heightSlices)\n  \n  # Count number of points per height slice using base aggregate\n  sliceCount &lt;- stats::aggregate(list(V1 = Z), list(heightSlices = heightSlices), length)\n  sliceCount$V1 &lt;- as.numeric(sliceCount$V1)  # Ensure numeric (not integer or factor)\n  \n  # Ensure all expected slice bins [0, maxZ] exist (fill with 0 if missing)\n  colRange &lt;- 0:maxZ\n  missing &lt;- colRange[!colRange %in% sliceCount$heightSlices]\n  if (length(missing) &gt; 0) {\n    fill &lt;- data.frame(heightSlices = missing, V1 = as.numeric(0))\n    sliceCount &lt;- rbind(sliceCount, fill)\n  }\n  \n  # Order slices from bottom to top\n  sliceCount &lt;- sliceCount[order(sliceCount$heightSlices), ]\n  \n  # Create readable column names for each slice\n  colNames &lt;- as.character(sliceCount$heightSlices)\n  colNames[1] &lt;- \"ground_0_1m\"  # Name for the lowest bin\n  colNames[-1] &lt;- paste0(\"pulses_\", sliceCount$heightSlices[-1], \"_\", sliceCount$heightSlices[-1] + 1, \"m\")\n  \n  # Create named list of metrics\n  metrics &lt;- list()\n  metrics[colNames] &lt;- sliceCount$V1\n  \n  return(metrics)\n}\n\n\n#' Recommend DEM Interpolation Method Based on Ground Point Quality\n#'\n#' This function analyzes a LAS object or LAS file and recommends an appropriate interpolation\n#' method (`tin()`, `knnidw()`, or `kriging()`) for `lidR::rasterize_terrain()` based on\n#' ground point density, ratio, and nearest-neighbor distance.\n#'\n#' @param las A LAS object or character path to a .las/.laz file.\n#' @param res Numeric. Raster resolution (in meters) for ground point density estimation. Default is 1.\n#' @param verbose Logical. If TRUE, prints diagnostic information. Default is TRUE.\n#'\n#' @return A character string with the recommended interpolation function (e.g., `\"tin()\"`)\n#'\n#' @details\n#' This function implements a rule-based scoring system to select an appropriate terrain\n#' interpolation algorithm for `lidR::rasterize_terrain()`. The recommendation is based on:\n#'\n#' \\itemize{\n#'   \\item Ground point ratio (percentage of points classified as ground)\n#'   \\item Mean ground point density (pts/m²)\n#'   \\item Mean nearest-neighbor distance between ground points (meters)\n#' }\n#'\n#' Depending on these indicators, one of the following interpolation algorithms is suggested:\n#' \\describe{\n#'   \\item{\\code{\"tin()\"}}{Recommended when ground point distribution is dense and regular.}\n#'   \\item{\\code{\"knnidw(k = 6, p = 2)\"}}{Used under intermediate conditions with moderate density.}\n#'   \\item{\\code{\"kriging(k = 10)\"}}{Recommended for sparse or clustered ground points.}\n#' }\n#'\n#' This approach follows best practices from airborne LiDAR filtering literature, including:\n#' \\itemize{\n#'   \\item Zhang et al. (2016): Cloth Simulation Filtering (CSF) – \\doi{10.3390/rs8060501}\n#'   \\item Ma et al. (2025): Partitioned Cloth Simulation Filtering (PCSF) – \\doi{10.3390/forests16071179}\n#'   \\item Chen et al. (2024): Adaptive DEM filtering with roughness-based interpolation – \\url{https://www.sciencedirect.com/science/article/pii/S0924271624002636}\n#' }\n#'\n#' These studies suggest that interpolation performance depends strongly on the spatial characteristics\n#' of ground point clouds, especially in forested terrain. The chosen metrics are commonly used to\n#' quantify LiDAR completeness and ground visibility.\n#'\n#' @seealso \\code{\\link[lidR]{rasterize_terrain}}, \\code{\\link[lidR]{filter_ground}}, \\code{\\link[lidR]{grid_density}}\n#'\n#' @examples\n#' \\dontrun{\n#'   las &lt;- readLAS(\"data/las/forest_tile.las\")\n#'   method &lt;- recommend_dem_interpolation(las, res = 1)\n#'   dem &lt;- rasterize_terrain(las, res = 1, algorithm = eval(parse(text = method)))\n#' }\n#'\n#' @importFrom lidR readLAS filter_ground grid_density\n#' @importFrom RANN nn2\n#' @export\nrecommend_dem_interpolation &lt;- function(las, res = 1, verbose = TRUE) {\n  if (inherits(las, \"character\")) las &lt;- readLAS(las)\n  if (is.empty(las)) stop(\"LAS file is empty or invalid\")\n  \n  ground &lt;- filter_ground(las)\n  \n  cls_tab &lt;- table(las@data$Classification)\n  n_total &lt;- sum(cls_tab)\n  n_ground &lt;- if (\"2\" %in% names(cls_tab)) cls_tab[\"2\"] else 0\n  ground_pct &lt;- 100 * as.numeric(n_ground) / n_total\n  \n  density_map &lt;- grid_density(ground, res = res)\n  density_vals &lt;- values(density_map)\n  density_vals &lt;- density_vals[!is.na(density_vals)]\n  mean_density &lt;- if (length(density_vals) &gt; 0) mean(density_vals) else 0\n  \n  if (nrow(ground@data) &gt;= 2) {\n    xy &lt;- ground@data[, c(\"X\", \"Y\")]\n    nn_dist &lt;- RANN::nn2(xy, k = 2)$nn.dists[, 2]\n    mean_nn &lt;- mean(nn_dist)\n  } else {\n    mean_nn &lt;- Inf\n  }\n  \n  score &lt;- 0\n  if (ground_pct &gt; 30) score &lt;- score + 1 else if (ground_pct &lt; 10) score &lt;- score - 1\n  if (mean_density &gt; 1) score &lt;- score + 1 else if (mean_density &lt; 0.3) score &lt;- score - 1\n  if (mean_nn &lt; 1.5) score &lt;- score + 1 else if (mean_nn &gt; 3) score &lt;- score - 1\n  \n  method &lt;- if (score &gt;= 2) {\n    \"tin()\"\n  } else if (score &lt;= -1) {\n    \"kriging(k = 10)\"\n  } else {\n    \"knnidw(k = 6, p = 2)\"\n  }\n  \n  if (verbose) {\n    message(sprintf(\"📊 Ground point ratio:     %.1f%%\", ground_pct))\n    message(sprintf(\"📊 Mean ground density:   %.2f pts/m²\", mean_density))\n    message(sprintf(\"📊 Mean NN distance:      %.2f m\", mean_nn))\n    message(sprintf(\"✅ Recommended method:    %s\", method))\n  }\n  \n  return(method)\n}\n\n\n#' Merge LAS/LAZ tiles into a single file using `lidR::catalog_retile`\n#'\n#' This function merges LAS/LAZ tiles from a directory into a single file.\n#' Internally, it loads the directory as a `LAScatalog`, sets chunking to cover the entire extent,\n#' and writes a single merged `.laz` or `.las` file. Uses parallel processing if desired.\n#'\n#' @param tile_dir Path to directory containing LAS/LAZ tiles (or a single LAS/LAZ file).\n#' @param output_file Full path to the merged output file (default: `\"merged.laz\"`).\n#' @param chunk_size Optional internal chunking for processing (default: `10000` m).\n#' @param workers Number of parallel workers (default: `4`).\n#'\n#' @return Character string path to the created merged `.laz` file.\n#'\n#' @examples\n#' \\dontrun{\n#' merge_las_tiles(\"tiles/\", \"merged.laz\", workers = 6)\n#' }\n#'\n#' @export\nmerge_las_tiles &lt;- function(tile_dir,\n                            output_file = \"merged.laz\",\n                            chunk_size = 10000,\n                            workers = 4) {\n  if (!dir.exists(tile_dir) && !file.exists(tile_dir)) {\n    stop(\"Input tile directory or file does not exist.\")\n  }\n  \n  library(lidR)\n  library(future)\n  \n  set_lidr_threads(workers)\n  future::plan(multisession, workers = workers)\n  \n  ctg &lt;- readLAScatalog(tile_dir)\n  opt_chunk_size(ctg) &lt;- chunk_size\n  opt_chunk_buffer(ctg) &lt;- 0\n  opt_output_files(ctg) &lt;- sub(\"\\\\.la[sz]$\", \"\", output_file)\n  \n  message(\"Merging tiles into: \", output_file)\n  catalog_retile(ctg)  # This writes the file\n  \n  # Return final path with correct extension\n  merged_path &lt;- paste0(sub(\"\\\\.la[sz]$\", \"\", output_file), \".las\")\n  return(merged_path)\n}\n\n#' Retile a LAS/LAZ file into regular tiles\n#'\n#' This function splits a large LAS/LAZ file into regular square tiles using `lidR::catalog_retile`.\n#' It supports parallel processing and optional compression.\n#'\n#' @param input_file Path to the input LAS/LAZ file.\n#' @param out_dir Directory to write the resulting tiles (default: `\"tiles/\"`).\n#' @param chunk_size Numeric. Tile size in meters (default: `250`).\n#' @param output_ext File extension of output tiles: either `\"laz\"` or `\"las\"` (default: `\"laz\"`).\n#' @param buffer Buffer size between tiles, in meters (default: `0`, i.e. no overlap).\n#' @param workers Number of parallel workers for processing (default: `4`).\n#'\n#' @return A `LAScatalog` object referencing the tiled files.\n#'\n#' @examples\n#' \\dontrun{\n#' retile_las(\"data/input.las\", out_dir = \"tiles/\", chunk_size = 200)\n#' }\n#'\n#' @export\nretile_las &lt;- function(input_file,\n                       out_dir = \"tiles/\",\n                       chunk_size = 250,\n                       output_ext = \"laz\",\n                       buffer = 0,\n                       workers = 4) {\n  if (!file.exists(input_file)) stop(\"Input file not found.\")\n  if (!output_ext %in% c(\"laz\", \"las\")) stop(\"Invalid extension: use 'laz' or 'las'.\")\n  \n  dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)\n  \n  library(lidR)\n  library(future)\n  \n  # Enable parallel processing\n  set_lidr_threads(workers)\n  future::plan(multisession, workers = workers)\n  \n  # Read LAS file as a catalog\n  ctg &lt;- readLAScatalog(input_file)\n  opt_laz_compression(ctg) &lt;- (output_ext == \"laz\")\n  opt_chunk_size(ctg) &lt;- chunk_size\n  opt_chunk_buffer(ctg) &lt;- buffer\n  opt_output_files(ctg) &lt;- file.path(out_dir, paste0(\"tile_{XLEFT}_{YBOTTOM}.\", output_ext))\n  \n  message(\"Tiling \", input_file, \" into \", out_dir, \" with chunk size \", chunk_size, \" m\")\n  tiled_ctg &lt;- catalog_retile(ctg)\n  \n  return(tiled_ctg)\n}\n\n\n\n#' Preprocess ALS/TLS Point Cloud into Voxel Slice Counts\n#'\n#' This function filters a normalized LAS point cloud to a maximum height (`zmax`), \n#' splits the points into horizontal Z-slices (via `pointsByZSlice()`), \n#' and computes slice-wise point counts per XY pixel using `lidR::pixel_metrics()`.\n#' The result is a flat data frame combining X/Y coordinates and vertical slice counts.\n#'\n#' @param normlas A normalized LAS object (e.g., output from `normalize_height()`) containing ground-aligned Z values.\n#' @param res_xy Numeric. The horizontal resolution (in meters) of the XY voxel grid. Default is 2 m.\n#' @param res_z Numeric. The vertical resolution (in meters) used for binning points into Z slices. Default is 2 m.\n#'              This is passed indirectly to `pointsByZSlice()` via `zmax`.\n#' @param zmax Numeric. The maximum height (in meters) to include for voxelization. Points above this value are excluded.\n#'\n#' @return A data frame where each row corresponds to an XY voxel column and contains:\n#' \\describe{\n#'   \\item{X, Y}{The center coordinates of the pixel (voxel column base).}\n#'   \\item{ground_0_1m, pulses_1_2m, ...}{Point counts per vertical slice from `pointsByZSlice()`.}\n#' }\n#'\n#' @details\n#' This function prepares voxel-based vertical profiles from a normalized LAS point cloud,\n#' which is a common preprocessing step in vegetation structure analysis, such as for:\n#' - Estimating Leaf Area Density (LAD)\n#' - Building 3D vegetation models (e.g., for ENVI-met)\n#' - Computing light extinction or aerodynamic roughness from LiDAR data\n#'\n#' The function performs the following steps:\n#'\n#' 1. **Z-Filtering**: Points are restricted to the height interval `[0, zmax]` to exclude\n#'    noise (e.g., below ground level) and irrelevant outliers (e.g., birds, clouds).\n#'\n#' 2. **Safety Check for Empty Point Cloud**: If filtering removes all points, the function\n#'    returns `NULL` to avoid errors in later processing stages.\n#'\n#' 3. **Dynamic Vertical Binning Limit**: The actual maximum height (`maxZ`) is computed as\n#'    the minimum between the highest Z-value and the user-defined `zmax`. This ensures the\n#'    binning range reflects both the data and physical modeling limits.\n#'\n#' 4. **Per-Pixel Vertical Slice Metrics**: Using `lidR::pixel_metrics()`, the function applies\n#'    a custom-defined metric — `pointsByZSlice(Z, maxZ)` — to each XY cell. This splits the \n#'    vertical column above each pixel into 1-meter height bins (Z-slices) and counts the\n#'    number of points in each slice. Empty bins are filled with 0 to ensure uniform output.\n#'\n#' 5. **Raster Geometry to XY Coordinates**: The function extracts the centroid (X, Y) of each\n#'    pixel cell using `terra::xyFromCell()` so that slice metrics can be mapped spatially.\n#'\n#' 6. **Output Formatting**: The final result is a flat data frame where each row represents\n#'    a voxel column. It includes the X/Y coordinate and point counts for each Z-slice,\n#'    formatted with descriptive column names like `\"ground_0_1m\"`, `\"pulses_2_3m\"`, etc.\n#'\n#' This regularized output is designed to be compatible with downstream modeling frameworks\n#' (e.g., Beer–Lambert LAD computation, ENVI-met's 3DPLANT input, or machine learning models).\n#' It bridges the gap between unstructured 3D point clouds and gridded model inputs.\n#'\n#' @note The function assumes the input LAS object is already normalized to ground level \n#' (i.e., Z = 0 corresponds to terrain surface). Use `normalize_height()` beforehand if needed.\n#'\n#' @seealso \n#' [lidR::pixel_metrics()], \n#' [lidR::voxel_metrics()], \n#' [pointsByZSlice()], \n#' [normalize_height()], \n#' [terra::xyFromCell()]\n#'\n#' @examples\n#' \\dontrun{\n#' las &lt;- lidR::readLAS(\"path/to/normalized.laz\")\n#' voxel_df &lt;- preprocess_voxels(las, res_xy = 2, res_z = 2, zmax = 40)\n#' head(voxel_df)\n#' }\n#'\n#' @export\npreprocess_voxels &lt;- function(normlas, res_xy = 2, res_z = 2, zmax = 40) {\n  # Assign LAS to working variable for clarity\n  las &lt;- normlas\n  \n  # Step 1: Filter points to a valid height range [0, zmax]\n  las &lt;- filter_poi(las, Z &gt;= 0 & Z &lt;= zmax)\n  \n  # Step 2: Check if the LAS object is empty after filtering\n  if (lidR::is.empty(las)) return(NULL)\n  \n  # Step 3: Define maximum vertical bin based on actual max Z (floored), capped at zmax\n  maxZ &lt;- min(floor(max(las@data$Z)), zmax)\n  \n  # Step 4: Define metric function for pixel-wise Z-slice counting\n  func &lt;- formula(paste0(\"~pointsByZSlice(Z, \", maxZ, \")\"))\n  \n  # Step 5: Compute per-pixel vertical structure using custom Z-slice function\n  voxels &lt;- pixel_metrics(las, func, res = res_xy)\n  \n  # Step 6: Extract the X/Y centroid coordinates of each pixel cell\n  xy &lt;- terra::xyFromCell(voxels, seq_len(ncell(voxels)))\n  \n  # Step 7: Extract Z-slice point counts from each pixel cell\n  vals &lt;- terra::values(voxels)\n  \n  # Step 8: Combine XY coordinates with slice values into a data frame\n  df &lt;- cbind(xy, vals)\n  colnames(df)[1:2] &lt;- c(\"X\", \"Y\")  # Rename coordinate columns\n  \n  return(df)\n}\n\n\n#' Convert Vertical Pulse Counts to LAD Using the Beer–Lambert Law\n#'\n#' This function applies the Beer–Lambert law to vertical LiDAR pulse count profiles\n#' (from voxelized ALS/TLS data) to estimate Leaf Area Density (LAD) per vertical slice.\n#' It normalizes the pulse density, corrects edge cases, applies the extinction formula,\n#' and scales or clips LAD values to stay within biologically plausible ranges.\n#'\n#' @param df A data frame containing columns with pulse counts per Z-slice. These columns must be named with the `\"pulses_\"` prefix, as produced by `pointsByZSlice()` and `preprocess_voxels()`.\n#' @param grainsize Numeric. Vertical resolution of each slice (in meters). Default is 2 m.\n#' @param k Numeric. Extinction coefficient (typically between 0.3 and 0.5); default is 0.3.\n#' @param scale_factor Numeric. Scaling factor applied after Beer–Lambert transformation to adjust LAD magnitude (default: 1.2).\n#' @param lad_max Numeric or `NULL`. Maximum LAD value allowed (default: 2.5). Use `NULL` to disable.\n#' @param lad_min Numeric or `NULL`. Minimum LAD value allowed (default: 0.05). Use `NULL` to disable.\n#' @param keep_pulses Logical. If `TRUE`, the original `\"pulses_\"` columns are retained in the output. Default is `FALSE`.\n#'\n#' @return A data frame with the same structure as input, but with new `\"lad_\"` columns\n#' for each original `\"pulses_\"` column, containing the estimated LAD values. \n#' Original pulse columns are optionally removed.\n#'\n#' @details\n#' The Beer–Lambert law models **light attenuation** through a medium such as foliage. In the context of LiDAR, \n#' this relationship is inverted to estimate **leaf area density** from the relative decrease in returned pulses:\n#'\n#' \\deqn{\n#' LAD = -\\frac{\\log(1 - p)}{k \\cdot \\Delta z}\n#' }\n#'\n#' where:\n#' - \\( p \\) is the normalized proportion of pulses in a given voxel column and slice\n#' - \\( k \\) is the extinction coefficient (typically 0.3–0.5)\n#' - \\( \\Delta z \\) is the vertical resolution of the slice (grainsize)\n#'\n#' To avoid mathematical issues:\n#' - Values of \\( p \\geq 1 \\) are clipped to 0.9999\n#' - Values of \\( p \\leq 0 \\) are clipped to 1e-5\n#'\n#' A **scaling factor** can be applied to tune the LAD magnitude (empirical correction).\n#' LAD values are optionally clipped to a maximum and minimum for ecological realism or \n#' to avoid over-saturation artifacts in further modeling (e.g., radiative transfer, ENVI-met input).\n#'\n#' This approach assumes:\n#' - Pulse counts per slice are proportional to occlusion probability\n#' - Normalization by column maximum represents local beam extinction well enough\n#' - The LiDAR pulse distribution is representative of foliage density (most valid in leaf-on conditions)\n#'\n#' @note\n#' - For ALS data, this method provides an **approximate LAD profile** suitable for modeling but\n#'   not physically exact. For more accurate LAD estimation, full waveform or TLS data is preferred.\n#' - Normalization by max pulse count assumes that the densest slice corresponds to near-complete attenuation.\n#'   This introduces uncertainty if canopy gaps dominate the scene.\n#'\n#' @seealso\n#' [preprocess_voxels()], [pointsByZSlice()], [lidR::voxel_metrics()], [terra::rast()]\n#'\n#' @examples\n#' \\dontrun{\n#' df_voxels &lt;- preprocess_voxels(las)\n#' df_lad &lt;- convert_to_LAD_beer(df_voxels, grainsize = 2, k = 0.3)\n#' head(df_lad)\n#' }\n#'\n#' @export\nconvert_to_LAD_beer &lt;- function(df, grainsize = 2, k = 0.3, scale_factor = 1.2,\n                                lad_max = 2.5, lad_min = 0.000, keep_pulses = FALSE) {\n\n  df_lad &lt;- as.data.frame(df)\n  # Find all columns containing vertical pulse count data\n  pulse_cols &lt;- grep(\"^pulses_\", names(df_lad), value = TRUE)\n  \n  # Iterate over each pulse column (i.e., per Z-slice)\n  for (col in pulse_cols) {\n    # Construct name for LAD output column\n    lad_col &lt;- paste0(\"lad_\", col)\n    \n    # Normalize pulse density per column (relative to max)\n    p_rel &lt;- df_lad[[col]] / max(df_lad[[col]], na.rm = TRUE)\n    \n    # # Avoid values of 0 or 1 that would break log(1 - p)\n    # p_rel[p_rel &gt;= 1] &lt;- 0.9999\n    # p_rel[p_rel &lt;= 0] &lt;- 1e-5\n    \n    # Apply Beer–Lambert transformation to estimate LAD\n    #lad_vals &lt;- -log1p(1 - p_rel) / (k * grainsize)\n    lad_vals &lt;- -log1p(-p_rel) / (k * grainsize)\n    \n    # Apply empirical scale factor to adjust LAD magnitude\n    lad_vals &lt;- lad_vals * scale_factor\n    \n    # Enforce maximum and minimum LAD values (if set)\n    if (!is.null(lad_max)) {\n      lad_vals &lt;- pmin(lad_vals, lad_max)\n    }\n    if (!is.null(lad_min)) {\n      lad_vals &lt;- pmax(lad_vals, lad_min)\n    }\n    \n    # Store LAD values in new column\n    df_lad[[lad_col]] &lt;- lad_vals\n    \n    # Optionally remove original pulse column\n    if (!keep_pulses) {\n      df_lad[[col]] &lt;- NULL\n    }\n  }\n  \n  return(df_lad)\n}\n\n\n\n#' Compute Canopy Traits from Long-Format LAD Profiles\n#'\n#' This function calculates key vegetation structure metrics from long-format \n#' Leaf Area Density (LAD) profiles, grouped by `ENVIMET_ID`. These traits \n#' include Leaf Area Index (LAI), maximum LAD, crown height, total height, \n#' leaf thickness, and aerodynamic roughness length, all of which are relevant \n#' for canopy modeling (e.g., in ENVI-met).\n#'\n#' @param lad_df A long-format data frame containing LAD values per voxel slice.\n#' Must include at least `ENVIMET_ID`, `lad`, `z`, and optionally `Height_CHM`, `LeafThickness`, and `cluster`.\n#' @param res_z Vertical voxel resolution (in meters). Default is 2 m.\n#' @param LAD_cutoff Minimum LAD threshold for crown detection and trait computation (default: 0.05).\n#' @param leaf_thickness_df Optional data frame mapping `cluster` to `LeafThickness` values.\n#' @param roughness_fun A function to derive aerodynamic roughness length from total height.\n#' Defaults to `height * 0.1`, but can be replaced with empirical models.\n#' @param plantclass_prefix Character string used to label the plant class, typically `\"Tree\"` or `\"Plant\"`.\n#'\n#' @return A data frame with one row per `ENVIMET_ID` and the following columns:\n#' \\describe{\n#'   \\item{ENVIMET_ID}{Unique identifier for the plant object (voxel column or point).}\n#'   \\item{LAI}{Leaf Area Index (sum of LAD across vertical profile, clipped at 95th percentile).}\n#'   \\item{MaxLAD}{Maximum LAD value (95th percentile capped at 3).}\n#'   \\item{CrownHeight}{Height (in res_z units) of topmost voxel with LAD above cutoff.}\n#'   \\item{Height}{Estimated total plant height, based on CHM or max Z.}\n#'   \\item{LeafThickness}{Estimated or provided leaf thickness (from lookup table or column).}\n#'   \\item{LADcutoff}{The LAD threshold used for crown height filtering.}\n#'   \\item{RoughnessLength}{Estimated aerodynamic roughness length (via `roughness_fun`).}\n#'   \\item{plantclass}{String ID used for matching plant profile in simulations.}\n#' }\n#'\n#' @details\n#' This function summarizes vertical LAD profiles (from ALS or TLS data) into trait values \n#' that are essential for microclimate or ecological modeling. The key components:\n#'\n#' - **LAI**: Computed as the sum of LAD values per profile, excluding extreme values above \n#'   the 95th percentile to avoid waveform or outlier artifacts.\n#'\n#' - **MaxLAD**: The 95th percentile LAD value, capped at 3 to avoid unphysical spikes.\n#'\n#' - **CrownHeight**: The highest voxel (Z/res_z) with LAD above the cutoff threshold, interpreted \n#'   as the vertical extent of the crown (not the total plant height).\n#'\n#' - **Height**: Either taken from an external `Height_CHM` column (if provided), or estimated \n#'   as the maximum Z value in the profile × `res_z`.\n#'\n#' - **LeafThickness**: Optionally joined from a lookup table (`leaf_thickness_df`) using `cluster`.\n#'\n#' - **RoughnessLength**: Estimated from height using a customizable function (default: 10% of height).\n#'\n#' This function ensures compatibility with ENVI-met’s 3DPLANT system, where LAI, MaxLAD, and \n#' CrownHeight directly map to physical vegetation parameters in `.pld` files.\n#'\n#' @note\n#' - CrownHeight is returned in voxel units (Z index × res_z); it is not a biologically precise\n#'   crown base height but an upper limit used for model placement.\n#' - The input `lad_df` must be long-format, with rows representing individual vertical slices per plant.\n#'\n#' @seealso\n#' [convert_to_LAD_beer()], [export_lad_to_envimet3d()], [normalize_height()], [rasterize_canopy()]\n#'\n#' @examples\n#' \\dontrun{\n#' traits_df &lt;- compute_traits_from_lad(lad_long_df, res_z = 2)\n#' head(traits_df)\n#' }\n#'\n#' @export\ncompute_traits_from_lad &lt;- function(lad_df, res_z = 2, LAD_cutoff = 0.05,\n                                    leaf_thickness_df = NULL,\n                                    roughness_fun = function(height) height * 0.1,\n                                    plantclass_prefix = \"Tree\") {\n  \n  # Optional: merge leaf thickness values if available\n  if (!is.null(leaf_thickness_df)) {\n    lad_df &lt;- lad_df %&gt;%\n      left_join(leaf_thickness_df %&gt;% select(cluster, LeafThickness), by = \"cluster\")\n  }\n  \n  # Split by ENVIMET_ID (each plant or tree column)\n  groups &lt;- split(lad_df, lad_df$ENVIMET_ID)\n  \n  # Trait calculation for each plant object\n  result_list &lt;- lapply(groups, function(group_df) {\n    \n    # LAD filtering: keep values under 95th percentile and above cutoff\n    lad_95 &lt;- quantile(group_df$lad, 0.95, na.rm = TRUE)\n    lad_filtered &lt;- group_df$lad[group_df$lad &lt;= lad_95 & group_df$lad &gt; LAD_cutoff]\n    \n    # LAI: sum of LAD (unit is m²/m² if LAD is in m²/m³ × height slice)\n    lai &lt;- sum(lad_filtered, na.rm = TRUE)\n    \n    # MaxLAD: 95th percentile capped at a max of 3\n    max_lad &lt;- min(lad_95, 3)\n    \n    # Crown height: highest voxel slice above LAD threshold\n    if (any(group_df$lad &gt; LAD_cutoff, na.rm = TRUE)) {\n      crown_z &lt;- max(group_df$z[group_df$lad &gt; LAD_cutoff]/res_z, na.rm = TRUE)\n      crown_height &lt;- crown_z\n    } else {\n      crown_height &lt;- NA\n    }\n    \n    # Total plant height: prefer CHM column, fall back to max Z\n    height_value &lt;- if (\"Height_CHM\" %in% names(group_df) && !all(is.na(group_df$Height_CHM))) {\n      unique(group_df$Height_CHM)[1]\n    } else {\n      max(group_df$z, na.rm = TRUE) * res_z\n    }\n    \n    # Leaf thickness: taken from joined lookup or left NA\n    lt &lt;- if (\"LeafThickness\" %in% names(group_df)) {\n      unique(group_df$LeafThickness)[1]\n    } else {\n      NA\n    }\n    \n    # Return one row of traits\n    data.frame(\n      ENVIMET_ID = unique(group_df$ENVIMET_ID),\n      LAI = lai,\n      MaxLAD = max_lad,\n      CrownHeight = crown_height,\n      Height = height_value,\n      LeafThickness = lt,\n      LADcutoff = LAD_cutoff,\n      RoughnessLength = roughness_fun(height_value),\n      plantclass = paste0(plantclass_prefix, \"_\", unique(group_df$ENVIMET_ID))\n    )\n  })\n  \n  # Combine all into one data frame\n  traits_df &lt;- do.call(rbind, result_list)\n  \n  # Remove rows with missing LAI (empty plant objects)\n  traits_df &lt;- traits_df[!is.na(traits_df$LAI), ]\n  \n  return(traits_df)\n}\n\n\n\n\n#' Export Clustered LAD Profiles to ENVI-met PLANT3D (.pld) XML Format\n#'\n#' This function exports vertical LAD profiles (e.g., from TLS or ALS clustering) to\n#' ENVI-met-compatible 3DPLANT XML (.pld) files. Each profile is mapped to a species definition,\n#' associated with a seasonal LAI and blossom cycle, and written in sparse matrix format.\n#'\n#' @param lad_df A long-format data frame with LAD values per voxel slice and per plant.\n#' Must contain columns `ENVIMET_ID`, `z`, and `lad`. Optionally includes `species_class`.\n#' @param file_out Output file path for the `.pld` XML file. Default is `\"tls_envimet_tree.pld\"`.\n#' @param res_z Vertical resolution of LAD slices (in meters). Default is 1 m.\n#' @param trait_df Optional data frame of additional plant traits (unused here but compatible for future use).\n#'\n#' @return Writes an XML file to disk in ENVI-met PLANT3D format and prints confirmation to console.\n#'\n#' @details\n#' This function is used to convert voxelized and clustered LAD profiles derived from TLS/ALS into\n#' XML `.pld` files for use in ENVI-met's 3DPLANT vegetation modeling system.\n#'\n#' For each unique `ENVIMET_ID` (typically corresponding to a tree column), a separate `&lt;PLANT3D&gt;` block\n#' is generated. Each plant object is assigned:\n#' - A default or class-based species name (e.g., `\"Fagus sylvatica\"`)\n#' - A height based on the number of vertical LAD layers\n#' - LAD values in sparse 3D format\n#' - A monthly season and blossom profile (12 months)\n#'\n#' The function uses a predefined lookup table (`custom_profiles`) for assigning seasonal LAI and blossom \n#' curves to known species. Unmapped species default to generic \"broadleaf\" or \"conifer\" profiles.\n#'\n#' Species-specific traits such as `Albedo`, `Transmittance`, `RootDiameter`, and `LeafType` are hard-coded\n#' per class ID but can be extended as needed.\n#'\n#' @note\n#' - `species_class` must be numeric and match predefined mappings. Unknown classes fall back to a default.\n#' - LAD values are averaged across horizontal slices (X/Y = 1) and rounded to 5 decimals.\n#' - The LAD format is `\"sparematrix-3D\"` with only z-direction layers, suitable for columnar tree shapes.\n#'\n#' @seealso\n#' [compute_traits_from_lad()], [convert_to_LAD_beer()], [ENVI-met documentation on PLANT3D profiles]\n#'\n#' @examples\n#' \\dontrun{\n#' export_lad_to_envimet_p3d(lad_df = my_lad_data, file_out = \"trees.pld\", res_z = 1)\n#' }\n#'\n#' @export\nexport_lad_to_envimet_p3d &lt;- function(lad_df, file_out = \"tls_envimet_tree.pld\", res_z = 1, trait_df = NULL) {\n  # --- Define seasonal LAI/blossom profiles per species ---\n  season_profiles &lt;- list(\n    broadleaf = list(\n      Season = c(0.3, 0.3, 0.3, 0.4, 0.7, 1, 1, 1, 0.8, 0.6, 0.3, 0.3),\n      Blossom = c(0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n    ),\n    conifer = list(\n      Season = rep(0.8, 12),\n      Blossom = rep(0, 12)\n    )\n  )\n  \n  # --- Assign species-specific profiles if known ---\n  custom_profiles &lt;- list(\n    \"Fagus sylvatica\" = season_profiles$broadleaf,\n    \"Alnus glutinosa\" = season_profiles$broadleaf,\n    \"Fraxinus excelsior\" = season_profiles$broadleaf,\n    \"Pseudotsuga menziesii\" = season_profiles$conifer,\n    \"Larix decidua\" = season_profiles$broadleaf,\n    \"Quercus robur\" = season_profiles$broadleaf,\n    \"Picea abies\" = list(Season = rep(0.75, 12), Blossom = rep(0, 12))\n  )\n  \n  # --- Clean input and encode Z-layer index ---\n  lad_df &lt;- lad_df[!is.na(lad_df$lad), ]\n  z_map &lt;- setNames(seq_along(sort(unique(lad_df$z))), sort(unique(lad_df$z)))\n  lad_df$k &lt;- z_map[as.character(lad_df$z)]\n  lad_df$lad_value &lt;- round(lad_df$lad, 5)\n  \n  # --- Initialize XML document ---\n  tree_ids &lt;- unique(lad_df$ENVIMET_ID)\n  now &lt;- format(Sys.time(), \"%Y-%m-%dT%H:%M:%S\")\n  root &lt;- newXMLNode(\"ENVI-MET_Datafile\")\n  header_node &lt;- newXMLNode(\"Header\")\n  addChildren(header_node, newXMLNode(\"filetype\", \"DATA\"))\n  addChildren(header_node, newXMLNode(\"version\", \"1\"))\n  addChildren(header_node, newXMLNode(\"revisiondate\", now))\n  addChildren(header_node, newXMLNode(\"remark\", \"Clustered TLS-based trees\"))\n  addChildren(header_node, newXMLNode(\"fileInfo\", \"Clustered LAD Trees\"))\n  addChildren(header_node, newXMLNode(\"checksum\", \"32767\"))\n  addChildren(header_node, newXMLNode(\"encryptionlevel\", \"1699612\"))\n  addChildren(root, header_node)\n  \n  # --- Iterate over all unique tree profiles ---\n  for (id in tree_ids) {\n    tree_df &lt;- lad_df[lad_df$ENVIMET_ID == id, ]\n    \n    # Average LAD across XY slices per height layer\n    profile &lt;- tree_df %&gt;%\n      group_by(z = k) %&gt;%\n      summarise(lad_value = mean(lad_value, na.rm = TRUE), .groups = \"drop\")\n    \n    # Define voxel profile parameters\n    zlayers &lt;- max(profile$z)\n    dataI &lt;- 1\n    dataJ &lt;- 1\n    Height &lt;- zlayers * res_z\n    \n    # --- Default plant parameters ---\n    name &lt;- \"Fagus sylvatica\"; albedo &lt;- 0.18; trans &lt;- 0.30; root_d &lt;- 4.5; leaf_type &lt;- 1\n    \n    # --- Override species traits if species_class is available ---\n    if (!all(is.na(tree_df$species_class))) {\n      class_val &lt;- na.omit(unique(tree_df$species_class))[1]\n      if (class_val == 2)      { name &lt;- \"Alnus glutinosa\";       albedo &lt;- 0.18; trans &lt;- 0.35; root_d &lt;- 3.5; leaf_type &lt;- 1 }\n      else if (class_val == 3) { name &lt;- \"Fraxinus excelsior\";    albedo &lt;- 0.19; trans &lt;- 0.38; root_d &lt;- 4.0; leaf_type &lt;- 1 }\n      else if (class_val == 4) { name &lt;- \"Fagus sylvatica\";       albedo &lt;- 0.18; trans &lt;- 0.30; root_d &lt;- 4.5; leaf_type &lt;- 1 }\n      else if (class_val == 5) { name &lt;- \"Pseudotsuga menziesii\"; albedo &lt;- 0.20; trans &lt;- 0.18; root_d &lt;- 4.2; leaf_type &lt;- 2 }\n      else if (class_val == 6) { name &lt;- \"Larix decidua\";         albedo &lt;- 0.23; trans &lt;- 0.25; root_d &lt;- 4.0; leaf_type &lt;- 2 }\n      else if (class_val == 7) { name &lt;- \"Quercus robur\";         albedo &lt;- 0.20; trans &lt;- 0.35; root_d &lt;- 5.0; leaf_type &lt;- 1 }\n      else if (class_val == 11){ name &lt;- \"Picea abies\";           albedo &lt;- 0.22; trans &lt;- 0.15; root_d &lt;- 3.0; leaf_type &lt;- 2 }\n    }\n    \n    # --- Assign seasonal profile ---\n    profile_key &lt;- name\n    season_profile &lt;- custom_profiles[[profile_key]]\n    if (is.null(season_profile)) {\n      season_profile &lt;- season_profiles[[ifelse(leaf_type == 1, \"broadleaf\", \"conifer\")]]\n    }\n    \n    # --- Build PLANT3D block ---\n    plant_node &lt;- newXMLNode(\"PLANT3D\")\n    addChildren(plant_node, newXMLNode(\"ID\", id))\n    addChildren(plant_node, newXMLNode(\"Description\", \"Clustered TLS Tree\"))\n    addChildren(plant_node, newXMLNode(\"AlternativeName\", name))\n    addChildren(plant_node, newXMLNode(\"Planttype\", \"0\"))\n    addChildren(plant_node, newXMLNode(\"Leaftype\", as.character(leaf_type)))\n    addChildren(plant_node, newXMLNode(\"Albedo\", sprintf(\"%.5f\", albedo)))\n    addChildren(plant_node, newXMLNode(\"Eps\", \"0.96000\"))\n    addChildren(plant_node, newXMLNode(\"Transmittance\", sprintf(\"%.5f\", trans)))\n    addChildren(plant_node, newXMLNode(\"Height\", sprintf(\"%.5f\", Height)))\n    addChildren(plant_node, newXMLNode(\"Width\", \"1.00000\"))\n    addChildren(plant_node, newXMLNode(\"Depth\", \"1.00000\"))\n    addChildren(plant_node, newXMLNode(\"RootDiameter\", sprintf(\"%.5f\", root_d)))\n    addChildren(plant_node, newXMLNode(\"cellsize\", sprintf(\"%.5f\", res_z)))\n    addChildren(plant_node, newXMLNode(\"xy_cells\", dataI))\n    addChildren(plant_node, newXMLNode(\"z_cells\", zlayers))\n    addChildren(plant_node, newXMLNode(\"scalefactor\", \"1.00000\"))\n    \n    # --- LAD Profile block (sparse 3D format) ---\n    lad_lines &lt;- apply(profile, 1, function(r) {\n      sprintf(\"%d,%d,%d,%.5f\", 1, 1, r[1], r[2])\n    })\n    lad_node &lt;- newXMLNode(\"LAD-Profile\",\n                           attrs = c(type = \"sparematrix-3D\",\n                                     dataI = dataI,\n                                     dataJ = dataJ,\n                                     zlayers = zlayers,\n                                     defaultValue = \"0.00000\"),\n                           .children = paste(lad_lines, collapse = \"\\n\"))\n    addChildren(plant_node, lad_node)\n    \n    # --- Add seasonality and flowering profiles ---\n    addChildren(plant_node, newXMLNode(\"Season-Profile\",\n                                       paste(sprintf(\"%.5f\", season_profile$Season), collapse = \",\")))\n    addChildren(plant_node, newXMLNode(\"Blossom-Profile\",\n                                       paste(sprintf(\"%.5f\", season_profile$Blossom), collapse = \",\")))\n    \n    # --- Disable L-System generation ---\n    addChildren(plant_node, newXMLNode(\"L-SystemBased\", \"0\"))\n    addChildren(root, plant_node)\n  }\n  \n  # --- Write XML to file ---\n  saveXML(root, file = file_out, indent = TRUE, encoding = \"UTF-8\")\n  message(\"✔ ENVI-met PLANT3D (.pld) written to: \", normalizePath(file_out))\n}\n\n#' Generate Compact ENVI-met-Compatible Base36 String IDs\n#'\n#' Converts an integer index to a left-padded base-36 string (digits + uppercase A–Z),\n#' prefixed with `\"S\"`, suitable for use as compact `ENVIMET_ID`s (e.g., `\"S0001A\"`).\n#' Useful when assigning unique but readable identifiers for synthetic plant elements\n#' in 3D simulation domains.\n#'\n#' @param n An integer (scalar) to convert to base-36.\n#' @param width Integer width of the resulting code (default: 5). Strings are zero-padded on the left.\n#'\n#' @return A character string in base-36 representation, prefixed with `\"S\"` and left-padded\n#' to match the desired `width`. Returns e.g. `\"S0000A\"`, `\"S0001Z\"`, `\"S00010\"`, etc.\n#'\n#' @details\n#' Base-36 encoding uses the characters `0–9` and `A–Z` to represent numbers in a compact alphanumeric form.\n#' This is commonly used in modeling frameworks like ENVI-met where short string IDs are needed to:\n#' - Uniquely label plant objects (`ENVIMET_ID`)\n#' - Avoid numeric-only names (which may conflict with XML or database formats)\n#' - Allow for high capacity in short formats (36⁵ = ~60 million unique IDs for `width = 5`)\n#'\n#' The function:\n#' 1. Converts the number `n` to base-36 using digit/modulo arithmetic\n#' 2. Left-pads the result to fixed width with zeros (`\"0\"`)\n#' 3. Adds a prefix `\"S\"` to ensure the string starts with a non-numeric character\n#'\n#' @note\n#' This function assumes positive integers (`n &gt; 0`). No validation is done for negative or non-integer input.\n#' It is up to the user to avoid duplicate IDs.\n#'\n#' @seealso\n#' [export_lad_to_envimet_p3d()], [sprintf()], [strtoi()] for inverse conversion\n#'\n#' @examples\n#' int_to_base36(1)      # \"S00001\"\n#' int_to_base36(35)     # \"S0000Z\"\n#' int_to_base36(36)     # \"S00010\"\n#' int_to_base36(12345)  # \"S009IX\"\n#'\n#' @export\nint_to_base36 &lt;- function(n, width = 5) {\n  # Base-36 character set: 0–9, A–Z\n  chars &lt;- c(0:9, LETTERS)\n  base &lt;- length(chars)\n  \n  # Convert to base-36 via division/remainder\n  result &lt;- character()\n  while (n &gt; 0) {\n    result &lt;- c(chars[(n %% base) + 1], result)\n    n &lt;- n %/% base\n  }\n  \n  # Collapse to single string\n  result &lt;- paste(result, collapse = \"\")\n  \n  # Pad with zeros on the left to match width\n  padded &lt;- sprintf(paste0(\"%0\", width, \"s\"), result)\n  \n  # Replace spaces with \"0\" and add \"S\" prefix\n  paste0(\"S\", substr(gsub(\" \", \"0\", padded), 1, width))\n}\n\n\n#' Suggest Optimal Number of Principal Components\n#'\n#' This function evaluates a PCA object and recommends the number of components to retain\n#' based on three common criteria: cumulative explained variance, Kaiser criterion, and the\n#' elbow method. Optionally, a diagnostic scree plot and cumulative variance plot are shown.\n#'\n#' @param pca_obj A PCA object as returned by [prcomp()], which must contain the `sdev` vector.\n#' @param variance_cutoff The cumulative explained variance threshold to use for the primary selection (default: 0.8).\n#' @param plot Logical. If `TRUE`, produces a scree plot and a cumulative variance plot for visual inspection.\n#'\n#' @return A list with the following elements:\n#' \\describe{\n#'   \\item{n_pcs}{Recommended number of principal components based on variance cutoff.}\n#'   \\item{explained_variance}{Number of components needed to reach the cutoff.}\n#'   \\item{kaiser}{Number of components with eigenvalue &gt; 1 (Kaiser criterion).}\n#'   \\item{elbow}{Index of the \"elbow\" point in the scree plot.}\n#'   \\item{info_table}{A summary data frame showing results for all three criteria.}\n#' }\n#'\n#' @details\n#' The number of principal components can be selected using multiple heuristics:\n#'\n#' - **Cumulative Explained Variance**: Retain the smallest number of components such that the\n#' cumulative variance exceeds `variance_cutoff`. This is often considered the primary criterion.\n#'\n#' - **Kaiser Criterion**: Retain all components with eigenvalue &gt; 1. Assumes input data has been scaled.\n#'\n#' - **Elbow Method**: Identifies the point where the marginal gain in explained variance drops off,\n#' i.e., the inflection point of the scree plot. Here approximated by the first minimum in the\n#' differences of explained variance.\n#'\n#' The function outputs all three estimates but uses only the **variance cutoff** for final selection.\n#'\n#' @note\n#' The scree plot assumes components are sorted by variance (as returned by [prcomp()]).\n#' The elbow method used here is a simplified heuristic and may not capture complex knees.\n#'\n#' @seealso\n#' [prcomp()], [factoextra::fviz_eig()], [psych::principal()]\n#'\n#' @examples\n#' pca &lt;- prcomp(USArrests, scale. = TRUE)\n#' suggest_n_pcs(pca, variance_cutoff = 0.9)\n#'\n#' @export\nsuggest_n_pcs &lt;- function(pca_obj, variance_cutoff = 0.8, plot = TRUE) {\n  # Extract standard deviations and calculate explained variance\n  std_dev &lt;- pca_obj$sdev\n  var_explained &lt;- std_dev^2 / sum(std_dev^2)\n  cum_var_explained &lt;- cumsum(var_explained)\n  \n  # Criterion 1: variance cutoff\n  n_var &lt;- which(cum_var_explained &gt;= variance_cutoff)[1]\n  \n  # Criterion 2: Kaiser criterion (eigenvalue &gt; 1)\n  eigenvalues &lt;- std_dev^2\n  n_kaiser &lt;- sum(eigenvalues &gt; 1)\n  \n  # Criterion 3: elbow (minimum change in explained variance)\n  diffs &lt;- diff(var_explained)\n  elbow &lt;- which.min(diffs)[1]\n  \n  # Summarize selection criteria\n  info_table &lt;- data.frame(\n    Criterion = c(\"Variance Cutoff\", \"Kaiser Criterion\", \"Elbow Method\"),\n    Components = c(n_var, n_kaiser, elbow),\n    Cumulative_Variance = c(\n      round(cum_var_explained[n_var], 3),\n      round(cum_var_explained[n_kaiser], 3),\n      round(cum_var_explained[elbow], 3)\n    )\n  )\n  \n  # Final decision based on variance criterion only\n  n_final &lt;- n_var\n  \n  # Output summary\n  cat(\"📊 PCA Component Selection Summary:\\n\")\n  print(info_table, row.names = FALSE)\n  cat(\"\\n✅ Recommended number of PCs (variance_cutoff =\", variance_cutoff, \"):\", n_final, \"\\n\")\n  \n  # Optional visualization\n  if (plot) {\n    par(mfrow = c(1, 2))\n    \n    # Scree plot\n    plot(var_explained, type = \"b\", pch = 19, col = \"steelblue\",\n         xlab = \"Component\", ylab = \"Explained Variance\",\n         main = \"Scree Plot\")\n    abline(h = 1, col = \"red\", lty = 2)\n    abline(v = elbow, col = \"darkgreen\", lty = 3)\n    legend(\"topright\", legend = c(\"Kaiser (λ &gt; 1)\", \"Elbow\"),\n           col = c(\"red\", \"darkgreen\"), lty = c(2, 3), bty = \"n\")\n    \n    # Cumulative variance plot\n    plot(cum_var_explained, type = \"b\", pch = 19, col = \"darkorange\",\n         xlab = \"Component\", ylab = \"Cumulative Variance\",\n         main = \"Cumulative Explained Variance\")\n    abline(h = variance_cutoff, col = \"red\", lty = 2)\n    abline(v = n_var, col = \"blue\", lty = 3)\n    legend(\"bottomright\", legend = c(\"Cutoff\", \"Selected Components\"),\n           col = c(\"red\", \"blue\"), lty = c(2, 3), bty = \"n\")\n    \n    par(mfrow = c(1, 1))\n  }\n  \n  # Return results invisibly\n  invisible(list(\n    n_pcs = n_final,\n    explained_variance = n_var,\n    kaiser = n_kaiser,\n    elbow = elbow,\n    info_table = info_table\n  ))\n}\n\n#' Export ENVI-met 3DPLANT XML from enriched LAD profile dataframe\n#'\n#' This function prepares and exports an ENVI-met compatible 3DPLANT (.pld) XML file\n#' using a fully enriched `lad_df_cleaned` dataframe, containing LAD profiles, traits,\n#' and metadata for each ENVIMET_ID.\n#'\n#' @param lad_df_cleaned A data.frame containing voxelized LAD data and associated plant traits.\n#'   Must contain: `ENVIMET_ID`, `cluster`, `z`, `lad`, `species_class`, `species_name`,\n#'   `LeafThickness`, `Height_CHM`, `Vertical_Evenness`, `Entropy`, `RoughnessLength`,\n#'   `LAI`, `MaxLAD`, `CrownHeight`, `Height`, `LADcutoff`, `plantclass`.\n#' @param file_out Path to output `.pld` file.\n#' @param res_z Vertical voxel resolution in meters (e.g. 2).\n#'\n#' @return Invisible `TRUE` if export succeeds, otherwise stops with an error.\n#' @export\n#'\n#' @examples\n#' export_envimet_pld_from_lad_clean(\n#'   lad_df_cleaned = lad_profiles_long,\n#'   file_out = \"data/envimet/envimet_pseudo3Dtree.pld\",\n#'   res_z = 2\n#' )\nexport_envimet_pld_from_lad_clean &lt;- function(lad_df_cleaned, file_out, res_z) {\n  \n  # --- Validation of required columns ---\n  required_cols &lt;- c(\n    \"ENVIMET_ID\", \"cluster\", \"z\", \"lad\",\n    \"species_class\", \"species_name\", \"LeafThickness\",\n    \"Height_CHM\", \"Vertical_Evenness\", \"Entropy\",\n    \"RoughnessLength\", \"LAI\", \"MaxLAD\",\n    \"CrownHeight\", \"Height\", \"LADcutoff\", \"plantclass\"\n  )\n  \n  missing &lt;- setdiff(required_cols, colnames(lad_df_cleaned))\n  if (length(missing) &gt; 0) {\n    stop(\"Missing columns in lad_df_cleaned: \", paste(missing, collapse = \", \"))\n  }\n  \n  # --- Ensure required columns are complete ---\n  stopifnot(!any(is.na(lad_df_cleaned$ENVIMET_ID)))\n  stopifnot(!any(is.na(lad_df_cleaned$lad)))\n  stopifnot(!any(is.na(lad_df_cleaned$species_name)))\n  stopifnot(!any(is.na(lad_df_cleaned$LeafThickness)))\n  \n  # --- Extract trait table (one row per plant ID) ---\n  trait_df &lt;- lad_df_cleaned %&gt;%\n    group_by(ENVIMET_ID) %&gt;%\n    summarise(\n      LAI = mean(LAI, na.rm = TRUE),\n      MaxLAD = max(MaxLAD, na.rm = TRUE),\n      CrownHeight = mean(CrownHeight, na.rm = TRUE),\n      Height = mean(Height, na.rm = TRUE),\n      RoughnessLength = mean(RoughnessLength, na.rm = TRUE),\n      LeafThickness = mean(LeafThickness, na.rm = TRUE),\n      LADcutoff = mean(LADcutoff, na.rm = TRUE),\n      plantclass = first(plantclass),\n      species_class = first(species_class),\n      species_name = first(species_name),\n      Entropy = mean(Entropy, na.rm = TRUE),\n      Vertical_Evenness = mean(Vertical_Evenness, na.rm = TRUE),\n      Height_CHM = mean(Height_CHM, na.rm = TRUE),\n      .groups = \"drop\"\n    ) %&gt;%\n    filter(!is.na(LAI), !is.na(MaxLAD))\n  \n  # --- Filter valid LAD entries ---\n  valid_ids &lt;- trait_df$ENVIMET_ID\n  lad_profiles_filtered &lt;- lad_df_cleaned %&gt;%\n    filter(ENVIMET_ID %in% valid_ids)\n  \n  # --- Export via custom XML function ---\n  export_lad_to_envimet_p3d(\n    lad_df = lad_profiles_filtered,\n    file_out = file_out,\n    res_z = res_z,\n    trait_df = trait_df\n  )\n  \n  invisible(TRUE)\n}\n\n#' Export ENVI-met and microclimf LAD Profiles from Clustered Data\n#'\n#' These functions aggregate a cleaned LAD dataset (lad_df_clean) by cluster,\n#' compute synthetic profiles, and export them for use in ENVI-met and microclimf.\n#'\n#' @param lad_df_clean Data frame with full LAD profile, coordinates, traits, and cluster ID.\n#' @param res_z Vertical voxel resolution (numeric).\n#' @param out_dir Output directory where .gpkg and .pld files will be written.\n#' @export\nexport_envimet_profiles &lt;- function(lad_df_clean, res_z, out_dir = \"data/envimet\") {\n  dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)\n  \n  # --- Mittelung der LAD-Profile nach Cluster ---\n  cluster_profiles &lt;- lad_df_clean %&gt;%\n    group_by(cluster) %&gt;%\n    summarise(across(starts_with(\"lad_pulses_\"), ~mean(.x, na.rm = TRUE)), .groups = \"drop\") %&gt;%\n    arrange(cluster)\n  \n  # --- Long-Format ---\n  lad_profiles_long &lt;- cluster_profiles %&gt;%\n    pivot_longer(cols = starts_with(\"lad_pulses_\"),\n                 names_to = \"layer\", values_to = \"lad\") %&gt;%\n    mutate(z = as.integer(gsub(\"lad_pulses_|_.*\", \"\", layer)) * res_z,\n           ENVIMET_ID = paste0(\"S\", formatC(cluster, width = 5, flag = \"0\"))) %&gt;%\n    select(ENVIMET_ID, cluster, z, lad)\n  \n  # --- Art und Traits ---\n  cluster_traits &lt;- lad_df_clean %&gt;%\n    group_by(cluster, species_class) %&gt;%\n    summarise(n = n(), .groups = \"drop\") %&gt;%\n    group_by(cluster) %&gt;%\n    slice_max(n, with_ties = FALSE) %&gt;%\n    ungroup()\n  \n  species_mapping &lt;- tibble::tibble(\n    species_class = c(1, 2, 3, 4, 5, 6, 7),\n    species_name = c(\"Acer pseudoplatanus\", \"Betula pendula\", \"Fagus sylvatica\",\n                     \"Picea abies\", \"Pinus sylvestris\", \"Quercus robur\", \"Tilia cordata\")\n  )\n  \n  leaf_thickness_lookup &lt;- tibble::tibble(\n    species_name = c(\"Fagus sylvatica\", \"Quercus robur\", \"Acer pseudoplatanus\",\n                     \"Pinus sylvestris\", \"Picea abies\", \"Betula pendula\", \"Tilia cordata\"),\n    LeafThickness = c(0.0025, 0.0025, 0.0022, 0.0015, 0.0016, 0.0021, 0.0023)\n  )\n  \n  cluster_heights &lt;- lad_df_clean %&gt;%\n    group_by(cluster) %&gt;%\n    summarise(Height = mean(CHM, na.rm = TRUE), .groups = \"drop\")\n  \n  # --- LAD-Profile anreichern ---\n  lad_profiles_long &lt;- lad_profiles_long %&gt;%\n    left_join(cluster_traits, by = \"cluster\") %&gt;%\n    left_join(species_mapping, by = \"species_class\") %&gt;%\n    left_join(leaf_thickness_lookup, by = \"species_name\") %&gt;%\n    left_join(cluster_heights, by = \"cluster\")\n  \n  # --- Traits berechnen ---\n  trait_df &lt;- compute_traits_from_lad(lad_profiles_long, res_z)\n  \n  # --- Spatial Centroid für jede Cluster-ID ---\n  cluster_points &lt;- lad_df_clean %&gt;%\n    group_by(cluster) %&gt;%\n    summarise(across(c(X, Y), mean), .groups = \"drop\") %&gt;%\n    mutate(ENVIMET_ID = paste0(\"S\", formatC(cluster, width = 5, flag = \"0\"))) %&gt;%\n    st_as_sf(coords = c(\"X\", \"Y\"), crs = 25832)\n  \n  # --- Export .gpkg und .pld ---\n  st_write(cluster_points, file.path(out_dir, \"synthetic_envimet_profiles.gpkg\"), delete_dsn = TRUE)\n  export_lad_to_envimet_p3d(lad_profiles_long, file.path(out_dir, \"synthetic_envimet_profiles.pld\"),\n                            res_z = res_z, trait_df = trait_df)\n}\n\n#' Export microclimf-compatible profiles per cluster\n#'\n#' @param lad_df_clean Cleaned LAD data with coordinates and cluster ID\n#' @param out_file Output GPKG path\n#' @export\nexport_microclimf_profiles &lt;- function(lad_df_clean, out_file = \"data/microclimf/synthetic_microclimf_profiles.gpkg\") {\n  dir.create(dirname(out_file), showWarnings = FALSE, recursive = TRUE)\n  \n  # Mittelung aller nicht-Koordinaten nach Cluster\n  mean_traits &lt;- lad_df_clean %&gt;%\n    group_by(cluster) %&gt;%\n    summarise(across(-c(X, Y), ~mean(.x, na.rm = TRUE)), .groups = \"drop\")\n  \n  # Cluster-Zentroiden\n  cluster_points &lt;- lad_df_clean %&gt;%\n    group_by(cluster) %&gt;%\n    summarise(across(c(X, Y), mean), .groups = \"drop\")\n  \n  # Zusammenfügen\n  out_df &lt;- left_join(cluster_points, mean_traits, by = \"cluster\") %&gt;%\n    st_as_sf(coords = c(\"X\", \"Y\"), crs = 25832)\n  \n  st_write(out_df, out_file, delete_dsn = TRUE)\n}\n\n#' Export all original LAD point profiles to GeoPackage with full spatial attribution\n#'\n#' @param lad_df_clean DataFrame with cleaned LAD columns and traits\n#' @param out_gpkg Path to output .gpkg file\n#' @param layer_name Layer name inside the GPKG\n#' @return Writes .gpkg file to disk\nexport_full_lad_profiles_gpkg &lt;- function(lad_df_clean, out_gpkg = \"data/lad_profiles_full.gpkg\", layer_name = \"lad_profiles\") {\n  stopifnot(all(c(\"X\", \"Y\") %in% colnames(lad_df_clean)))\n  \n  # Create sf object\n  sf_points &lt;- sf::st_as_sf(lad_df_clean, coords = c(\"X\", \"Y\"), crs = 25832)  # EPSG 25832 = UTM Zone 32N\n  \n  # Write to GPKG\n  sf::st_write(sf_points, out_gpkg, layer = layer_name, delete_layer = TRUE)\n} \nDownload new_utils.R",
    "crumbs": [
      "ENVI-met 3DPLANTs from ALS Data"
    ]
  },
  {
    "objectID": "doc/microclimate_ALS_tc_v4.html#main-pipeline-from-microclimate_als_tc_v4.r",
    "href": "doc/microclimate_ALS_tc_v4.html#main-pipeline-from-microclimate_als_tc_v4.r",
    "title": "ENVI-met 3DPLANTs from ALS Data",
    "section": "Main pipeline from microclimate_ALS_tc_v4.R",
    "text": "Main pipeline from microclimate_ALS_tc_v4.R\nWarning in readLines(\"../src/microclimate_ALS_tc_v4.R\"): incomplete final line\nfound on '../src/microclimate_ALS_tc_v4.R'\n #' --- ENVI-met 3DPLANT column generator from voxelized ALS data ---\n#' Full pipeline from ALS voxelization to XML export for ENVI-met 3DPLANT.\n#' Includes normalization, topographic metrics, LAD calculation, clustering, and XML writing.\n\n# === Load Libraries ===\nlibrary(lidR)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sf)\nlibrary(here)\nlibrary(XML)\nlibrary(stats)\nlibrary(tibble)\nlibrary(rprojroot)\nlibrary(tools)\nlibrary(RANN)\nlibrary(clusternomics)\nlibrary(e1071)\nlibrary(entropy)\nlibrary(NbClust)\nlibrary(matrixStats)\n\n# === Configuration ===\nts &lt;- data.frame(\n  ID = 1:12,\n  value = c(\"agriculture\", \"alder\", \"ash\", \"beech\", \"douglas_fir\", \"larch\",\n            \"oak\", \"pastures\", \"roads\", \"settlements\", \"spruce\", \"water\")\n)\nvalid_species &lt;- c(\"alder\", \"ash\", \"beech\", \"douglas_fir\", \"larch\", \"oak\", \"spruce\")\nvalid_ids &lt;- ts$ID[ts$value %in% valid_species]\nplant_prefix=\"SYN\"\nvisualize &lt;- FALSE\nlas_file &lt;- here(\"data/ALS/tiles/\")\nres_xy &lt;- 2\nres_z &lt;- 2\nk &lt;- 0.3\nscale_factor &lt;- 1.2\ncrs_code &lt;- 25832\noutput_gpkg &lt;- \"data/envimet/envimet_p3dtree_points.gpkg\"\nxml_output_file &lt;- \"data/envimet/als_envimet_trees.pld\"\nspecies_raster &lt;- rast(\"data/aerial/treespecies_cleaned.tif\")\n\ndir.create(\"data/output\", showWarnings = FALSE, recursive = TRUE)\nsource(\"src/new_utils.R\")\n\n# === ⬛ STAGE: LAS Merging and Normalization ===\nlas_fn &lt;- merge_las_tiles(las_file, \"data/ALS/merged_output.laz\", chunk_size = 10000, workers = 6)\nlas &lt;- readLAS(las_fn)\ncrs(las) &lt;- \"EPSG:25832\"\n\n# === ⬛ STAGE: Ground Classification and DEM/CHM Generation ===\n# ⚠ MEMORY: large point cloud + rasters\nchm_pre &lt;- rasterize_canopy(las, res = res_xy, algorithm = pitfree(c(0,1,3,6,9,12,16)))\nrugosity &lt;- terra::focal(chm_pre, w = matrix(1, 3, 3), fun = sd, na.rm = TRUE)\nmean_rug &lt;- global(rugosity, fun = \"mean\", na.rm = TRUE)[[1]]\n\ncsf_params &lt;- if (mean_rug &gt; 1) {\n  message(\"Detected complex/dense canopy – using fine CSF settings\")\n  csf(cloth_resolution = 0.5, rigidness = 2, class_threshold = 0.4, iterations = 800)\n} else {\n  message(\"Detected open canopy – using coarse CSF settings\")\n  csf(cloth_resolution = 1.5, rigidness = 4, class_threshold = 0.6, iterations = 300)\n}\n\nlas &lt;- classify_ground(las, csf_params)\ndem_algo &lt;- eval(parse(text = recommend_dem_interpolation(las, res_xy)))\ndem &lt;- rasterize_terrain(las, res = res_xy, algorithm = dem_algo)\ndsm &lt;- rasterize_canopy(las, res = res_xy, algorithm = p2r())\n\nlas_norm &lt;- normalize_height(las, algorithm = knnidw(k = 6L, p = 2))\n\n# ✅ Free original LAS\n#las &lt;- NULL; gc()\n\n# === ⬛ STAGE: CHM + DSM + Topographic Metrics ===\npit_algo &lt;- pitfree(c(0, 1, 3, 6, 9, 12, 16))\nchm &lt;- rasterize_canopy(las_norm, res = res_xy, algorithm = pit_algo)\nslope &lt;- terrain(dem, \"slope\", unit = \"radians\")\naspect &lt;- terrain(dem, \"aspect\", unit = \"degrees\")\nTPI &lt;- terra::focal(terrain(dsm, \"TPI\"), w = matrix(1,3,3), fun = mean)\nTPI[TPI &lt; 0] &lt;- -1; TPI[TPI &gt; 0] &lt;- 1\n\ntopo &lt;- c(dem, dsm, chm, slope, aspect, TPI)\nnames(topo) &lt;- c(\"dem\", \"dsm\", \"chm\", \"slope\", \"aspect\", \"TPI\")\nwriteRaster(topo, \"data/ALS/topo_stack.tif\", overwrite = TRUE)\nplot(topo)\n\n# === ⬛ STAGE: Voxelization + LAD ===\n# ⚠ MEMORY: voxel and LAD matrices can be huge\nvoxels &lt;- preprocess_voxels(las_norm, res_xy, res_z)\nlad_df &lt;- convert_to_LAD_beer(voxels, grainsize = res_z, k = k, scale_factor = scale_factor)\nsaveRDS(voxels,\"data/voxels.rds\")\n\n# ✅ Free voxelized LAS\n#las_norm &lt;- NULL; voxels &lt;- NULL; gc()\n\n\n\n# === ⬛ STAGE: Compute structural indices ===\n# Only keep numeric LAD columns\n\nlad_matrix &lt;- select(lad_df, starts_with(\"lad_\")) |&gt; as.matrix()\nrownames(lad_matrix) &lt;- paste(lad_df$X, lad_df$Y, sep = \"_\")\n\n# More efficient summary statistics\nlad_df$LAD_mean        &lt;- matrixStats::rowMeans2(lad_matrix, na.rm = TRUE)\nlad_df$LAD_max         &lt;- matrixStats::rowMaxs(lad_matrix, na.rm = TRUE)\nlad_df$LAD_height_max  &lt;- max.col(lad_matrix, ties.method = \"first\") * res_z\n\n# Skewness, kurtosis, entropy — still using apply (no native rowSkewness etc.)\nlad_df$LAD_skewness    &lt;- apply(lad_matrix, 1, skewness)\ngc()\nlad_df$LAD_kurtosis    &lt;- apply(lad_matrix, 1, kurtosis)\ngc()\nlad_df$LAD_entropy     &lt;- apply(lad_matrix, 1, entropy)\ngc()\n\n# === ⬛ STAGE: Ecological indices ===\nn_layers &lt;- ncol(lad_matrix)\ntop_third &lt;- seq(ceiling(2 * n_layers / 3), n_layers)\nlad_df$Gap_Fraction &lt;- rowMeans(lad_matrix == 0, na.rm = TRUE)\nlad_df$Canopy_Cover_Top &lt;- rowMeans(lad_matrix[, top_third] &gt; 0, na.rm = TRUE)\nlad_df$LAD_CV &lt;- apply(lad_matrix, 1, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE))\n\n# Vertical evenness\nlad_sums &lt;- rowSums(lad_matrix, na.rm = TRUE)\nlad_df$Vertical_Evenness &lt;- sapply(seq_len(nrow(lad_matrix)), function(i) {\n  p &lt;- lad_matrix[i, ] / lad_sums[i]\n  p &lt;- p[p &gt; 0]\n  -sum(p * log(p)) / log(length(p))\n})\n\n# ✅ Remove LAD matrix if done\ngc()\n\n# === ⬛ STAGE: Add topographic & species data ===\ntopo_stack &lt;- rast(\"data/ALS/topo_stack.tif\")\n\n# Erzeuge ein sf-Objekt aus lad_df\nsf_lad &lt;- st_as_sf(as.data.frame(lad_df), coords = c(\"X\", \"Y\"), crs = crs_code)\ngeom_only &lt;- st_geometry(sf_lad)\nlad_df$elev   &lt;- exactextractr::exact_extract( topo_stack[[\"dem\"]], st_buffer(geom_only, dist = 0.1), \"mean\")\nlad_df$slope  &lt;- exactextractr::exact_extract( topo_stack[[\"slope\"]], st_buffer(geom_only, dist = 0.1), \"mean\")\nlad_df$aspect &lt;- exactextractr::exact_extract( topo_stack[[\"aspect\"]], st_buffer(geom_only, dist = 0.1), \"mean\")\nlad_df$TPI    &lt;- exactextractr::exact_extract( topo_stack[[\"TPI\"]], st_buffer(geom_only, dist = 0.1), \"mean\")\nlad_df$CHM    &lt;- exactextractr::exact_extract( topo_stack[[\"chm\"]], st_buffer(geom_only, dist = 0.1), \"mean\")\nlad_df$species_class &lt;- exactextractr::exact_extract( species_raster, st_buffer(geom_only, dist = 0.1), \"mean\")\n\nsaveRDS(lad_df,\"data/lad_df.rds\")\n\n# ✅ Remove sf and stack\n#sf_lad &lt;- NULL; topo_stack &lt;- NULL;geom_only=NULL; gc()\n\n# === ⬛ STAGE: Combine data for clustering ===\nlad_matrix &lt;- lad_df %&gt;%\n  select(starts_with(\"lad_pulses_\"), starts_with(\"LAD_\"),LAD_skewness,LAD_kurtosis,LAD_CV ,LAD_entropy , Vertical_Evenness) %&gt;%\n  as.matrix()\nrownames(lad_matrix) &lt;- paste(lad_df$X, lad_df$Y, sep = \"_\")\n\n# === ⬛ STAGE: Filter for valid species and complete rows ===\nvalid_idx &lt;- lad_df$species_class %in% valid_ids\nlad_df &lt;- lad_df[valid_idx, ]\nlad_matrix &lt;- lad_matrix[valid_idx, ]\n\npulse_cols &lt;- grep(\"^lad_pulses_\", colnames(lad_matrix), value = TRUE)\nlad_pulses &lt;- lad_matrix[, pulse_cols]\n\n# Filter valid rows for clustering\nvalid_rows &lt;- apply(lad_pulses, 1, function(x) all(is.finite(x) & !is.na(x)))\nlad_pulses &lt;- lad_pulses[valid_rows, ]\nlad_df &lt;- lad_df[valid_rows, ]\n\n# === ⬛ STAGE: PCA sampling + NbClust ===\nset.seed(42)\nsample_idx &lt;- sample(seq_len(nrow(lad_pulses)), floor(nrow(lad_pulses) * 0.01))\nsample_data &lt;- lad_pulses[sample_idx, ]\n\ncat(\"Sample size before NA filtering:\", nrow(sample_data), \"\\n\")\nkeep_cols &lt;- which(colMeans(is.na(sample_data)) &lt;= 0.2)\nsample_data &lt;- sample_data[, keep_cols]\nsample_data &lt;- sample_data[, apply(sample_data, 2, var, na.rm = TRUE) &gt; 1e-10]\nsample_data &lt;- na.omit(sample_data)\ncat(\"Sample size after cleaning:\", nrow(sample_data), \"\\n\")\n\n# ⚠ MEMORY: PCA and NbClust can explode RAM use\npca_res &lt;- prcomp(sample_data, scale. = TRUE)\npc_info &lt;- suggest_n_pcs(pca_res, variance_cutoff = 0.9)\nsample_data_pca &lt;- pca_res$x[, 1:pc_info$n_pcs]\n\nnb &lt;- NbClust(sample_data_pca, distance = \"euclidean\", min.nc = 2, max.nc = 30, method = \"kmeans\")\noptimal_k &lt;- as.integer(names(which.max(table(nb$Best.nc[1, ]))))\n\n\n# Best number of clusters\ncat(\"🔢 Best number of clusters (Best.nc):\", optimal_k, \"\\n\\n\")\n\n        # --- Bereinige ungültige Zeilen für Clustering ---\n        valid_rows &lt;- apply(lad_pulses, 1, function(x) all(is.finite(x) & !is.na(x)))\n        lad_pulses &lt;- lad_pulses[valid_rows, ]\n        valid_rows &lt;- apply(lad_df, 1, function(x) all(is.finite(x) & !is.na(x)))\n        lad_df_clean &lt;- lad_df[valid_rows, ]\n\n        # 1. Clustering-Spalten selektieren\n        lad_dftocluster &lt;- lad_df_clean %&gt;%\n          select(starts_with(\"lad_pulses_\"),\n                 starts_with(\"LAD_\"),\n                 LAD_skewness, LAD_kurtosis, LAD_CV, LAD_entropy, Vertical_Evenness) %&gt;%\n          mutate(across(everything(), ~ ifelse(. &lt;= 2e-5, 0, .)))\n        \n        #lad_df &lt;- lad_df[valid_rows, ]\n        lad_features &lt;- lad_df_clean %&gt;%\n          select(starts_with(\"lad_pulses_\"), starts_with(\"LAD_\"), LAD_skewness, LAD_kurtosis, LAD_CV, LAD_entropy, Vertical_Evenness) %&gt;%\n          mutate(across(everything(), ~ ifelse(. &lt;= 2e-5, 0, .)))   \n        # Entferne leere Zeilen\n        lad_dftocluster &lt;- lad_dftocluster[rowSums(lad_dftocluster &gt; 0) &gt; 3, ]\n        \n        lad_dftocluster_scaled &lt;- scale(lad_dftocluster)\n  \n        \n# --- Clustering (KMeans_arma von ClusterR) ---\nkm_arma &lt;- ClusterR::KMeans_arma(\n  data = lad_dftocluster,\n  clusters = optimal_k,\n  n_iter = 100,\n  seed_mode = \"random_subset\"\n)\n\n# --- Cluster-Zuweisung berechnen ---\nlad_df_clean$cluster &lt;- as.integer(\n  ClusterR::predict_KMeans(\n    data = lad_dftocluster,\nkm_arma\n  )\n)\n\nlibrary(dplyr)\n\n# Schritt 1: Synthetische Mittelprofile erzeugen (alle LADs mitteln je Cluster)\ncluster_profiles &lt;- lad_df_clean %&gt;%\n  group_by(cluster) %&gt;%\n  summarise(across(starts_with(\"lad_\"), mean, na.rm = TRUE), .groups = \"drop\")\n\n# Schritt 2: Eindeutige ENVIMET_ID je Cluster erzeugen\ncluster_profiles &lt;- cluster_profiles %&gt;%\n  mutate(ENVIMET_ID = paste0(plant_prefix, formatC(cluster, width = 3, flag = \"0\")))\n# Schritt 3: ID zurück an alle Originalpunkte hängen\nlad_df_clean &lt;- lad_df_clean %&gt;%\n  left_join(cluster_profiles %&gt;% select(cluster, ENVIMET_ID), by = \"cluster\")\n\n        # ENVI-met exportieren\n        export_envimet_profiles(\n          lad_df_clean = lad_df_clean,\n          res_z = 1  # oder 2 – je nachdem, wie deine Voxelhöhe lautet\n        )\n        \n        # microclimf Input exportieren\n        export_microclimf_profiles(\n          lad_df_clean = lad_df_clean\n        )\n        \n     export_full_lad_profiles_gpkg(\n          lad_df_clean = lad_df_clean,\n          out_gpkg = \"data/lad_profiles_full.gpkg\",\n          layer_name = \"lad_profiles\"\n        )\n        # --- Plot spatial distribution of clusters (optional diagnostics) ---\n        if (visualize) {\n          library(ggplot2)\n          ggplot(lad_df, aes(x = X, y = Y, color = as.factor(cluster))) +\n            geom_point(size = 0.8) +\n            coord_equal() +\n            scale_color_viridis_d(name = \"Cluster\") +\n            labs(title = \"Spatial distribution of LAD clusters\") +\n            theme_minimal()\n        }\n\n         \nDownload microclimate_ALS_tc_v4.R",
    "crumbs": [
      "ENVI-met 3DPLANTs from ALS Data"
    ]
  },
  {
    "objectID": "doc/new_utils2.html",
    "href": "doc/new_utils2.html",
    "title": "Function & Workflow Reference: LAD and ALS Processing",
    "section": "",
    "text": "This tutorial provides an advanced and reproducible workflow to generate ENVI-met 3DPLANT vegetation objects from Airborne Laser Scanning (ALS) data using voxel-based LAD profiles and species classification. It leverages expert functions with embedded documentation and explains the theory behind each computational step.",
    "crumbs": [
      "Function & Workflow Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils2.html#beerlambert-law-for-lad",
    "href": "doc/new_utils2.html#beerlambert-law-for-lad",
    "title": "Function & Workflow Reference: LAD and ALS Processing",
    "section": "Beer–Lambert Law for LAD",
    "text": "Beer–Lambert Law for LAD\n\\[\nLAD = -\\frac{\\ln(1 - p)}{k \\cdot \\Delta z}\n\\]\nWhere:\n\n\\(p\\) is the normalized proportion of pulses (relative to max column density)\n\\(k\\) is the extinction coefficient (typically 0.3–0.5)\n\\(\\Delta z\\) is the vertical slice height (e.g., 2 m)\n\nThis models beam extinction through foliage as a function of vertical occlusion.",
    "crumbs": [
      "Function & Workflow Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils2.html#lai-leaf-area-index",
    "href": "doc/new_utils2.html#lai-leaf-area-index",
    "title": "Function & Workflow Reference: LAD and ALS Processing",
    "section": "LAI (Leaf Area Index)",
    "text": "LAI (Leaf Area Index)\n\\[\nLAI = \\sum_{i=1}^{n} LAD_i \\cdot \\Delta z\n\\]\nThe vertical integral of LAD, optionally clipped at 95th percentile for stability.",
    "crumbs": [
      "Function & Workflow Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils2.html#vertical-entropy-evenness",
    "href": "doc/new_utils2.html#vertical-entropy-evenness",
    "title": "Function & Workflow Reference: LAD and ALS Processing",
    "section": "Vertical Entropy (Evenness)",
    "text": "Vertical Entropy (Evenness)\nGiven vertical slice fractions \\(p\\_i\\) (LAD share per layer):\n\\[\nH = -\\sum_{i=1}^{n} p_i \\log(p_i) \\quad\\text{(Shannon)}\n\\]\nNormalized:\n\\[\nH^* = \\frac{H}{\\log(n)}\n\\]\nAssesses uniformity of vertical LAD distribution.",
    "crumbs": [
      "Function & Workflow Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils2.html#kurtosis-skewness",
    "href": "doc/new_utils2.html#kurtosis-skewness",
    "title": "Function & Workflow Reference: LAD and ALS Processing",
    "section": "Kurtosis & Skewness",
    "text": "Kurtosis & Skewness\nFor LAD vector \\(x\\):\n\nSkewness:\n\n\\[\n\\gamma_1 = \\frac{E[(x - \\mu)^3]}{\\sigma^3}\n\\]\n\nKurtosis:\n\n\\[\n\\gamma_2 = \\frac{E[(x - \\mu)^4]}{\\sigma^4} - 3\n\\]",
    "crumbs": [
      "Function & Workflow Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils2.html#canopy-height-chm",
    "href": "doc/new_utils2.html#canopy-height-chm",
    "title": "Function & Workflow Reference: LAD and ALS Processing",
    "section": "Canopy Height (CHM)",
    "text": "Canopy Height (CHM)\nFrom DSM and DTM:\n\\[\nCHM = DSM - DEM\n\\]\nUsed to estimate maximum vegetation height per voxel column.",
    "crumbs": [
      "Function & Workflow Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils2.html#pca-component-selection",
    "href": "doc/new_utils2.html#pca-component-selection",
    "title": "Function & Workflow Reference: LAD and ALS Processing",
    "section": "PCA Component Selection",
    "text": "PCA Component Selection\n\\(\\lambda\\) (lambda) refers in PCA to the eigenvalue of a principal component, i.e. the amount of variance that component explains. λᵢ is then eigenvalue of the \\(i\\)-th principal component in PCA and represents how much variance that component explains. Larger λᵢ indicate more informative components.\n\nCumulative Variance\n\\[\n\\sum_{i=1}^k \\frac{\\lambda_i}{\\sum_j \\lambda_j} \\geq \\text{cutoff}\n\\]\n\n\nKaiser Criterion\nKeep \\(\\lambda\\_i &gt; 1\\)\n\n\nElbow Method\nFind \\(i\\) where:\n\\[\n\\Delta \\lambda_i = \\lambda_{i} - \\lambda_{i+1}\\text{ is minimal}\n\\]",
    "crumbs": [
      "Function & Workflow Reference: LAD and ALS Processing"
    ]
  },
  {
    "objectID": "doc/new_utils.html",
    "href": "doc/new_utils.html",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "",
    "text": "This document provides a detailed explanation of the core functions used in the ENVI-met 3DPLANT ALS pipeline. These functions handle voxelization, LAD conversion, trait computation, clustering, and export preparation.\nEach function is explained with:\n\nDescription and purpose\nInputs and outputs\nDetailed internal logic\nUsage examples (if applicable)"
  },
  {
    "objectID": "doc/new_utils.html#overview",
    "href": "doc/new_utils.html#overview",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "",
    "text": "This document provides a detailed explanation of the core functions used in the ENVI-met 3DPLANT ALS pipeline. These functions handle voxelization, LAD conversion, trait computation, clustering, and export preparation.\nEach function is explained with:\n\nDescription and purpose\nInputs and outputs\nDetailed internal logic\nUsage examples (if applicable)"
  },
  {
    "objectID": "doc/new_utils.html#preprocess_voxels",
    "href": "doc/new_utils.html#preprocess_voxels",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "preprocess_voxels()",
    "text": "preprocess_voxels()\n#' Preprocess ALS point cloud into voxel grid\n#'\n#' @param las_norm Normalized LAS object\n#' @param res_xy Horizontal resolution (in meters)\n#' @param res_z Vertical resolution (in meters)\n#'\n#' @return A list of voxel information used for LAD calculation\nThis function converts a normalized ALS point cloud into a structured voxel representation. Each voxel counts the number of returns in a fixed 3D grid.\nInternally uses grid_metrics() with z-based slicing to accumulate point counts."
  },
  {
    "objectID": "doc/new_utils.html#convert_to_lad_beer",
    "href": "doc/new_utils.html#convert_to_lad_beer",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "convert_to_LAD_beer()",
    "text": "convert_to_LAD_beer()\n#' Convert voxel pulse counts to LAD using Beer–Lambert Law\n#'\n#' @param df Data frame containing voxel counts per vertical bin (e.g. \"layer_1\", ..., \"layer_n\")\n#' @param grainsize Vertical voxel height (in meters)\n#' @param k Extinction coefficient (default 0.3–0.5)\n#' @param scale_factor Optional scaling multiplier\n#' @param lad_max Optional upper LAD threshold\n#' @param lad_min Optional lower LAD threshold\n#' @param keep_pulses Whether to retain original pulse columns\n#'\n#' @return Data frame with LAD values by voxel column\n\nExplanation\nThis function applies the Beer–Lambert law to pulse return counts to estimate LAD per voxel. The formula is:\n\\[\n\\text{LAD}_i = -\\frac{\\ln(1 - \\frac{N_i}{N_{max}})}{k \\cdot \\Delta z}\n\\]\n\n\\(N_i\\): Number of pulses in voxel \\(i\\)\n\\(N_{max}\\): Maximum number of pulses in that column\n\\(k\\): Extinction coefficient\n\\(\\Delta z\\): Voxel height\n\nThe result is a per-voxel LAD profile suitable for vertical vegetation structure modeling."
  },
  {
    "objectID": "doc/new_utils.html#compute_traits_from_lad",
    "href": "doc/new_utils.html#compute_traits_from_lad",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "compute_traits_from_lad()",
    "text": "compute_traits_from_lad()\n#' Compute plant traits from LAD profile\n#'\n#' @param lad_df Data frame with LAD profile (long format)\n#' @param res_z Voxel height (in meters)\n#'\n#' @return Data frame with structural traits per ENVIMET_ID\nComputes key biophysical traits needed by ENVI-met 3DPLANT:\n\nLAI (Leaf Area Index)\nMax LAD\nCrown Height\nTotal Height\nRoughness Length\nLeaf Thickness (from species table)\nLAD Cutoff (default: inferred)\n\nThese are required for synthetic plant representation in .pld files."
  },
  {
    "objectID": "doc/new_utils.html#export_lad_to_envimet_p3d",
    "href": "doc/new_utils.html#export_lad_to_envimet_p3d",
    "title": "Function Reference: LAD and ALS Processing",
    "section": "export_lad_to_envimet_p3d()",
    "text": "export_lad_to_envimet_p3d()\n#' Export clustered LAD profiles to ENVI-met PLANT3D XML\n#'\n#' @param lad_df LAD profile in long format with traits and species info\n#' @param file_out Path to output `.pld` file\n#' @param res_z Vertical resolution (meters)\n#' @param trait_df Optional trait table to override defaults\nGenerates a PLANT3D-compliant .pld file with:\n\n&lt;plant&gt; entries for each cluster (with &lt;LAD&gt; series)\nMetadata including LAI, MaxLAD, RoughnessLength, etc.\n&lt;plantclass&gt; label and species name\n\nEach vertical LAD profile is saved per ENVIMET_ID for use in simulation.\n\nLet me know if you’d like to: - Include example plots - Add function internals (e.g., how LAD cutoff is inferred) - Integrate this into your main .qmd pipeline tutorial - Export to PDF or GitHub Pages"
  },
  {
    "objectID": "doc/osm2envi.html",
    "href": "doc/osm2envi.html",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "",
    "text": "The OSM2Envi_met QGIS Processing tool provides a fully automated workflow for preparing spatial input data from OpenStreetMap (OSM) and elevation datasets for use in ENVI-met 3D simulations. It combines a Python-based QGIS interface with a shell script that performs a robust geospatial preprocessing pipeline via qgis_process, gdal, and ogr2ogr.\nThis document describes how to install the tool, explains the internal processing workflow, and provides a link to the complete codebase.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#requirements",
    "href": "doc/osm2envi.html#requirements",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "Requirements",
    "text": "Requirements\nTo run this tool successfully, you need:\n\nQGIS 3.28+ with qgis_process installed and accessible from the command line\nGDAL with ogr2ogr and gdal_calc.py\nA Bash shell (macOS/Linux, or Git Bash on Windows)",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#setup-steps",
    "href": "doc/osm2envi.html#setup-steps",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "Setup Steps",
    "text": "Setup Steps\n\nDownload the tool:\n\nFrom GitHub as ZIP:\nhttps://github.com/gisma/qgis-processing-workflows/archive/refs/heads/main.zip\nOr clone via Git: git clone https://github.com/gisma/qgis-processing-workflows.git\n\nCopy the relevant scripts to your QGIS processing script directory:\n~/.local/share/QGIS/QGIS3/profiles/default/processing/scripts/ ├── osm2envi_qgis.sh # Main processing Bash script └── osm2envi_tool.py # QGIS Processing tool wrapper\nMake the Bash script executable:\nchmod +x osm2envi_qgis.sh\nRestart QGIS.\nThe tool will now appear under: Processing Toolbox → Envi_met Tools → OSM2Envi_met",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#osm-conversion",
    "href": "doc/osm2envi.html#osm-conversion",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "1. OSM Conversion",
    "text": "1. OSM Conversion\n\nConverts the .osm file into a multi-layered GeoPackage using ogr2ogr.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#feature-extraction",
    "href": "doc/osm2envi.html#feature-extraction",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "2. Feature Extraction",
    "text": "2. Feature Extraction\n\nExtracts thematic layers from multipolygons or lines using SQL expressions:\n\nVegetation: landuse like forest, meadow, orchard, etc.\nSurfaces: roads and natural surfaces from highway and landuse\nBuildings: all OSM geometries tagged as building\n\nEach layer is:\n\nReprojected into the target CRS using qgis_process\nClipped to the specified extent",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#classification-envimet-id",
    "href": "doc/osm2envi.html#classification-envimet-id",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "3. Classification (ENVIMET ID)",
    "text": "3. Classification (ENVIMET ID)\n\nAssigns an ENVIMET_ID attribute based on feature type:\n\nForest → 0000SM\nAsphalt → 0200AK\nIndustrial landuse → 0200AK\nWetland → 0200LI\n\nClassification is done using SQL CASE statements in qgis_process:fieldcalculator.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#height-extraction-optional",
    "href": "doc/osm2envi.html#height-extraction-optional",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "4. Height Extraction (optional)",
    "text": "4. Height Extraction (optional)\nIf both DSM and DEM are provided:\n\nCalculates a difference raster (DSM − DEM) using gdal_calc.py\nComputes mean building heights (height_mean) via zonal statistics\nAdds height attribute to the building layer\n\nIf no elevation data is provided, this step is skipped.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#road-buffering",
    "href": "doc/osm2envi.html#road-buffering",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "5. Road Buffering",
    "text": "5. Road Buffering\n\nBuffers road geometries based on highway type:\n\nPrimary: 10 m, Secondary: 6 m, Tertiary: 4 m, Track: 1 m\n\nBuffers are classified like surfaces and merged later.",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#layer-merging",
    "href": "doc/osm2envi.html#layer-merging",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "6. Layer Merging",
    "text": "6. Layer Merging\n\nMerges:\n\nBuffered roads\nSurface landuse polygons\n\nCreates a unified surface layer for ENVIMET (*_surface_final.gpkg)",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#cleanup",
    "href": "doc/osm2envi.html#cleanup",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "7. Cleanup",
    "text": "7. Cleanup\n\nDeletes all intermediate files:\n\n_tmp.gpkg, _proj.gpkg, _clip.gpkg\n\nKeeps only the final classified and merged output layers",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/osm2envi.html#optimization-for-envi-met",
    "href": "doc/osm2envi.html#optimization-for-envi-met",
    "title": "QGIS ENVIMET Preprocessor",
    "section": "8. Optimization for ENVI-met",
    "text": "8. Optimization for ENVI-met\n\nRetains only necessary fields:\n\nGeometry\nENVIMET_ID\nheight_mean (if computed)\n\nSaves final layers as:\n\n*_surface_final_envimet.gpkg\n*_vegetation_final_envimet.gpkg\n*_buildings_final_envimet.gpkg",
    "crumbs": [
      "QGIS ENVIMET Preprocessor"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html",
    "href": "doc/fluxes_corrected.html",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "",
    "text": "This tutorial calculates surface energy balance components using micrometeorological observations from an energy balance station. We implement a simplified diagnostic approach based on physical theory and field measurements.\n\n\nUnderstanding the surface energy balance is essential for quantifying the partitioning of energy into heating, evaporation, and ground storage. This notebook follows a physically grounded approach to derive the key energy fluxes from field observations.\n\n\n\nThe fundamental balance at the land surface is:\n\\[ R_n = H + LE + G + S \\]\nWhere:\n\n\\(R_n\\): Net radiation \\(W/m²\\) — total energy input from shortwave and longwave radiation\n\n\\(H\\): Sensible heat flux \\(W/m²\\) — convective transfer of heat to the air\n\n\\(LE\\): Latent heat flux \\(W/m²\\) — energy used for evapotranspiration\n\n\\(G\\): Ground heat flux \\(W/m²\\) — conduction of heat into the soil\n\n\\(S\\): Storage term (e.g. canopy heat, biomass, or air column storage; neglected here)\n\nAssumption: For half-hourly or hourly timesteps and shallow sensors, we assume \\(S \\approx 0\\)$.\nTherefore, we estimate:\n\\[LE = R_n - G - H\\] ### Data Used in This Analysis\n\n\n\n\n\n\n\n\nVariable\nMeaning\nSource\n\n\n\n\nrad_bil\nApproximate net radiation (R_n)\nMeasured via 4-component radiometer or surrogate\n\n\nheatflux_soil\nGround heat flux (G)\nSoil heat flux plate\n\n\nTa_2m, Ta_10m\nAir temperature at 2 m and 10 m\nThermistors\n\n\nWindspeed_2m, Windspeed_10m\nWind speed at two heights\nAnemometers\n\n\n\nIn surface energy balance analysis, the bulk transfer and residual methods complement each other:\n\nThe bulk method estimates the sensible heat flux (\\(H\\)) using measured temperature and wind gradients.\nThe residual method then infers the latent heat flux (\\(LE\\)) by closing the energy balance using the measured net radiation (\\(R_n\\)), soil heat flux (\\(G\\)), and calculated \\(H\\).\n\nThis creates a simple yet effective hybrid approach: - Physically based: \\(H\\) is grounded in turbulence theory. - Energetically constrained: \\(LE\\) ensures energy conservation at the surface.\nTogether, they enable complete estimation of surface fluxes from standard meteorological data.",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#introduction",
    "href": "doc/fluxes_corrected.html#introduction",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "",
    "text": "This tutorial calculates surface energy balance components using micrometeorological observations from an energy balance station. We implement a simplified diagnostic approach based on physical theory and field measurements.\n\n\nUnderstanding the surface energy balance is essential for quantifying the partitioning of energy into heating, evaporation, and ground storage. This notebook follows a physically grounded approach to derive the key energy fluxes from field observations.\n\n\n\nThe fundamental balance at the land surface is:\n\\[ R_n = H + LE + G + S \\]\nWhere:\n\n\\(R_n\\): Net radiation \\(W/m²\\) — total energy input from shortwave and longwave radiation\n\n\\(H\\): Sensible heat flux \\(W/m²\\) — convective transfer of heat to the air\n\n\\(LE\\): Latent heat flux \\(W/m²\\) — energy used for evapotranspiration\n\n\\(G\\): Ground heat flux \\(W/m²\\) — conduction of heat into the soil\n\n\\(S\\): Storage term (e.g. canopy heat, biomass, or air column storage; neglected here)\n\nAssumption: For half-hourly or hourly timesteps and shallow sensors, we assume \\(S \\approx 0\\)$.\nTherefore, we estimate:\n\\[LE = R_n - G - H\\] ### Data Used in This Analysis\n\n\n\n\n\n\n\n\nVariable\nMeaning\nSource\n\n\n\n\nrad_bil\nApproximate net radiation (R_n)\nMeasured via 4-component radiometer or surrogate\n\n\nheatflux_soil\nGround heat flux (G)\nSoil heat flux plate\n\n\nTa_2m, Ta_10m\nAir temperature at 2 m and 10 m\nThermistors\n\n\nWindspeed_2m, Windspeed_10m\nWind speed at two heights\nAnemometers\n\n\n\nIn surface energy balance analysis, the bulk transfer and residual methods complement each other:\n\nThe bulk method estimates the sensible heat flux (\\(H\\)) using measured temperature and wind gradients.\nThe residual method then infers the latent heat flux (\\(LE\\)) by closing the energy balance using the measured net radiation (\\(R_n\\)), soil heat flux (\\(G\\)), and calculated \\(H\\).\n\nThis creates a simple yet effective hybrid approach: - Physically based: \\(H\\) is grounded in turbulence theory. - Energetically constrained: \\(LE\\) ensures energy conservation at the surface.\nTogether, they enable complete estimation of surface fluxes from standard meteorological data.",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#load-libraries-and-data",
    "href": "doc/fluxes_corrected.html#load-libraries-and-data",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "Load Libraries and Data",
    "text": "Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(units)\n\nenergy &lt;- read_csv(here::here(\"data\", \"energie_bil_wiese.csv\")) %&gt;%\n  mutate(datetime = dmy_hm(datetime))",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#preprocessing",
    "href": "doc/fluxes_corrected.html#preprocessing",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "Preprocessing",
    "text": "Preprocessing\n\n\nCode\nenergy &lt;- energy %&gt;%\n  rename(\n    Rn = rad_bil,\n    G = heatflux_soil,\n    Ta_2m = Ta_2m,\n    Ta_10m = Ta_10m,\n    WS_2m = Windspeed_2m,\n    WS_10m = Windspeed_10m\n  ) %&gt;%\n  mutate(\n    delta_T = Ta_2m - Ta_10m,\n    WS_mean = (WS_2m + WS_10m) / 2,\n    month_num = month(datetime),\n    month_label = case_when(\n      month_num == 6 ~ \"June\",\n      month_num == 11 ~ \"November\",\n      TRUE ~ as.character(month(datetime, label = TRUE))\n    )\n  )",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#sensible-heat-flux-h-bulk-transfer-method",
    "href": "doc/fluxes_corrected.html#sensible-heat-flux-h-bulk-transfer-method",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "Sensible Heat Flux (H): Bulk Transfer Method",
    "text": "Sensible Heat Flux (H): Bulk Transfer Method\nSensible heat flux (H) is the rate at which thermal energy is transferred from the Earth’s surface to the atmosphere due to a temperature difference. It is a critical term in the surface energy balance and is especially important in meteorology, hydrology, and micrometeorology.\nIt describes how warm the surface is compared to the air above it — and how turbulent air motion carries that heat away. The bulk transfer formulation for (H) assumes a logarithmic wind profile and neutral atmospheric conditions short explanation adding to the scalar laws.\n\\[ H = \\rho c_p \\cdot \\frac{\\Delta T}{r_a} \\]\nWhere:\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(H\\)\nSensible heat flux \\(W/m²\\)\n\n\n\\(rho\\)\nAir density \\(kg/m³\\) (typically ≈ 1.225 at sea level)\n\n\n\\(c_p\\)\nSpecific heat of air at constant pressure \\(J/kg/K\\) \\(≈ 1005 J/kg/K\\)\n\n\n\\(Delta T\\)\nTemperature difference between two heights: \\(T\\_{z2} - T\\_{z1}\\) \\(K\\)\n\n\n\\(r_a\\)\nAerodynamic resistance to heat transfer \\(s/m\\)\n\n\n\n\nAerodynamic Resistance \\(r_a\\)\nAerodynamic resistance quantifies how turbulent air mixes heat. It depends on wind speed, measurement height, and surface roughness.\nFor a flat and open surface, and assuming a neutral atmosphere:\n\\[r_a = \\frac{\\ln(z_2/z_1)}{k \\cdot u_{mean}}\\]\nWhere:\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(z_1\\)\nLower measurement height (e.g. 2 m)\n\n\n\\(z_2\\)\nUpper measurement height (e.g. 10 m)\n\n\n\\(k\\)\nvon Kármán constant (≈ 0.41)\n\n\n\\(u\\_{mean}\\)\nMean horizontal wind speed between \\(z_1\\) and \\(z_2\\) \\(m/s\\)\n\n\n\n\nThis formula assumes: - horizontally homogeneous surface, - fully turbulent flow, - negligible atmospheric stability effects (i.e. neutral conditions).\n\n\n\n\n\n\n\nWind profile, buoyancy effects and turbulent scalar transport\n\n\n\n\n\n\nNote on logarithmic wind profile\nA logarithmic wind profile means that wind speed increases with height according to:\n\\[\n  u(z) = \\frac{u_*}{k} \\ln\\left(\\frac{z}{z_0}\\right)\n\\]\nwhere \\(u_*\\) is the friction velocity, \\(k\\) the von Kármán constant, and \\(z_0\\) the roughness length. This profile arises from surface-layer theory under turbulent, steady conditions.\n\n\nNote on Diagnosing Atmospheric Stability\nTo determine whether neutral atmospheric stratification applies, we estimate the Obukhov length \\(L\\), a key parameter from Monin–Obukhov similarity theory:\n\\[\nL = -\\frac{\\rho \\cdot c_p \\cdot T \\cdot u_*^3}{k \\cdot g \\cdot H}\n\\]\nwhere:\n\n\\(\\rho = 1.225\\,\\text{kg/m}^3\\): air density\n\n\\(c_p = 1005\\,\\text{J/kg/K}\\): specific heat of air\n\n\\(T\\): mean air temperature (in K)\n\n\\(u_*\\): friction velocity (approximated)\n\n\\(k = 0.41\\): von Kármán constant\n\n\\(g = 9.81\\,\\text{m/s}^2\\): gravitational acceleration\n\n\\(H\\): sensible heat flux (W/m²)\n\nWe estimate \\(u_*\\) from wind speed using the log-profile equation:\n\\[\nu_* = \\frac{k \\cdot \\bar{u}}{\\ln(z_2 / z_1)}\n\\]\nand compute:\n\n\\(T = (T_{2m} + T_{10m})/2 + 273.15\\)\n\\(\\bar{u} = (u_{2m} + u_{10m}) / 2\\)\n\nThe dimensionless height \\(z/L\\) is then used to classify stability:\n\n\\(|z/L| &lt; 0.1\\) → Neutral\n\\(z/L &gt; 0.1\\) → Stable\n\\(z/L &lt; -0.1\\) → Unstable\n\nThis allows us to screen the dataset and verify whether bulk formulations assuming neutral stratification are valid at given time steps.\n\n\nNote on Scalar Transport Laws\nTurbulent scalar transport refers to the movement of quantities like temperature or humidity due to turbulent eddies in the atmosphere. These fluxes follow a gradient–flux relationship:\n\\[\nF_\\phi = -K_\\phi \\cdot \\frac{\\partial \\phi}{\\partial z}\n\\]\nwhere:\n\n\\(F_\\phi\\): vertical flux of scalar \\(\\phi\\) (e.g. \\(T\\), \\(q\\)),\n\\(K_\\phi\\): eddy diffusivity [m²/s],\n\\(\\frac{\\partial \\phi}{\\partial z}\\): vertical gradient of the scalar.\n\nIn field applications, this is simplified into bulk transfer formulas like:\n\\[\nH = \\rho \\cdot c_p \\cdot \\frac{\\Delta T}{r_a}\n\\]\nwhich relate scalar differences to vertical fluxes via aerodynamic resistance \\(r_a\\).\n\n\n\n\n\n\nAssumptions and Limitations\n\n\n\n\n\n\n-   Assumes neutral stability — ignores buoyancy effects (unstable/stable conditions).\n\n-   Requires accurate wind and temperature measurements.\n\n-   Works best over homogeneous, flat terrain.\n\n-   May underestimate or overestimate fluxes under low wind or very moist/dry conditions.\n\n\n\n\n\nCode\n# Constants\nrho_air &lt;- set_units(1.225, \"kg/m^3\")\ncp_air &lt;- set_units(1005, \"J/kg/K\")\nz1 &lt;- 2\nz2 &lt;- 10\nk &lt;- 0.41\n\nenergy &lt;- energy %&gt;%\n  mutate(\n    ra = log(z2 / z1) / (k * WS_mean),\n    H = drop_units(rho_air * cp_air * delta_T / ra)\n  )",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#latent-heat-flux-le-residual-method",
    "href": "doc/fluxes_corrected.html#latent-heat-flux-le-residual-method",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "Latent Heat Flux \\(LE\\): Residual Method",
    "text": "Latent Heat Flux \\(LE\\): Residual Method\nThe latent heat flux \\(LE\\) quantifies the energy used for evapotranspiration — the combined loss of water to the atmosphere by:\n\nevaporation from soil and wet surfaces,\ntranspiration through stomata in plants, and\nevaporation of intercepted rainfall from canopy surfaces.\n\n\nResidual Energy Balance Equation\nWhen direct measurement is unavailable, we estimate \\(LE\\) as the residual of the surface energy balance by subtracting all the known energy components from the total incoming energy:\n\\[\nLE = R_n - G - H\n\\]\nWhere:\n\n\\(LE\\): latent heat flux [W/m²]\n\\(R_n\\): net radiation [W/m²] (incoming − outgoing radiation)\n\\(G\\): ground heat flux [W/m²] (into or out of the soil)\n\\(H\\): sensible heat flux [W/m²] (convective heat to the air)\n\n\n\nAssumptions and Cautions\n\nAssumes energy balance closure (no significant storage (S)).\nErrors in \\(R_n\\), \\(G\\), or \\(H\\) directly affect \\(LE\\).\nNighttime or low-flux periods often show non-physical negative (LE).",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#linking-bulk-and-residual-approaches",
    "href": "doc/fluxes_corrected.html#linking-bulk-and-residual-approaches",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "Linking Bulk and Residual Approaches",
    "text": "Linking Bulk and Residual Approaches\nTo estimate the surface energy balance components, we combine two complementary methods:\n\nSensible heat flux \\(H\\) is derived using the bulk transfer approach, which relies on measured temperature and wind speed differences at two heights.\nLatent heat flux \\(LE\\) is then calculated as the residual energy — the portion of net radiation not used for heating the air (\\(H\\)) or the soil (\\(G\\)).\n\n\n\n\n\n\n\n\n\nComponent\nBulk Transfer Method\nResidual Method\n\n\n\n\nWhat it estimates\nSensible heat flux (\\(H\\))\nLatent heat flux (\\(LE\\))\n\n\nRequired inputs\n\\(\\Delta T\\) (temp. difference), \\(\\bar{u}\\) (mean wind speed)\n\\(R_n\\) (net radiation), \\(G\\) (ground heat), and \\(H\\)\n\n\nPhysical principle\nTurbulent heat transport via scalar gradient\nEnergy conservation (surface energy balance)\n\n\nFormula used\n\\(H = \\rho \\cdot c_p \\cdot \\dfrac{\\Delta T}{r_a}\\)\n\\(LE = R_n - G - H\\)\n\n\nKey assumption\nLogarithmic wind profile, neutral stratification\nNegligible storage term \\(S \\approx 0\\)\n\n\nOutput in your case\n\\(H = 251\\,\\text{W/m}^2\\) (based on realistic inputs)\n\\(LE = 289\\,\\text{W/m}^2\\) (by residual)\n\n\n\n\nStep 1: Bulk Estimation of Sensible Heat Flux\nWe use the following measured or typical values from a grassland site at midday:\n\n\\(T_{10m} = 19.0^\\circ C\\)\n\\(T_{2m} = 18.5^\\circ C\\) → \\(\\Delta T = 0.5\\,\\text{K}\\)\nWind speeds: \\(u_{2m} = 1.2\\,\\text{m/s}\\), \\(u_{10m} = 2.0\\,\\text{m/s}\\) → \\(\\bar{u} = 1.6\\,\\text{m/s}\\)\nConstants:\n\n\\(\\rho = 1.225\\,\\text{kg/m}^3\\) (air density)\n\\(c_p = 1005\\,\\text{J/kg/K}\\) (specific heat of air)\n\\(k = 0.41\\) (von Kármán constant)\n\\(z_1 = 2\\,\\text{m}\\), \\(z_2 = 10\\,\\text{m}\\)\n\n\nAerodynamic resistance is estimated using the logarithmic wind profile assumption:\n\\[\nr_a = \\frac{\\ln(z_2 / z_1)}{k \\cdot \\bar{u}} = \\frac{\\ln(10 / 2)}{0.41 \\cdot 1.6} \\approx 2.453\\,\\text{s/m}\n\\]\nNow we insert this into the bulk formula for \\(H\\):\n\\[\nH = \\rho \\cdot c_p \\cdot \\frac{\\Delta T}{r_a} = 1.225 \\cdot 1005 \\cdot \\frac{0.5}{2.453} \\approx 251\\,\\text{W/m}^2\n\\]\n\n\nStep 2: Residual Estimation of Latent Heat Flux\nWe now assume the following additional energy balance terms are available:\n\n\\(R_n = 620\\,\\text{W/m}^2\\): net radiation (measured via radiometer)\n\\(G = 80\\,\\text{W/m}^2\\): ground heat flux (from soil heat flux plates)\n\nWe calculate latent heat flux \\(LE\\) as the residual:\n\\[\nLE = R_n - G - H = 620 - 80 - 251 = 289\\,\\text{W/m}^2\n\\]\nThis gives the amount of energy used for evaporation and transpiration, inferred from energy conservation.\n\n\nInterpretation\nBy first estimating \\(H\\) via physical gradients (bulk approach), and then applying the residual method, we close the energy balance without needing direct \\(LE\\) measurements. This is a standard practice in micrometeorology when eddy covariance data are not available.\nUsing the residual method:\n\\(LE = R_n - G - H\\)\n\n\nCode\nenergy &lt;- energy %&gt;% mutate(LE = Rn - G - H)",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#plot-seasonal-comparison-of-energy-fluxes",
    "href": "doc/fluxes_corrected.html#plot-seasonal-comparison-of-energy-fluxes",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "Plot: Seasonal Comparison of Energy Fluxes",
    "text": "Plot: Seasonal Comparison of Energy Fluxes\n\n\nCode\nlibrary(ggplot2)\n\nenergy_long &lt;- energy %&gt;%\n  select(datetime, month_label, Rn, G, H, LE) %&gt;%\n  pivot_longer(cols = c(Rn, G, H, LE), names_to = \"flux\", values_to = \"value\")\n\nplot_fluxes &lt;- function(df, title) {\n  ggplot(df, aes(x = datetime, y = value, color = flux)) +\n    geom_line() +\n    labs(title = title, x = \"Time\", y = \"Flux (W/m²)\") +\n    theme_minimal()\n}\n\nplot_fluxes(filter(energy_long, month_label == \"June\"), \"Energy Fluxes in June\")\n\n\n\n\n\n\n\n\n\n\nEnergy Fluxes in June\n\nHigh latent heat flux (LE) dominates, indicating active evapotranspiration.\nSensible heat (H) remains moderate, as much energy is used to vaporize water.\nGround heat flux (G) is positive in early hours, storing heat in the soil.\n\nThis matches expectations for early summer (June): longer days, moist soils, and active vegetation cover.\n\n\nCode\nplot_fluxes(filter(energy_long, month_label == \"November\"), \"Energy Fluxes in November\")\n\n\n\n\n\n\n\n\n\n\n\nEnergy Fluxes in November\n\nLatent heat (LE) is minimal — vegetation is inactive and evapotranspiration drops.\nSensible heat (H) becomes dominant — energy heats the air due to lack of biological water flux.\nGround heat flux (G) may show inversion (soil losing heat), particularly in the evening.\n\nThis pattern reflects the dormant season, with minimal biological activity and colder atmospheric conditions.\nLegend for Energy Flux Components\n\n\n\nSymbol\nDescription\nSign Convention\n\n\n\n\n(R_n)\nNet radiation\nPositive downward\n\n\n(H)\nSensible heat flux\nPositive upward (to air)\n\n\n(LE)\nLatent heat flux\nPositive upward (evaporation)\n\n\n(G)\nGround heat flux\nPositive downward (into soil)",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#notes-on-interpretation",
    "href": "doc/fluxes_corrected.html#notes-on-interpretation",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "Notes on Interpretation",
    "text": "Notes on Interpretation\n\nFluxes are sensitive to soil moisture, vegetation, radiation, and wind.\nDeviations in (R_n - G - H) often indicate instrument imbalance, storage terms, or closure errors.\nThe residual approach to compute (LE) assumes high accuracy of the measured and modeled terms.\n\n\n\n\n\n\n\nConvert to Evapotranspiration (mm/day)\n\n\n\n\n\nWe calculate latent heat flux \\(LE\\) as the residual:\n\\[\nLE = R_n - G - H = 620 - 80 - 251 = 289\\,\\text{W/m}^2\n\\]\nUsing latent heat of vaporization \\(lambda = 2.45 \\cdot 10^6\\,\\text{J/kg}\\):\n\\[\nET = \\frac{LE}{\\lambda} \\cdot \\frac{86400}{1000}\n\\]\nInsert values:\n\\[\nET = \\frac{330}{2.45 \\cdot 10^6} \\cdot 86.4 \\approx 11.64\\,\\text{mm/day}\n\\]\nOn this day, the land surface lost ~11.6 mm of water via evapotranspiration — a very high rate, consistent with moist soil, strong radiation, and active vegetation.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Berechne ET\nlambda &lt;- 2.45e6  # J/kg\nenergy &lt;- energy %&gt;%\n  mutate(\n    ET_mm_day = (LE / lambda) * 86400,\n    month_label = month(datetime, label = TRUE, abbr = FALSE)\n  )\n\n# Aggregiere pro Monat\nmonthly_et &lt;- energy %&gt;%\n  filter(!is.na(ET_mm_day)) %&gt;%\n  group_by(month_label) %&gt;%\n  summarise(\n    mean_ET = mean(ET_mm_day, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Plot\nggplot(monthly_et, aes(x = month_label, y = mean_ET, fill = month_label)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    title = \"Mean Daily Evapotranspiration (ET) from Residual Method\",\n    x = \"Month\",\n    y = \"Evapotranspiration (mm/day)\"\n  ) +\n  scale_fill_manual(values = c(\"June\" = \"tomato\", \"November\" = \"turquoise3\")) +\n  theme_minimal()",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#export-optional",
    "href": "doc/fluxes_corrected.html#export-optional",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "Export (Optional)",
    "text": "Export (Optional)\n\n\nCode\n# write_csv(energy, here::here(\"data\", \"processed_energy_fluxes.csv\"))",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#complete-code",
    "href": "doc/fluxes_corrected.html#complete-code",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "Complete Code",
    "text": "Complete Code\n\n\nCode\n# Setup: source external script\nsource(\"../src/fluxes_corrected.R\")",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/fluxes_corrected.html#references",
    "href": "doc/fluxes_corrected.html#references",
    "title": "Basic Energy Flux Analysis from Energy Balance Station",
    "section": "References",
    "text": "References\n\nFoken, T. (2008). Micrometeorology. Springer.\n\nMonteith, J. L., & Unsworth, M. H. (2013). Principles of Environmental Physics. Academic Press.\n\nAllen, R. G., Pereira, L. S., Raes, D., & Smith, M. (1998). Crop evapotranspiration – FAO Irrigation and Drainage Paper 56.\n\nArya, S. P. (2001). Introduction to Micrometeorology. Academic Press.\n\nOke, T. R. (2002). Boundary Layer Climates. Routledge.",
    "crumbs": [
      "Basic Energy Flux Analysis from Energy Balance Station"
    ]
  },
  {
    "objectID": "doc/p3D.html",
    "href": "doc/p3D.html",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "",
    "text": "This tutorial demonstrates a full workflow to derive simplified ENVI-met-compatible 3DPLANT trees from airborne laser scanning (ALS) data. The process includes:\n\nVoxelization of ALS point cloud data\nLeaf Area Density (LAD) calculation per voxel column\nClustering of LAD profiles to define representative tree types\nExport of tree locations as GIS vector points\nExport of LAD profiles as ENVI-met .pld 3DPLANT definitions"
  },
  {
    "objectID": "doc/p3D.html#overview",
    "href": "doc/p3D.html#overview",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "",
    "text": "This tutorial demonstrates a full workflow to derive simplified ENVI-met-compatible 3DPLANT trees from airborne laser scanning (ALS) data. The process includes:\n\nVoxelization of ALS point cloud data\nLeaf Area Density (LAD) calculation per voxel column\nClustering of LAD profiles to define representative tree types\nExport of tree locations as GIS vector points\nExport of LAD profiles as ENVI-met .pld 3DPLANT definitions"
  },
  {
    "objectID": "doc/p3D.html#load-required-libraries",
    "href": "doc/p3D.html#load-required-libraries",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Load required libraries",
    "text": "Load required libraries\nlibrary(lidR)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(here)\nlibrary(XML)\nlibrary(stats)\nlibrary(tibble)\nlibrary(rprojroot)"
  },
  {
    "objectID": "doc/p3D.html#set-global-parameters",
    "href": "doc/p3D.html#set-global-parameters",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Set global parameters",
    "text": "Set global parameters\nlas_file &lt;- here(\"data/ALS/tiles/\")\nres_xy &lt;- 1\nres_z  &lt;- 1\nk      &lt;- 0.3\nscale_factor &lt;- 1.2\ncrs_code &lt;- 25832\noutput_gpkg &lt;- \"data/envimet/envimet_p3dtree_points.gpkg\"\nxml_output_file &lt;- \"data/envimet/als_envimet_trees.pld\"\nspecies_raster &lt;- rast(sprintf(\"data/aerial/%s_%sm.tif\", \"agg_cleand\", res_xy))\nn_clusters &lt;- 100"
  },
  {
    "objectID": "doc/p3D.html#step-1-read-and-preprocess-las-data",
    "href": "doc/p3D.html#step-1-read-and-preprocess-las-data",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 1: Read and preprocess LAS data",
    "text": "Step 1: Read and preprocess LAS data\nlas = merge_las_tiles(\n  tile_dir = las_file,\n  output_file = \"data/ALS/merged_output.laz\",\n  chunk_size = 10000,\n  workers = 6\n)\n\ncrs(las) &lt;- \"EPSG:25832\"\nlas &lt;- normalize_height(las, knnidw(k = 6, p = 2))\nlas &lt;- filter_poi(las, Z &gt; 0)"
  },
  {
    "objectID": "doc/p3D.html#step-2-voxelization",
    "href": "doc/p3D.html#step-2-voxelization",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 2: Voxelization",
    "text": "Step 2: Voxelization\nvoxels &lt;- voxel_metrics(las, ~length(Z), res = res_xy, dz = res_z)"
  },
  {
    "objectID": "doc/p3D.html#step-3-lad-profile-calculation",
    "href": "doc/p3D.html#step-3-lad-profile-calculation",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 3: LAD profile calculation",
    "text": "Step 3: LAD profile calculation\nlad_df &lt;- convert_voxel_lad_long(voxels, res_z = res_z, k = k, scale_factor = scale_factor)"
  },
  {
    "objectID": "doc/p3D.html#step-4-add-tree-species-class",
    "href": "doc/p3D.html#step-4-add-tree-species-class",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 4: Add tree species class",
    "text": "Step 4: Add tree species class\nspecies_at_xy &lt;- extract(species_raster, lad_df[, c(\"x\", \"y\")])\nlad_df$species_class &lt;- species_at_xy[, 2]"
  },
  {
    "objectID": "doc/p3D.html#step-5-clustering-lad-profiles",
    "href": "doc/p3D.html#step-5-clustering-lad-profiles",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 5: Clustering LAD profiles",
    "text": "Step 5: Clustering LAD profiles\nlad_df$xy_key &lt;- paste(lad_df$x, lad_df$y)\nlad_matrix &lt;- lad_df %&gt;% \n  tidyr::pivot_wider(names_from = z, values_from = lad, values_fill = 0) %&gt;%\n  column_to_rownames(\"xy_key\") %&gt;%\n  as.matrix()\n\nclustering &lt;- kmeans(lad_matrix, centers = n_clusters, nstart = 100)\nlad_df$cluster &lt;- clustering$cluster[match(lad_df$xy_key, rownames(lad_matrix))]"
  },
  {
    "objectID": "doc/p3D.html#step-6-assign-envi-met-compatible-ids",
    "href": "doc/p3D.html#step-6-assign-envi-met-compatible-ids",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 6: Assign ENVI-met compatible IDs",
    "text": "Step 6: Assign ENVI-met compatible IDs\ncluster_ids &lt;- unique(lad_df$cluster)\ncluster_mapping &lt;- data.frame(\n  cluster = cluster_ids,\n  ENVIMET_ID = sapply(cluster_ids, int_to_base36)\n)\nlad_df &lt;- left_join(lad_df, cluster_mapping, by = \"cluster\")"
  },
  {
    "objectID": "doc/p3D.html#step-7-export-spatial-locations",
    "href": "doc/p3D.html#step-7-export-spatial-locations",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 7: Export spatial locations",
    "text": "Step 7: Export spatial locations\npoint_df &lt;- lad_df[!duplicated(lad_df$xy_key), c(\"x\", \"y\", \"ENVIMET_ID\")]\nsf_points &lt;- st_as_sf(point_df, coords = c(\"x\", \"y\"), crs = crs_code)\nst_write(sf_points, output_gpkg, delete_layer = TRUE)"
  },
  {
    "objectID": "doc/p3D.html#step-8-export-xml-based-plant3d-definitions",
    "href": "doc/p3D.html#step-8-export-xml-based-plant3d-definitions",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Step 8: Export XML-based PLANT3D definitions",
    "text": "Step 8: Export XML-based PLANT3D definitions\nexport_lad_to_envimetp3d(lad_df, file_out = xml_output_file)"
  },
  {
    "objectID": "doc/p3D.html#result",
    "href": "doc/p3D.html#result",
    "title": "Generating ENVI-met 3DPLANT Trees from ALS Data",
    "section": "Result",
    "text": "Result\n\nA geopackage containing tree positions and ENVIMET_IDs used for ENVI-met domain placement.\nA .pld file with clustered LAD profiles, ready for use in ENVI-met 3DPLANT simulations.\n\nThis method allows a realistic but computationally manageable vegetation representation in ENVI-met using TLS/ALS-derived structure."
  },
  {
    "objectID": "doc/evaluation.html",
    "href": "doc/evaluation.html",
    "title": "Evaluation of Atmosphere–Vegetation Interaction Models",
    "section": "",
    "text": "Understanding microclimatic and ecological processes at high spatial resolution requires integrated modeling approaches that couple atmospheric dynamics with plant and vegetation processes. The models compared here span a wide range of capabilities, from fluid dynamics solvers to ecophysiological simulators. This document introduces the evaluation criteria, explains the scoring approach, and provides literature references for further exploration.\n\n\nThis model comparison aims to evaluate and categorize existing modeling platforms that simulate atmosphere-vegetation interactions, emphasizing realistic deployment, dimensional and structural detail, and biophysical relevance. While many models offer advanced features in isolation, such as turbulence modeling, vegetation physiology, or 3D structural input, only a few integrate these capabilities in a usable and accessible way. This evaluation considers not only modeling power but also technical complexity, scientific maturity, and practical deployability.\n\nFull 3D support with high spatial resolution\nIntegration of real-world vegetation structure from TLS, QSM, or ALS\nExplicit representation of plant–atmosphere feedback (e.g., transpiration effects on local climate)\nAvailability of user interfaces, documentation, licensing, and feasibility on common research workstations\n\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"default\", \"themeVariables\": { \"fontSize\": \"9px\", \"nodePadding\": \"20\", \"width\": \"300\" }}}%%\ngraph TD\n    A[Model Evaluation]\n\n    A --&gt; C[Flag Classification]\n    C --&gt; C1[🟩 High capability + usability&lt;br&gt;Score: 7.0–10.0&lt;br&gt;GUI, docs, user base]\n    C --&gt; C2[🟧 High capability, low usability&lt;br&gt;Score: 7.0–10.0&lt;br&gt;Expert-only, low maturity]\n    C --&gt; C3[🟦 Moderate & well-integrated&lt;br&gt;Score: 5.0–6.9&lt;br&gt;Lacks coupling, usable]\n    C --&gt; C4[🟨 Specialized + usable&lt;br&gt;Score: 4.0–6.9&lt;br&gt;Limited scope, accessible]\n    C --&gt; C5[🟥 Experimental / niche&lt;br&gt;Score: 4.0–6.9&lt;br&gt;Non-generalizable use]\n    C --&gt; C6[❌ Legacy / not usable&lt;br&gt;Any score&lt;br&gt;No dev, no workflow]\n    A --&gt; D[Microscale Applicability]\n    D --&gt; D1[✅ Fully applicable&lt;br&gt;≤10 m, 3D veg–air coupling]\n    D --&gt; D2[🟨 Conditionally applicable&lt;br&gt;Partial 3D or physics]\n    D --&gt; D3[❌ Not applicable&lt;br&gt;Too\n coarse or schematic]\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#ccf\n    style D fill:#ffc\n    style C1 fill:#cfc\n    style C2 fill:#ffebcc\n    style C3 fill:#e0f0ff\n    style C4 fill:#ffffcc\n    style C5 fill:#ffe0e0\n    style C6 fill:#ddd\n    style D1 fill:#cfc\n    style D2 fill:#ffffcc\n    style D3 fill:#eee\n\n\n\n\n\n\n\n\n\n\n\n\nWhen selecting a suitable model for simulating interactions between the atmosphere and vegetation, more aspects than just scientific completeness must be taken into account in the context of operationalization. Models differ considerably in their structure, focus, technical maturity, and accessibility. What does this mean? Some models achieve excellent physical accuracy (e.g., LES-based turbulence and complete feedback between plants and the atmosphere) but require in-depth technical knowledge or high-performance computers. Other models, on the other hand, sacrifice physical completeness in favor of practicality and easy integration into planning or monitoring processes.\nOur attempt is to enable this multi-criteria assessment and make it comprehensible by means of an evaluation framework in order to arrive at a pre-selection. This is done by combining quantitative assessments with a qualitative classification system, in particular:\n\nscientific completeness based on various categories\noperational usability\n\nThe evaluation is based on clearly defined criteria such as model physics, plant physiology, plant structure, model dimensionality and scales, and user-friendliness. However, the relative weights reflect value-based assessments – what is considered more important depends on the context (urban planning vs. ecohydrology vs. forest micrometeorology). In concrete terms, this means:\n\nIn urban planning, user-friendliness and realistic 3D air flows may be paramount.\nIn plant science, biophysics and physiology may be paramount.\nIn forestry, compatibility with TLS or QSM inputs may be required.\n\nThis is not a weakness, but a strength, as it allows adaptation to the user’s goals – as long as the weighting and criteria are transparent.\n\n\n\n\n\n\n\n\n\n\n\n\nCriterion\nWeight\nSub-Capabilities / Description\nScoring Guidelines\n\n\n\n\nAtmosphere\n3.0\nLES/RANS, radiative transfer, soil–vegetation coupling\n1.5–3.0: Full LES or RANS / radiation / soil 0.6–1.4: Partial (e.g., radiation only) &lt; 0.6: Basic or missing physics\n\n\nStructure\n1.0\nRealistic vegetation structure (e.g., QSM/LAD, voxel, TLS-derived)\n0.8–1.0: Full 3D voxel/QSM/tree geometry 0.4–0.7: Schematic trees &lt; 0.4: No or empirical structure\n\n\nFeedback\n1.0\nBiophysical feedback transpiration ↔︎ air\n0.8–1.0: Full coupling 0.4–0.7: Drag or partial interaction &lt; 0.4: One-way/static\n\n\nPhysiology\n1.0\nTranspiration, photosynthesis, stomata, water flow\n0.8–1.0: Full physiology 0.4–0.7: Simplified &lt; 0.4: None\n\n\nUsability\n2.0\nGUI/CLI, documentation, install, license, hardware\n1.5–2.0: Documented, user-friendly 0.6–1.4: CLI/complex &lt; 0.6: Legacy/unusable\n\n\n3D\n2.0\nFull 3D resolution and within-canopy gradients\n1.0–1.9: Full 3D 0.6–0.9: Pseudo-3D 0.3–0.5: Layered &lt; 0.3: None\n\n\n\n\n\n\n\nMicroscale applicable models can resolve processes on spatial scales of ≤ 10 m. At this scale, air flows, radiation, and microclimate dynamics become relevant at the tree level. These models are typically characterized by the following features:\n\nThey work with high-resolution 3D grids.\nVegetation is explicitly represented (e.g., voxel- or TLS-based).\nLocalized simulations of plots, tree groups, or urban areas are possible.\n\nMicroscale capability is essential for LiDAR-based modeling, urban forestry, and the investigation of microclimates in tree canopies.\n\n\n\n\n\n\n\n\nSymbol\nMicroscale Capability Description\n\n\n\n\n✅\nFully applicable: Designed for a resolution of ≤ 10 m with 3D vegetation–atmosphere coupling.\n\n\n🟨\nConditionally applicable: Partial support for high resolutions, but limited physics or geometry.\n\n\n❌\nNot applicable: Coarse resolution or missing spatial details.\n\n\n\n\nThe following characteristics are flagged with an ❌ for experimental or obsolete models:\n\nno longer maintained or widely used.\nthey were developed for niche applications or obsolete use cases.\nthere is a complete lack of user-friendly workflows or adequate documentation.\n\nThis leads to the following classification scheme:\n\n\n\n\n\n\n\n\n\n\nFlag\nLabel\nImplied Score\nExplanation of the Classification\n\n\n\n\n🟩\nHigh capability, high usability\n7.0–10.0\nFully featured and deployable with reasonable effort (GUI, docs, user base)\n\n\n🟧\nHigh capability, low usability\n7.0–10.0\nPowerful but difficult to use; expert-only setup, low maturity\n\n\n🟦\nModerate capability, well-integrated\n5.0–6.9\nSolid for many tasks; lacks advanced coupling but usable and balanced\n\n\n🟨\nSpecialized model, high usability\n4.0–6.9\nLimited scope (e.g. radiation only), but very accessible and documented\n\n\n🟥\nExperimental or niche\n4.0–6.9\nLimited audience or non-generalizable application\n\n\n❌\nLegacy/unmaintained/ not usable\nany\nNo active development or practical use case today\n\n\n\n\nNote: This classification does not always correspond to the ranking by score, which is intentional. For example, a model with a high score may still be marked with 🟧 or ❌ if it is technically difficult to implement, is not documented or maintained, or is simply no longer available. This helps to put the pure scoring performance into perspective, which would otherwise lead to the selection of models that prove to be unusable in practice.\n\n\n\n\nFifteen models were systematically evaluated based on the assessment framework presented above. The models were selected on an exploratory basis using Google searches, specialist literature (e.g., GMD Geoscientific Model Development, ScienceDirect), well-known model comparisons, and open-source repositories. The selection is therefore well-founded but not entirely systematic. The following tables shows the results of this evaluation. It not only shows the weighted overall score, but also differentiates between the underlying individual criteria. In addition, the applicability in the microscale range is indicated and each model is classified using a color-coded classification system. This makes the performance of a model in terms of the atmosphere-vegetation coupling transparent and shows the extent to which a model can be realistically integrated into scientific or operational processes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nScore\nAtmosphere Physics\nStructure\nFeedback\nPhysiology\nUsability\n3D\nMicroscale Capabilities\nAll\nMicroScale\n\n\n\n\n1\nPALM-4U\n7.6\n1.0Full LES and radiative transfer; no soil–water coupling\n0.8Static 3D vegetation model\n0.7Air–vegetation drag interaction\n0.3No explicit physiology\n0.4Complex install, Linux only; well documented\n1.0Fully resolved CFD\nLES solver at meter-scale; used in urban microclimate studies &lt;10 m; limited physiology, strong flow–structure resolution\n🟧\n🟩\n\n\n2\nMuSICA\n7.4\n0.8Soil–plant-atmosphere exchange; no LES or radiative transfer\n0.6Layered cohort model, not voxel-based\n1.0Detailed biophysical feedback\n1.0Includes transpiration, photosynthesis, stomata, water dynamics\n0.3Legacy Fortran, hard to install\n1.01D canopy model\netailed canopy and physiology; vertical 1D, not voxel; grid spacing cohort-based\n🟥 ❌\n🟨\n\n\n3\nOpenFOAM\n7.3\n2.5LES/RANS CFD; partial radiation support\n1.0Porosity/drag approach for tree structure\n0.5Customizable feedback via coding\n0.0No vegetation physiology\n0.2Expert use only, CLI\n1.9Full 3D CFD\nCFD model with &lt;1 m resolution; porosity drag; physiology via coding\n🟧\n🟩\n\n\n4\nENVI-met\n6.9\n0.8RANS turbulence and radiation; no soil coupling\n0.6Blocky trees, parametric structure\n0.4Basic one-way coupling\n0.3Simplified energy–water exchange\n0.6GUI, documented, PC-compatible\n1.0Pseudo-3D (layered)\nGrid 0.5–2 m; urban canopy focus; simplified structure\n🟩\n🟩\n\n\n5\nWRF-Urban\n6.5\n0.9Urban-scale RANS + radiation; coarse resolution\n0.7Urban and vegetation layering\n0.6Bulk vegetation–atmosphere interactions\n0.2No individual physiology model\n0.3HPC-heavy, complex setup\n1.0Grid-based, some canopy effects\nUrban RANS, VEGE3D coupling; no true &lt;10 m flow+veg\n🟧\n🟨\n\n\n6\nForestED\n6.1\n0.9Radiation balance only\n0.8TLS-derived trees, but limited structural detail\n0.5Weak air interaction\n0.3No detailed physiology\n0.4Prototype, no GUI\n1.0Real 3D from TLS\nFull 3D via TLS; radiation only, no flow coupling\n🟥 ❌\n🟩\n\n\n7\nDART\n5.7\n0.6Radiative transfer simulation, no air dynamics\n1.0Voxel-based structural import\n0.0No air–plant feedback\n0.2No physiology\n0.3Complex setup, technical barrier\n1.0Radiative 3D\nRadiative voxel model &lt;1 m; no airflow or feedback\n🟥 ❌\n🟩\n\n\n8\nED2\n5.6\n0.7Surface energy and hydrology; no LES\n0.5Functional cohort-based\n0.6Partial feedback, not dynamic\n1.0Rich physiology, plant hydraulics\n0.4Command-line only, heavy model\n0.5Vertical layers, no 3D\nCohort-based; no individual trees or microclimate\n🟦\n❌\n\n\n9\nMAESPA\n5.3\n0.4No LES or full RANS\n0.5Elliptic crown geometries\n0.5Simple coupling (transpiration ↔︎ air temp)\n1.0Includes water and stomatal response\n0.4CLI, old Fortran\n0.8Layered or semi-3D\nTree-level radiation, no fluid; stand-scale design\n🟦\n❌\n\n\n10\nPyDOM\n5.0\n0.5Solar irradiance modeling using discrete ordinates method; no fluid dynamics\n0.6Uses simplified canopy layers or volumes; structure inferred\n0.0No biophysical feedback\n0.3No physiology model\n0.5Script-based usage; moderately usable\n1.0Volumetric but low-resolution\nVoxelized solar DOM; no air coupling\n🟨 ❌\n🟨\n\n\n11\nCOMOKIT\n4.9\n0.4No atmosphere simulation; only agent-level heat/energy accounting\n0.3Simplified static vegetation\n0.5Agent-based feedback loops via scenario definition\n0.5No continuous transpiration or photosynthesis, only thermal behavior\n0.7Accessible GUI, easy scenario logic, extensive documentation\n0.5Visual pseudo-3D, no physical gradients\nAgent-based; no physical coupling; coarse graphics\n🟨\n❌\n\n\n12\nLPJ-GUESS\n4.4\n0.7Energy and gas exchange with climate integration; no internal 3D resolution or LES\n0.4Functional PFTs with vertical profiles, no explicit geometry\n0.4Climate–vegetation coupling, but coarse\n0.9Complex physiology model with stomatal control, photosynthesis\n0.3CLI-based ecosystem model, config-heavy\n0.0No 3D, grid-cell aggregated PFT composition\nDGVM, no 3D, coarse PFT representation\n🟦 ❌\n❌\n\n\n13\nMicroclimc\n4.2\n0.6Radiative energy balance and temperature with simple terrain effects; no CFD or LES\n0.3Schematic trees only, no voxel or lidar structure\n0.4One-way: vegetation affects energy balance, no feedback from air\n0.4Simple transpiration and energy fluxes only\n0.8R-based, documented, GUI-like interface\n0.5Layered canopy modules, semi-3D\n1–5 m radiation/temp; schematic trees; no LES\n🟨\n🟨\n\n\n14\niTREETools\n3.1\n0.3No atmosphere model; static inventory system\n0.2Tree inventory tables only; no geometry\n0.2None; no feedback, no fluxes\n0.0No physiology modeling\n0.9Very accessible GUI, well supported\n0.0No 3D; inventory only\nEmpirical inventory; no spatial simulation\n🟨 ❌\n❌\n\n\n15\nFluspect\n1.9\n0.1Leaf-scale radiative simulation; no atmosphere\n0.0No spatial structure\n0.0None\n1.0Detailed leaf spectral and biochemical simulation\n0.3Niche tool, standalone use; requires integration\n0.0No 3D; single-leaf model\nLeaf-level only; no domain or airflow\n🟥 ❌\n❌\n\n\n\n\nBased on the upper table, the below graph helps visualize the trade-off between model capability and usability, making the multi-dimensional classification system intuitively accessible at a glance.\n\n\n\nModel usability vs. total score across classified microclimate models. The inverse relationship reflects a common trade-off: high-performing models (right) often require expert-level setup (low usability, bottom), while user-friendly models (top) tend to be simplified. Colored points reflect model classification: 🟩 usable and capable, 🟧 powerful but complex, 🟦 balanced, 🟨 specialized, and 🟥 experimental. Models above the trend line (e.g., ENVI-met) offer better usability than expected for their score; those below (e.g., MuSICA) require disproportionately high effort.\n\n\n\n\n\nIf we narrow the selection by removing models that are either classified as legacy systems (❌ ) or that lack both microscale applicability (❌) and fully resolved 3D spatial representation (&lt;1.0), the set of viable modeling platforms is drastically reduced. This filtering excludes tools that are either outdated, lack dimensional realism, or are designed for coarse-scale applications incompatible with high-resolution vegetation–atmosphere modeling. What remains is a focused set of platforms that combine scientific robustness with operational feasibility for modeling processes at the tree or plot level. These remaining models offer realistic support for implementation in urban microclimate design, LiDAR-informed ecological studies, and vegetation-based climate adaptation strategies where 3D feedbacks and spatial structure matter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nModel\nScore\nAtmosphere\nStructure\nFeedback\nPhysiology\nUsability\n3D\nAll\nMicroScale\n\n\n\n\n1\nPALM-4U\n7.6\n1.0 – Full LES, radiation, no soil\n0.8 – Static vegetation models\n0.7 – Basic interaction\n0.3 – No physiology\n0.4 – Complex setup, Linux only, docs\n1.0 – True 3D\n🟧\n🟩\n\n\n2\nOpenFOAM\n7.3\n2.5 – Full LES/RANS, partial radiation\n1.0 – Porosity/drag approach\n0.5 – Feedback via coding\n0.0 – None\n0.2 – CLI, expert only\n1.9 – Full CFD\n🟧\n🟩\n\n\n3\nENVI-met\n6.9\n0.8 – RANS, radiation\n0.6 – Block trees only\n0.4 – Weak one-way coupling\n0.3 – Simplified transpiration\n0.6 – GUI, docs, runs on PC\n1.0 – Pseudo-3D layering\n🟩\n🟩\n\n\n4\nWRF-Urban\n6.5\n0.9 – RANS, radiation\n0.7 – Layered urban/vegetation\n0.6 – Bulk interaction\n0.2 – None\n0.3 – HPC, not easily usable\n1.0 – Grid-based\n🟧\n🟨\n\n\n5\nMicroclimc\n4.2\n0.6 – Radiation and temp model, no LES\n0.3 – Schematic trees\n0.4 – Indirect surface coupling\n0.4 – Empirical transpiration\n0.8 – GUI, documentation, R integration\n0.5 – Semi-3D canopy layer\n🟨\n🟨\n\n\n\nSorting applied:\nPrimary: Microscale applicability (✅ &gt; 🟨 &gt; ❌)\nSecondary: Total Score (descending)\n\n\n\n\n\nto be completed\nENVI-met\n\nBruse, M., & Fleer, H. (1998). Simulating surface–plant–air interactions inside urban environments with a three-dimensional numerical model. Environmental Modelling & Software, 13(3–4), 373–384.\nENVI-met Documentation: https://envi-met.info/\n\nPALM-4U\n\nMaronga, B., et al. (2020). Overview of the PALM model system 6.0. Geoscientific Model Development, 13, 1335–1372. https://doi.org/10.5194/gmd-13-1335-2020\nhttps://palm-model.org/\n\nMuSICA\n\nOgée, J., et al. (2003). MuSICA, a CO2, water and energy multilayer, multileaf model for the analysis of function of vegetation at the canopy scale. Ecological Modelling, 156(2–3), 181–204.\n\nOpenFOAM (custom vegetation)\n\nGromke, C., & Blocken, B. (2015). CFD simulation of near-field pollutant dispersion including vegetation effects. Atmospheric Environment, 100, 238–249.\nThe OpenFOAM Foundation Hom\n\nWRF-Urban\n\nChen, F., Yu, B., Wu, M., Yang, X., et al. (2021). Improved urban finescale forecasting during a heat wave by using high-resolution urban canopy parameters. Frontiers in Climate, 3, 771441. https://doi.org/10.3389/fclim.2021.771441\nMartilli, A., Nazarian, N., Krayenhoff, E. S., Lachapelle, J., Lu, J., Rivas, E., Rodriguez‑Sanchez, A., Sanchez, B., & Santiago, J. L. (2024). WRF‑Comfort: Simulating microscale variability in outdoor heat stress at the city scale with a mesoscale model. Geoscientific Model Development, 17, 5023–5039. https://doi.org/10.5194/gmd-17-5023-2024",
    "crumbs": [
      "Evaluation of Atmosphere–Vegetation Interaction Models"
    ]
  },
  {
    "objectID": "doc/evaluation.html#objective-of-the-evaluation",
    "href": "doc/evaluation.html#objective-of-the-evaluation",
    "title": "Evaluation of Atmosphere–Vegetation Interaction Models",
    "section": "",
    "text": "This model comparison aims to evaluate and categorize existing modeling platforms that simulate atmosphere-vegetation interactions, emphasizing realistic deployment, dimensional and structural detail, and biophysical relevance. While many models offer advanced features in isolation, such as turbulence modeling, vegetation physiology, or 3D structural input, only a few integrate these capabilities in a usable and accessible way. This evaluation considers not only modeling power but also technical complexity, scientific maturity, and practical deployability.\n\nFull 3D support with high spatial resolution\nIntegration of real-world vegetation structure from TLS, QSM, or ALS\nExplicit representation of plant–atmosphere feedback (e.g., transpiration effects on local climate)\nAvailability of user interfaces, documentation, licensing, and feasibility on common research workstations",
    "crumbs": [
      "Evaluation of Atmosphere–Vegetation Interaction Models"
    ]
  },
  {
    "objectID": "doc/evaluation.html#model-evaluation-structure",
    "href": "doc/evaluation.html#model-evaluation-structure",
    "title": "Evaluation of Atmosphere–Vegetation Interaction Models",
    "section": "",
    "text": "%%{init: {\"theme\": \"default\", \"themeVariables\": { \"fontSize\": \"9px\", \"nodePadding\": \"20\", \"width\": \"300\" }}}%%\ngraph TD\n    A[Model Evaluation]\n\n    A --&gt; C[Flag Classification]\n    C --&gt; C1[🟩 High capability + usability&lt;br&gt;Score: 7.0–10.0&lt;br&gt;GUI, docs, user base]\n    C --&gt; C2[🟧 High capability, low usability&lt;br&gt;Score: 7.0–10.0&lt;br&gt;Expert-only, low maturity]\n    C --&gt; C3[🟦 Moderate & well-integrated&lt;br&gt;Score: 5.0–6.9&lt;br&gt;Lacks coupling, usable]\n    C --&gt; C4[🟨 Specialized + usable&lt;br&gt;Score: 4.0–6.9&lt;br&gt;Limited scope, accessible]\n    C --&gt; C5[🟥 Experimental / niche&lt;br&gt;Score: 4.0–6.9&lt;br&gt;Non-generalizable use]\n    C --&gt; C6[❌ Legacy / not usable&lt;br&gt;Any score&lt;br&gt;No dev, no workflow]\n    A --&gt; D[Microscale Applicability]\n    D --&gt; D1[✅ Fully applicable&lt;br&gt;≤10 m, 3D veg–air coupling]\n    D --&gt; D2[🟨 Conditionally applicable&lt;br&gt;Partial 3D or physics]\n    D --&gt; D3[❌ Not applicable&lt;br&gt;Too\n coarse or schematic]\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#ccf\n    style D fill:#ffc\n    style C1 fill:#cfc\n    style C2 fill:#ffebcc\n    style C3 fill:#e0f0ff\n    style C4 fill:#ffffcc\n    style C5 fill:#ffe0e0\n    style C6 fill:#ddd\n    style D1 fill:#cfc\n    style D2 fill:#ffffcc\n    style D3 fill:#eee",
    "crumbs": [
      "Evaluation of Atmosphere–Vegetation Interaction Models"
    ]
  },
  {
    "objectID": "doc/evaluation.html#the-evaluation-framework",
    "href": "doc/evaluation.html#the-evaluation-framework",
    "title": "Evaluation of Atmosphere–Vegetation Interaction Models",
    "section": "",
    "text": "When selecting a suitable model for simulating interactions between the atmosphere and vegetation, more aspects than just scientific completeness must be taken into account in the context of operationalization. Models differ considerably in their structure, focus, technical maturity, and accessibility. What does this mean? Some models achieve excellent physical accuracy (e.g., LES-based turbulence and complete feedback between plants and the atmosphere) but require in-depth technical knowledge or high-performance computers. Other models, on the other hand, sacrifice physical completeness in favor of practicality and easy integration into planning or monitoring processes.\nOur attempt is to enable this multi-criteria assessment and make it comprehensible by means of an evaluation framework in order to arrive at a pre-selection. This is done by combining quantitative assessments with a qualitative classification system, in particular:\n\nscientific completeness based on various categories\noperational usability\n\nThe evaluation is based on clearly defined criteria such as model physics, plant physiology, plant structure, model dimensionality and scales, and user-friendliness. However, the relative weights reflect value-based assessments – what is considered more important depends on the context (urban planning vs. ecohydrology vs. forest micrometeorology). In concrete terms, this means:\n\nIn urban planning, user-friendliness and realistic 3D air flows may be paramount.\nIn plant science, biophysics and physiology may be paramount.\nIn forestry, compatibility with TLS or QSM inputs may be required.\n\nThis is not a weakness, but a strength, as it allows adaptation to the user’s goals – as long as the weighting and criteria are transparent.\n\n\n\n\n\n\n\n\n\n\n\n\nCriterion\nWeight\nSub-Capabilities / Description\nScoring Guidelines\n\n\n\n\nAtmosphere\n3.0\nLES/RANS, radiative transfer, soil–vegetation coupling\n1.5–3.0: Full LES or RANS / radiation / soil 0.6–1.4: Partial (e.g., radiation only) &lt; 0.6: Basic or missing physics\n\n\nStructure\n1.0\nRealistic vegetation structure (e.g., QSM/LAD, voxel, TLS-derived)\n0.8–1.0: Full 3D voxel/QSM/tree geometry 0.4–0.7: Schematic trees &lt; 0.4: No or empirical structure\n\n\nFeedback\n1.0\nBiophysical feedback transpiration ↔︎ air\n0.8–1.0: Full coupling 0.4–0.7: Drag or partial interaction &lt; 0.4: One-way/static\n\n\nPhysiology\n1.0\nTranspiration, photosynthesis, stomata, water flow\n0.8–1.0: Full physiology 0.4–0.7: Simplified &lt; 0.4: None\n\n\nUsability\n2.0\nGUI/CLI, documentation, install, license, hardware\n1.5–2.0: Documented, user-friendly 0.6–1.4: CLI/complex &lt; 0.6: Legacy/unusable\n\n\n3D\n2.0\nFull 3D resolution and within-canopy gradients\n1.0–1.9: Full 3D 0.6–0.9: Pseudo-3D 0.3–0.5: Layered &lt; 0.3: None\n\n\n\n\n\n\n\nMicroscale applicable models can resolve processes on spatial scales of ≤ 10 m. At this scale, air flows, radiation, and microclimate dynamics become relevant at the tree level. These models are typically characterized by the following features:\n\nThey work with high-resolution 3D grids.\nVegetation is explicitly represented (e.g., voxel- or TLS-based).\nLocalized simulations of plots, tree groups, or urban areas are possible.\n\nMicroscale capability is essential for LiDAR-based modeling, urban forestry, and the investigation of microclimates in tree canopies.\n\n\n\n\n\n\n\n\nSymbol\nMicroscale Capability Description\n\n\n\n\n✅\nFully applicable: Designed for a resolution of ≤ 10 m with 3D vegetation–atmosphere coupling.\n\n\n🟨\nConditionally applicable: Partial support for high resolutions, but limited physics or geometry.\n\n\n❌\nNot applicable: Coarse resolution or missing spatial details.\n\n\n\n\nThe following characteristics are flagged with an ❌ for experimental or obsolete models:\n\nno longer maintained or widely used.\nthey were developed for niche applications or obsolete use cases.\nthere is a complete lack of user-friendly workflows or adequate documentation.\n\nThis leads to the following classification scheme:\n\n\n\n\n\n\n\n\n\n\nFlag\nLabel\nImplied Score\nExplanation of the Classification\n\n\n\n\n🟩\nHigh capability, high usability\n7.0–10.0\nFully featured and deployable with reasonable effort (GUI, docs, user base)\n\n\n🟧\nHigh capability, low usability\n7.0–10.0\nPowerful but difficult to use; expert-only setup, low maturity\n\n\n🟦\nModerate capability, well-integrated\n5.0–6.9\nSolid for many tasks; lacks advanced coupling but usable and balanced\n\n\n🟨\nSpecialized model, high usability\n4.0–6.9\nLimited scope (e.g. radiation only), but very accessible and documented\n\n\n🟥\nExperimental or niche\n4.0–6.9\nLimited audience or non-generalizable application\n\n\n❌\nLegacy/unmaintained/ not usable\nany\nNo active development or practical use case today\n\n\n\n\nNote: This classification does not always correspond to the ranking by score, which is intentional. For example, a model with a high score may still be marked with 🟧 or ❌ if it is technically difficult to implement, is not documented or maintained, or is simply no longer available. This helps to put the pure scoring performance into perspective, which would otherwise lead to the selection of models that prove to be unusable in practice.",
    "crumbs": [
      "Evaluation of Atmosphere–Vegetation Interaction Models"
    ]
  },
  {
    "objectID": "doc/evaluation.html#atmospherevegetation-model-comparison-of-models",
    "href": "doc/evaluation.html#atmospherevegetation-model-comparison-of-models",
    "title": "Evaluation of Atmosphere–Vegetation Interaction Models",
    "section": "",
    "text": "Fifteen models were systematically evaluated based on the assessment framework presented above. The models were selected on an exploratory basis using Google searches, specialist literature (e.g., GMD Geoscientific Model Development, ScienceDirect), well-known model comparisons, and open-source repositories. The selection is therefore well-founded but not entirely systematic. The following tables shows the results of this evaluation. It not only shows the weighted overall score, but also differentiates between the underlying individual criteria. In addition, the applicability in the microscale range is indicated and each model is classified using a color-coded classification system. This makes the performance of a model in terms of the atmosphere-vegetation coupling transparent and shows the extent to which a model can be realistically integrated into scientific or operational processes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nScore\nAtmosphere Physics\nStructure\nFeedback\nPhysiology\nUsability\n3D\nMicroscale Capabilities\nAll\nMicroScale\n\n\n\n\n1\nPALM-4U\n7.6\n1.0Full LES and radiative transfer; no soil–water coupling\n0.8Static 3D vegetation model\n0.7Air–vegetation drag interaction\n0.3No explicit physiology\n0.4Complex install, Linux only; well documented\n1.0Fully resolved CFD\nLES solver at meter-scale; used in urban microclimate studies &lt;10 m; limited physiology, strong flow–structure resolution\n🟧\n🟩\n\n\n2\nMuSICA\n7.4\n0.8Soil–plant-atmosphere exchange; no LES or radiative transfer\n0.6Layered cohort model, not voxel-based\n1.0Detailed biophysical feedback\n1.0Includes transpiration, photosynthesis, stomata, water dynamics\n0.3Legacy Fortran, hard to install\n1.01D canopy model\netailed canopy and physiology; vertical 1D, not voxel; grid spacing cohort-based\n🟥 ❌\n🟨\n\n\n3\nOpenFOAM\n7.3\n2.5LES/RANS CFD; partial radiation support\n1.0Porosity/drag approach for tree structure\n0.5Customizable feedback via coding\n0.0No vegetation physiology\n0.2Expert use only, CLI\n1.9Full 3D CFD\nCFD model with &lt;1 m resolution; porosity drag; physiology via coding\n🟧\n🟩\n\n\n4\nENVI-met\n6.9\n0.8RANS turbulence and radiation; no soil coupling\n0.6Blocky trees, parametric structure\n0.4Basic one-way coupling\n0.3Simplified energy–water exchange\n0.6GUI, documented, PC-compatible\n1.0Pseudo-3D (layered)\nGrid 0.5–2 m; urban canopy focus; simplified structure\n🟩\n🟩\n\n\n5\nWRF-Urban\n6.5\n0.9Urban-scale RANS + radiation; coarse resolution\n0.7Urban and vegetation layering\n0.6Bulk vegetation–atmosphere interactions\n0.2No individual physiology model\n0.3HPC-heavy, complex setup\n1.0Grid-based, some canopy effects\nUrban RANS, VEGE3D coupling; no true &lt;10 m flow+veg\n🟧\n🟨\n\n\n6\nForestED\n6.1\n0.9Radiation balance only\n0.8TLS-derived trees, but limited structural detail\n0.5Weak air interaction\n0.3No detailed physiology\n0.4Prototype, no GUI\n1.0Real 3D from TLS\nFull 3D via TLS; radiation only, no flow coupling\n🟥 ❌\n🟩\n\n\n7\nDART\n5.7\n0.6Radiative transfer simulation, no air dynamics\n1.0Voxel-based structural import\n0.0No air–plant feedback\n0.2No physiology\n0.3Complex setup, technical barrier\n1.0Radiative 3D\nRadiative voxel model &lt;1 m; no airflow or feedback\n🟥 ❌\n🟩\n\n\n8\nED2\n5.6\n0.7Surface energy and hydrology; no LES\n0.5Functional cohort-based\n0.6Partial feedback, not dynamic\n1.0Rich physiology, plant hydraulics\n0.4Command-line only, heavy model\n0.5Vertical layers, no 3D\nCohort-based; no individual trees or microclimate\n🟦\n❌\n\n\n9\nMAESPA\n5.3\n0.4No LES or full RANS\n0.5Elliptic crown geometries\n0.5Simple coupling (transpiration ↔︎ air temp)\n1.0Includes water and stomatal response\n0.4CLI, old Fortran\n0.8Layered or semi-3D\nTree-level radiation, no fluid; stand-scale design\n🟦\n❌\n\n\n10\nPyDOM\n5.0\n0.5Solar irradiance modeling using discrete ordinates method; no fluid dynamics\n0.6Uses simplified canopy layers or volumes; structure inferred\n0.0No biophysical feedback\n0.3No physiology model\n0.5Script-based usage; moderately usable\n1.0Volumetric but low-resolution\nVoxelized solar DOM; no air coupling\n🟨 ❌\n🟨\n\n\n11\nCOMOKIT\n4.9\n0.4No atmosphere simulation; only agent-level heat/energy accounting\n0.3Simplified static vegetation\n0.5Agent-based feedback loops via scenario definition\n0.5No continuous transpiration or photosynthesis, only thermal behavior\n0.7Accessible GUI, easy scenario logic, extensive documentation\n0.5Visual pseudo-3D, no physical gradients\nAgent-based; no physical coupling; coarse graphics\n🟨\n❌\n\n\n12\nLPJ-GUESS\n4.4\n0.7Energy and gas exchange with climate integration; no internal 3D resolution or LES\n0.4Functional PFTs with vertical profiles, no explicit geometry\n0.4Climate–vegetation coupling, but coarse\n0.9Complex physiology model with stomatal control, photosynthesis\n0.3CLI-based ecosystem model, config-heavy\n0.0No 3D, grid-cell aggregated PFT composition\nDGVM, no 3D, coarse PFT representation\n🟦 ❌\n❌\n\n\n13\nMicroclimc\n4.2\n0.6Radiative energy balance and temperature with simple terrain effects; no CFD or LES\n0.3Schematic trees only, no voxel or lidar structure\n0.4One-way: vegetation affects energy balance, no feedback from air\n0.4Simple transpiration and energy fluxes only\n0.8R-based, documented, GUI-like interface\n0.5Layered canopy modules, semi-3D\n1–5 m radiation/temp; schematic trees; no LES\n🟨\n🟨\n\n\n14\niTREETools\n3.1\n0.3No atmosphere model; static inventory system\n0.2Tree inventory tables only; no geometry\n0.2None; no feedback, no fluxes\n0.0No physiology modeling\n0.9Very accessible GUI, well supported\n0.0No 3D; inventory only\nEmpirical inventory; no spatial simulation\n🟨 ❌\n❌\n\n\n15\nFluspect\n1.9\n0.1Leaf-scale radiative simulation; no atmosphere\n0.0No spatial structure\n0.0None\n1.0Detailed leaf spectral and biochemical simulation\n0.3Niche tool, standalone use; requires integration\n0.0No 3D; single-leaf model\nLeaf-level only; no domain or airflow\n🟥 ❌\n❌\n\n\n\n\nBased on the upper table, the below graph helps visualize the trade-off between model capability and usability, making the multi-dimensional classification system intuitively accessible at a glance.\n\n\n\nModel usability vs. total score across classified microclimate models. The inverse relationship reflects a common trade-off: high-performing models (right) often require expert-level setup (low usability, bottom), while user-friendly models (top) tend to be simplified. Colored points reflect model classification: 🟩 usable and capable, 🟧 powerful but complex, 🟦 balanced, 🟨 specialized, and 🟥 experimental. Models above the trend line (e.g., ENVI-met) offer better usability than expected for their score; those below (e.g., MuSICA) require disproportionately high effort.\n\n\n\n\n\nIf we narrow the selection by removing models that are either classified as legacy systems (❌ ) or that lack both microscale applicability (❌) and fully resolved 3D spatial representation (&lt;1.0), the set of viable modeling platforms is drastically reduced. This filtering excludes tools that are either outdated, lack dimensional realism, or are designed for coarse-scale applications incompatible with high-resolution vegetation–atmosphere modeling. What remains is a focused set of platforms that combine scientific robustness with operational feasibility for modeling processes at the tree or plot level. These remaining models offer realistic support for implementation in urban microclimate design, LiDAR-informed ecological studies, and vegetation-based climate adaptation strategies where 3D feedbacks and spatial structure matter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nModel\nScore\nAtmosphere\nStructure\nFeedback\nPhysiology\nUsability\n3D\nAll\nMicroScale\n\n\n\n\n1\nPALM-4U\n7.6\n1.0 – Full LES, radiation, no soil\n0.8 – Static vegetation models\n0.7 – Basic interaction\n0.3 – No physiology\n0.4 – Complex setup, Linux only, docs\n1.0 – True 3D\n🟧\n🟩\n\n\n2\nOpenFOAM\n7.3\n2.5 – Full LES/RANS, partial radiation\n1.0 – Porosity/drag approach\n0.5 – Feedback via coding\n0.0 – None\n0.2 – CLI, expert only\n1.9 – Full CFD\n🟧\n🟩\n\n\n3\nENVI-met\n6.9\n0.8 – RANS, radiation\n0.6 – Block trees only\n0.4 – Weak one-way coupling\n0.3 – Simplified transpiration\n0.6 – GUI, docs, runs on PC\n1.0 – Pseudo-3D layering\n🟩\n🟩\n\n\n4\nWRF-Urban\n6.5\n0.9 – RANS, radiation\n0.7 – Layered urban/vegetation\n0.6 – Bulk interaction\n0.2 – None\n0.3 – HPC, not easily usable\n1.0 – Grid-based\n🟧\n🟨\n\n\n5\nMicroclimc\n4.2\n0.6 – Radiation and temp model, no LES\n0.3 – Schematic trees\n0.4 – Indirect surface coupling\n0.4 – Empirical transpiration\n0.8 – GUI, documentation, R integration\n0.5 – Semi-3D canopy layer\n🟨\n🟨\n\n\n\nSorting applied:\nPrimary: Microscale applicability (✅ &gt; 🟨 &gt; ❌)\nSecondary: Total Score (descending)",
    "crumbs": [
      "Evaluation of Atmosphere–Vegetation Interaction Models"
    ]
  },
  {
    "objectID": "doc/evaluation.html#literature-and-sources-by-model",
    "href": "doc/evaluation.html#literature-and-sources-by-model",
    "title": "Evaluation of Atmosphere–Vegetation Interaction Models",
    "section": "",
    "text": "to be completed\nENVI-met\n\nBruse, M., & Fleer, H. (1998). Simulating surface–plant–air interactions inside urban environments with a three-dimensional numerical model. Environmental Modelling & Software, 13(3–4), 373–384.\nENVI-met Documentation: https://envi-met.info/\n\nPALM-4U\n\nMaronga, B., et al. (2020). Overview of the PALM model system 6.0. Geoscientific Model Development, 13, 1335–1372. https://doi.org/10.5194/gmd-13-1335-2020\nhttps://palm-model.org/\n\nMuSICA\n\nOgée, J., et al. (2003). MuSICA, a CO2, water and energy multilayer, multileaf model for the analysis of function of vegetation at the canopy scale. Ecological Modelling, 156(2–3), 181–204.\n\nOpenFOAM (custom vegetation)\n\nGromke, C., & Blocken, B. (2015). CFD simulation of near-field pollutant dispersion including vegetation effects. Atmospheric Environment, 100, 238–249.\nThe OpenFOAM Foundation Hom\n\nWRF-Urban\n\nChen, F., Yu, B., Wu, M., Yang, X., et al. (2021). Improved urban finescale forecasting during a heat wave by using high-resolution urban canopy parameters. Frontiers in Climate, 3, 771441. https://doi.org/10.3389/fclim.2021.771441\nMartilli, A., Nazarian, N., Krayenhoff, E. S., Lachapelle, J., Lu, J., Rivas, E., Rodriguez‑Sanchez, A., Sanchez, B., & Santiago, J. L. (2024). WRF‑Comfort: Simulating microscale variability in outdoor heat stress at the city scale with a mesoscale model. Geoscientific Model Development, 17, 5023–5039. https://doi.org/10.5194/gmd-17-5023-2024",
    "crumbs": [
      "Evaluation of Atmosphere–Vegetation Interaction Models"
    ]
  },
  {
    "objectID": "doc/treespecies.html",
    "href": "doc/treespecies.html",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "",
    "text": "Purpose of Tree Data for ENVI-met Modeling This workflow prepares classified tree species data as a basis for generating individual tree objects for use in ENVI-met’s 3DPLANT module. Each tree location is linked to a simplified vertical LAD profile and assigned to a species class (e.g. Fagus sylvatica, Quercus robur, Pseudotsuga menziesii), which defines its interaction with ENVI-met’s radiation and vegetation modules.\nThe underlying classification raster originates from official, state-level aerial RGB orthophotos with a spatial resolution of 0.3 m. These orthophotos provide sufficient detail to allow object-based species classification at the level of individual tree crowns.\nSpecies prediction was performed using a leave-location-out forward feature selection approach implemented via the CAST package in R. This ensures that classification results generalize across spatially distinct regions by avoiding overfitting to local spectral conditions.\nBefore assigning vegetation objects to the ENVI-met model domain, species maps are despeckled, aggregated, and contextually corrected to remove isolated or misclassified tree crowns (e.g. Douglas-fir pixels in beech-dominated stands). This ensures that each synthetic ENVI-met tree is placed in a semantically and structurally consistent vegetation context.",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#setup-and-environment",
    "href": "doc/treespecies.html#setup-and-environment",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "1. Setup and Environment",
    "text": "1. Setup and Environment\nManual Setup of OTB Environment for use with link2GI\nThe R package link2GI provides wrapper functions to connect R with external geospatial software like Orfeo Toolbox (OTB), GRASS GIS, and QGIS. The function linkOTB() is used to locate OTB binaries and configure the R session to allow calling OTB applications via command-line interface (CLI) from R.\nHowever, in many modern setups—especially on Linux or in manually installed environments (e.g., extracted zip files)—the required environment variables are not set globally, and linkOTB() alone is not sufficient. This typically leads to errors like:\n\n“Application not found”\n“No XML application descriptors”\n“Could not find CLI tools”\n\nTo fix this, two critical environment variables need to be explicitly set after calling linkOTB():\n\nOTB_APPLICATION_PATH: This must point to the directory lib/otb/applications, where all XML definitions of the OTB applications are stored. These XML files describe how to call each OTB tool from the command line.\nPATH: This must include the directory where OTB binaries like otbcli_BandMath are stored (typically bin/). Without this, system calls from R to OTB will fail.\n\nExample for Linux:\n\notb &lt;- link2GI::linkOTB(searchLocation = \"~/apps/OTB-9.1.0-Linux/\")\nSys.setenv(OTB_APPLICATION_PATH = file.path(dirname(as.character(otb$pathOTB)), \"lib/otb/applications\"))\nSys.setenv(PATH = paste(otb$pathOTB, Sys.getenv(\"PATH\"), sep = \":\"))\n\nExample for Windows:\n\notb &lt;- link2GI::linkOTB(searchLocation = \"C:/OTB-9.1.0-Win64/\")\nSys.setenv(OTB_APPLICATION_PATH = \"C:/OTB-9.1.0-Win64/lib/otb/applications\")\nSys.setenv(PATH = paste(\"C:/OTB-9.1.0-Win64/bin\", Sys.getenv(\"PATH\"), sep = \";\"))\n\nNote:\n\nOn Windows, use forward slashes / in the path.\nThe PATH separator is ; on Windows and : on Unix-based systems.\n\nThis workaround is often necessary in portable, containerized, or research setups where full system integration (e.g., PATH exports, registry entries) is not available or not desired. It ensures that link2GI can still function as intended by emulating the expected environment internally within R.\n\n# Load libraries\nlibrary(terra)\nlibrary(RColorBrewer)\nlibrary(link2GI)\nlibrary(tools)\nlibrary(mapview)\nlibrary(dplyr)\n\n# Project root and OTB environment\nroot_folder &lt;- rprojroot::find_rstudio_root_file()\notb &lt;- link2GI::linkOTB(searchLocation = \"~/apps/OTB-9.1.0-Linux/\")\nSys.setenv(OTB_APPLICATION_PATH = file.path(dirname(as.character(otb$pathOTB)), \"lib/otb/applications\"))\nSys.setenv(PATH = paste(otb$pathOTB, Sys.getenv(\"PATH\"), sep = \":\"))",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#parameters-and-class-legend",
    "href": "doc/treespecies.html#parameters-and-class-legend",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "2. Parameters and Class Legend",
    "text": "2. Parameters and Class Legend\n\ntarget_res &lt;- 1\nfn &lt;- \"5-25_MOF_rgb\"\nepsg &lt;- 25832\nsapflow_ext &lt;- raster::extent(477500, 478218, 5631730, 5632500)\n\nts &lt;- data.frame(\n  ID = 1:12,\n  value = c(\"agriculture\", \"alder\", \"ash\", \"beech\", \"douglas_fir\", \"larch\",\n            \"oak\", \"pastures\", \"roads\", \"settlements\", \"spruce\", \"water\")\n)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#rationale-why-despeckle-first",
    "href": "doc/treespecies.html#rationale-why-despeckle-first",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "3. Rationale: Why Despeckle First?",
    "text": "3. Rationale: Why Despeckle First?\n\n🔍 Why do we despeckle at original resolution before aggregation and contextual filtering?\n\nPreserve spatial detail: High-frequency noise (e.g., misclassified single pixels) must be removed before they get averaged into larger grid cells.\nAvoid error propagation: Aggregating first would carry speckle artifacts into the coarser grid.\nEnable ecologically meaningful correction: Focal filtering (e.g., Douglas-fir to Oak) should be applied on ~1 m resolution where “dominance” of classes has meaning.\nStep order summary:\n\nClassificationMapRegularization: Clean noise at 0.2 m\naggregate(): Smooth to 1 m (e.g., crown scale)\nfocal(): Replace ecologically implausible patches",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#load-and-preprocess-species-classification",
    "href": "doc/treespecies.html#load-and-preprocess-species-classification",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "4. Load and Preprocess Species Classification",
    "text": "4. Load and Preprocess Species Classification\n\nsapflow_species &lt;- readRDS(\"../data/aerial/sfprediction_ffs_5-25_MOF_rgb.rds\")\nraster::writeRaster(sapflow_species, \"../data/aerial/prediction_ffs.tif\", overwrite = TRUE)\nsapflow_species &lt;- raster::crop(sapflow_species, sapflow_ext)\nraster::writeRaster(sapflow_species, \"../data/aerial/prediction_ffs_cut.tif\", overwrite = TRUE)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#majority-filtering-otb-despeckle",
    "href": "doc/treespecies.html#majority-filtering-otb-despeckle",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "5. Majority Filtering (OTB Despeckle)",
    "text": "5. Majority Filtering (OTB Despeckle)\n\ncmr &lt;- parseOTBFunction(\"ClassificationMapRegularization\", otb)\ncmr$io.in &lt;- \"../data/aerial/prediction_ffs.tif\"\ncmr$io.out &lt;- \"../data/aerial/majority_out.tif\"\ncmr$ip.radius &lt;- \"1\"\ncmr$progress &lt;- \"true\"\nfilter_treespecies &lt;- runOTB(cmr, gili = otb$pathOTB, quiet = FALSE, retRaster = TRUE)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#aggregate-to-1-m-resolution",
    "href": "doc/treespecies.html#aggregate-to-1-m-resolution",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "6. Aggregate to 1 m Resolution",
    "text": "6. Aggregate to 1 m Resolution\n\nr &lt;- rast(\"../data/aerial/majority_out.tif\")\ncur_res &lt;- res(r)[1]\nfact &lt;- round(target_res / cur_res)\nif (target_res &lt;= cur_res) stop(\"Target resolution is lower than input resolution.\")\nr_agg &lt;- aggregate(r, fact = fact, fun = median, na.rm = TRUE)\noutfile &lt;- sprintf(\"../data/aerial/%s_%sm.tif\", tools::file_path_sans_ext(basename(\"../data/aerial/aggregate.tif\")), target_res)\nwriteRaster(r_agg, outfile, overwrite = TRUE)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#contextual-correction-douglas-beechoak",
    "href": "doc/treespecies.html#contextual-correction-douglas-beechoak",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "7. Contextual Correction (Douglas → Beech/Oak)",
    "text": "7. Contextual Correction (Douglas → Beech/Oak)\n\nreplace_douglas_in_buche_eiche &lt;- function(rast_input,\n                                           window_size = 5,\n                                           douglas_value = 5,\n                                           target_values = c(4, 7),\n                                           target_res = 1.0) {\n  if (!inherits(rast_input, \"SpatRaster\")) stop(\"Input must be SpatRaster\")\n  if (window_size %% 2 == 0) stop(\"window_size must be odd\")\n  w &lt;- matrix(1, nrow = window_size, ncol = window_size)\n  r_mode &lt;- focal(rast_input, w = w, fun = modal, na.policy = \"omit\", na.rm = TRUE, progress = \"text\")\n  is_douglas &lt;- rast_input == douglas_value\n  is_oak_beech_mode &lt;- r_mode %in% target_values\n  replace_mask &lt;- is_douglas & is_oak_beech_mode\n  r_new &lt;- rast_input\n  r_new[replace_mask] &lt;- r_mode[replace_mask]\n  writeRaster(r_new, sprintf(\"../data/aerial/%s_%sm.tif\", \"agg_cleand\", target_res), overwrite = TRUE)\n  return(r_new)\n}\nspecies_cleaned &lt;- replace_douglas_in_buche_eiche(r_agg, window_size = 5)",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/treespecies.html#complete-code",
    "href": "doc/treespecies.html#complete-code",
    "title": "Tree Species Classification Cleaning with OTB and Terra",
    "section": "Complete Code",
    "text": "Complete Code\n\n#------------------------------------------------------------------------------\n# Script Type: Processing script\n# Script Name: 30_filter_classified_species_map.R\n# Author: Chris Reudenbach, creuden@gmail.com\n#\n# Description:\n#   - Loads a classified raster map of tree species\n#   - Applies spatial smoothing using OTB ClassificationMapRegularization\n#   - Aggregates to 1 m resolution using median filtering\n#   - Performs contextual correction: replaces isolated Douglas-fir pixels\n#     with Beech or Oak if those are dominant in the local neighborhood\n#\n# Input:\n#   - RDS and GeoTIFF classification of tree species (0.2 m resolution)\n#\n# Output:\n#   - Cleaned and aggregated species raster (1 m resolution)\n#\n# Dependencies:\n#   - OTB 9.1+ with PATH and OTB_APPLICATION_PATH correctly set\n#\n# Copyright: Chris Reudenbach 2021, GPL (&gt;= 3)\n# Git: https://github.com/gisma-courses/microclimate.git\n#\n# Commentary:\n#   This workflow separates two crucial but distinct steps in map cleaning:\n#   1. **Noise reduction (smoothing):** Applied via OTB's ClassificationMapRegularization.\n#      - It performs a fast majority-based smoothing using a local moving window (e.g., 3x3).\n#      - This step removes small speckles or misclassified pixels in homogeneous areas.\n#      - It is computationally efficient due to OTB's C++-based implementation.\n#\n#   2. **Semantic filtering:** Performed in R via a contextual reclassification function.\n#      - Specifically targets ecologically unlikely or isolated Douglas-fir pixels.\n#      - These are replaced with surrounding Beech or Oak pixels if they dominate locally.\n#      - Allows flexible, rule-based filtering that OTB cannot natively perform.\n#\n#   ➤ Both steps are technically possible in R using terra::focal(), but:\n#     - Smoothing with `focal()` is **much slower** on large rasters (single-threaded).\n#     - OTB is highly recommended for performance.\n#\n#   ➤ The R-based semantic filtering step is **required** if logical replacement\n#     rules (like Douglas-fir substitution) are needed. This goes beyond statistical smoothing.\n#------------------------------------------------------------------------------\n\n\n# === Libraries ===\nlibrary(terra)           # raster handling\n\nterra 1.8.60\n\nlibrary(RColorBrewer)    # color palettes\nlibrary(link2GI)         # OTB integration\nlibrary(rprojroot)\nlibrary(tools)           # file name tools\nlibrary(mapview)         # interactive maps\nlibrary(dplyr)           # data manipulation\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:terra':\n\n    intersect, union\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# === Environment and paths ===\nroot_folder &lt;- find_rstudio_root_file()\n\n# Set up OTB environment\notb &lt;- link2GI::linkOTB(searchLocation = \"~/apps/OTB-9.1.0-Linux/\")\nSys.setenv(OTB_APPLICATION_PATH = file.path(dirname(as.character(otb$pathOTB)), \"lib/otb/applications\"))\nSys.setenv(PATH = paste(otb$pathOTB, Sys.getenv(\"PATH\"), sep = \":\"))\n\n# === Parameters ===\ntarget_res &lt;- 1                # desired resolution in meters\nmin_tree_height &lt;- 2           # (not used yet)\nfn &lt;- \"5-25_MOF_rgb\"           # image stem\nepsg &lt;- 25832                  # UTM32N\nsapflow_ext &lt;- raster::extent(477500, 478218, 5631730, 5632500)  # area of interest\n\n# === Class ID legend ===\nts &lt;- data.frame(\n  ID = 1:12,\n  value = c(\n    \"agriculture\",\n    \"alder\",\n    \"ash\",\n    \"beech\",\n    \"douglas_fir\",\n    \"larch\",\n    \"oak\",\n    \"pastures\",\n    \"roads\",\n    \"settlements\",\n    \"spruce\",\n    \"water\"\n  )\n)\n\n#------------------------------------------------------------------------------\n# FUNCTION: Replace isolated Douglas-fir with Beech or Oak if dominant around\n#------------------------------------------------------------------------------\nreplace_douglas_in_buche_eiche &lt;- function(rast_input,\n                                           window_size = 5,\n                                           douglas_value = 5,\n                                           target_values = c(4, 7),\n                                           target_res = 1.0) {\n  if (window_size %% 2 == 0)\n    stop(\"window_size must be odd\")\n  \n  # Focal window matrix (square)\n  w &lt;- matrix(1, nrow = window_size, ncol = window_size)\n  \n  # Run OTB ClassificationMapRegularization to compute local mode\n  cmr &lt;- parseOTBFunction(\"ClassificationMapRegularization\", otb)\n  cmr$io.in &lt;- sprintf(\"../data/aerial/%s_%sm.tif\",\n                       tools::file_path_sans_ext(basename(\"../data/aerial/aggregate.tif\")),\n                       target_res)\n  cmr$io.out &lt;- sprintf(\"../data/aerial/%s_%sm.tif\",\n                        tools::file_path_sans_ext(basename(\"../data/aerial/aggregate_mode.tif\")),\n                        window_size)\n  cmr$ip.radius &lt;- as.character((window_size - 1) / 2)  # for 5x5 window: radius = (5 - 1)/2 = 2\n  cmr$progress &lt;- \"true\"\n  \n  runOTB(cmr, gili = otb$pathOTB, quiet = FALSE)\n\n  # Identify Douglas-fir pixels and surrounding Beech/Oak dominance\n  r_mode = rast(cmr$io.out)\n  rast_input = rast(cmr$io.in)\n  is_douglas &lt;- rast_input == douglas_value\n  is_oak_beech_mode &lt;- r_mode %in% target_values\n  replace_mask &lt;- is_douglas & is_oak_beech_mode\n  \n  # Replace Douglas-fir where Beech or Oak dominate\n  r_new &lt;- rast_input\n  r_new[replace_mask] &lt;- r_mode[replace_mask]\n  \n  # Construct output path \n  outname &lt;- paste0(\"../data/aerial/\",\n                    \"agg_cleand_\",\n                    as.character(target_res),\n                    \"m.tif\")\n  writeRaster(r_new, outname,overwrite = TRUE)\n  \n  return(r_new)\n}\n\n#------------------------------------------------------------------------------\n# STEP 1: Read tree species classification from RDS\n#------------------------------------------------------------------------------\nsapflow_species &lt;- readRDS(\"../data/aerial/sfprediction_ffs_5-25_MOF_rgb.rds\")\n\n# Write to GeoTIFF for further processing\nraster::writeRaster(\n  sapflow_species,\n  \"../data/aerial/prediction_ffs.tif\",\n  progress = \"text\",\n  overwrite = TRUE\n)\n\n# Crop to sapflow test area\nsapflow_species &lt;- raster::crop(sapflow_species, sapflow_ext)\nraster::writeRaster(\n  sapflow_species,\n  \"../data/aerial/prediction_ffs_cut.tif\",\n  progress = \"text\",\n  overwrite = TRUE\n)\n\n#------------------------------------------------------------------------------\n# STEP 2: Run OTB ClassificationMapRegularization (majority filter)\n#------------------------------------------------------------------------------\ncmr &lt;- parseOTBFunction(\"ClassificationMapRegularization\", otb)\ncmr$io.in &lt;- \"../data/aerial/prediction_ffs.tif\"\ncmr$io.out &lt;- \"../data/aerial/majority_out.tif\"\ncmr$progress &lt;- \"true\"\ncmr$ip.radius &lt;- \"1\"\n\nfilter_treespecies &lt;- runOTB(cmr,\n                             gili = otb$pathOTB,\n                             quiet = FALSE,\n                             retRaster = TRUE)\n\notbcli_ClassificationMapRegularization  -io.in ../data/aerial/prediction_ffs.tif -io.out /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/majority_out.tif -ip.radius 1 -ip.suvbool false -ip.nodatalabel 0 -ip.undecidedlabel 0 -ip.onlyisolatedpixels false -ip.isolatedthreshold 1 -ram 256 -progress true\n\n\n[1] \"2025-07-30 12:30:36 (INFO) ClassificationMapRegularization: Default RAM limit for OTB is 256 MB\"\n[1] \"2025-07-30 12:30:36 (INFO) ClassificationMapRegularization: GDAL maximum cache size is 4814 MB\"\n[1] \"2025-07-30 12:30:36 (INFO) ClassificationMapRegularization: OTB will use at most 32 threads\"\n[1] \"2025-07-30 12:30:36 (INFO): Loading metadata from official product\"\n[1] \"2025-07-30 12:30:36 (INFO): Estimated memory for full processing: 1052.34MB (avail.: 256 MB), optimal image partitioning: 5 blocks\"\n[1] \"2025-07-30 12:30:36 (INFO): File /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/majority_out.tif will be written in 6 blocks of 14798x2044 pixels\"\n[1] \"Writing /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/majority_out.tif...: 100% [**************************************************] (4s)\"\n\n#------------------------------------------------------------------------------\n# STEP 3: Aggregate to 1 m resolution using median\n#------------------------------------------------------------------------------\nr &lt;- rast(\"../data/aerial/majority_out.tif\")\ncur_res &lt;- res(r)[1]\nfact &lt;- round(target_res / cur_res)\n\nif (target_res &lt;= cur_res)\n  stop(\"Zielauflösung ist kleiner als aktuelle.\")\n\nr_agg &lt;- aggregate(r,\n                   fact = fact,\n                   fun = median,\n                   na.rm = TRUE)\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n# Build automatic filename\noutfile &lt;- sprintf(\"../data/aerial/%s_%sm.tif\",\n                   tools::file_path_sans_ext(basename(\"../data/aerial/aggregate.tif\")),\n                   target_res)\n\n# Save aggregated raster\nwriteRaster(r_agg, outfile, overwrite = TRUE)\n\n#------------------------------------------------------------------------------\n# STEP 4: Clean Douglas-fir patches contextually\n#------------------------------------------------------------------------------\nspecies_cleaned &lt;- replace_douglas_in_buche_eiche(window_size = 9)\n\notbcli_ClassificationMapRegularization  -io.in ../data/aerial/aggregate_1m.tif -io.out /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/aggregate_mode_9m.tif -ip.radius 4 -ip.suvbool false -ip.nodatalabel 0 -ip.undecidedlabel 0 -ip.onlyisolatedpixels false -ip.isolatedthreshold 1 -ram 256 -progress true\n\n\n[1] \"2025-07-30 12:30:43 (INFO) ClassificationMapRegularization: Default RAM limit for OTB is 256 MB\"\n[1] \"2025-07-30 12:30:43 (INFO) ClassificationMapRegularization: GDAL maximum cache size is 4814 MB\"\n[1] \"2025-07-30 12:30:43 (INFO) ClassificationMapRegularization: OTB will use at most 32 threads\"\n[1] \"2025-07-30 12:30:43 (INFO): Loading metadata from official product\"\n[1] \"2025-07-30 12:30:43 (INFO): Estimated memory for full processing: 43.842MB (avail.: 256 MB), optimal image partitioning: 1 blocks\"\n[1] \"2025-07-30 12:30:43 (INFO): File /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/aggregate_mode_9m.tif will be written in 1 blocks of 2960x2453 pixels\"\n[1] \"Writing /home/creu/edu/gisma-courses/tls-tree-climate/data/aerial/aggregate_mode_9m.tif...: 100% [**************************************************] (0s)\"\n\n#------------------------------------------------------------------------------\n# STEP 5: Visualize intermediate steps (interactive)\n#------------------------------------------------------------------------------\n\nlibrary(mapview)\nlibrary(leafsync)\nlibrary(htmlwidgets)\nlibrary(terra)\nlibrary(RColorBrewer)\n\n# Define common parameters\npalette &lt;- brewer.pal(12, \"Paired\")\nzoom_center &lt;- list(lng = 8.68443, lat = 50.84089, zoom = 18)\n\n# -- Map 1: Raw species classification (0.2 m)\nm1 &lt;- mapview(\n  terra::crop(sapflow_species, sapflow_ext),\n  col.regions = palette,\n  at = ts$ID,\n  layer.name = \"Species 0.2m\"\n)\n\nWarning in rasterCheckSize(x, maxpixels = maxpixels): maximum number of pixels for Raster* viewing is 5e+05 ; \nthe supplied Raster* has 13821500 \n ... decreasing Raster* resolution to 5e+05 pixels\n to view full resolution set 'maxpixels =  13821500 '\n\n# -- Map 2: OTB 3×3 modal smoothing\nm2 &lt;- mapview(\n  terra::crop(filter_treespecies, sapflow_ext),\n  col.regions = palette,\n  at = ts$ID,\n\n  layer.name = \"3x3 modal_filt\"\n)\n\nNumber of pixels is above 5e+05.Only about 5e+05 pixels will be shown.\nYou can increase the value of `maxpixels` to 13821500 to avoid this.\n\n# -- Map 3: Aggregated to 1 m resolution\nm3 &lt;- mapview(\n  terra::crop(r_agg, sapflow_ext),\n  col.regions = palette,\n  at = ts$ID,\n  layer.name = \"Aggregated 1m\"\n)\n\nNumber of pixels is above 5e+05.Only about 5e+05 pixels will be shown.\nYou can increase the value of `maxpixels` to 552860 to avoid this.\n\n\nWarning: Found more colors (12) than zcol values (11)! \nTrimming colors to match number of zcol values.\n\n# -- Map 4: Douglas-fir replaced by contextual rules\nm4 &lt;- mapview(\n  terra::crop(species_cleaned, sapflow_ext),\n  col.regions = palette,\n\n  fgb = TRUE,\n  layer.name = \"Douglas out 1m\"\n)\n\nNumber of pixels is above 5e+05.Only about 5e+05 pixels will be shown.\nYou can increase the value of `maxpixels` to 552860 to avoid this.\n\n\nWarning: Found more colors (12) than zcol values (11)! \nTrimming colors to match number of zcol values.\n\n# Convert to leaflet and apply zoom center\nlm1 &lt;- m1@map %&gt;% leaflet::setView(zoom_center$lng, zoom_center$lat, zoom_center$zoom)\nlm2 &lt;- m2@map %&gt;% leaflet::setView(zoom_center$lng, zoom_center$lat, zoom_center$zoom)\nlm3 &lt;- m3@map %&gt;% leaflet::setView(zoom_center$lng, zoom_center$lat, zoom_center$zoom)\nlm4 &lt;- m4@map %&gt;% leaflet::setView(zoom_center$lng, zoom_center$lat, zoom_center$zoom)\n\n# Synchronize maps side-by-side\n\nout &lt;- sync(lm1, lm2, lm3, lm4)\n\nout",
    "crumbs": [
      "Tree Species Classification Cleaning with OTB and Terra"
    ]
  },
  {
    "objectID": "doc/tls_v1_3.html",
    "href": "doc/tls_v1_3.html",
    "title": "Untitled",
    "section": "",
    "text": "For a realitic sourounding we need voxelized ALS (Airborne Laser Scanning) data which needs to be usable as ENVI-met compatible 3DPLANT profiles. It includes LAD (Leaf Area Density) computation, profile clustering, and export to both GIS and XML formats for ENVI-met integration.\nTo provide a realistic but computationally tractable vegetation structure, we apply a pseudo-3D columnar tree approach: each voxel column of ALS returns is interpreted as a simplified vertical tree profile. These profiles are clustered to reduce complexity, then exported as ENVI-met 3DPLANT objects.\n\n\nUnlike TLS (Terrestrial Laser Scanning), which scans from the bottom-up and suffers from occlusion in upper layers, ALS samples vegetation top-down. This means:\n\nALS oversamples upper canopy layers\nALS undersamples lower canopy due to occlusion\n\nTo correct for this sampling bias, we estimate LAD using a modified form of Beer’s Law, based on the normalized proportion of hits per voxel layer. The key difference lies in the way “gap probability” is estimated: rather than tracking cumulative occlusion, ALS uses the maximum return count per column as a proxy for full canopy closure.\n\n\n\nWe model LAD using:\n\\[\nLAD = -\\frac{\\ln(1 - p)}{k \\cdot dz}\n\\]\nWhere: - ( p ) is the normalized proportion of hits per voxel column (( 0 &lt; p &lt; 1 )) - ( k ) is the light extinction coefficient - ( dz ) is the vertical resolution (voxel height)\nIn our script, we set: - ( k = 0.3 ) (typical value) - LAD values are scaled using a multiplicative factor (default 1.2)\n\n\nIn TLS-based LAD estimation, we assume that the LiDAR sensor is located near ground level and that returns are accumulated from bottom to top. In this setup, each voxel’s return count ( N_i ) is interpreted as contributing to the cumulative transmittance through the canopy.\nThe Beer–Lambert law is applied as:\n\\[\n\\text{LAD}_i = -\\frac{\\ln\\left(1 - \\frac{N_i}{N_{\\max}}\\right)}{k \\cdot \\Delta z}\n\\]\nHere: - ( N_i ): number of returns in voxel layer ( i ) - ( N_{} ): maximum number of returns in any voxel in the column (used to normalize return density) - ( z ): voxel height - ( k ): extinction coefficient\n\n\nThe ratio ( ) estimates the fraction of light intercepted at layer ( i ), assuming the densest layer represents near-total occlusion. Thus, the term ( 1 - ) represents the gap fraction — i.e., the probability that a beam of light traveling from the ground upward has not yet been occluded by vegetation up to that layer.\nThis interpretation fits the TLS scanning geometry, where lower layers are sampled first and occlusion increases with height.\n\n\n\n\nWe assume that the highest return count in the column corresponds to full canopy closure (i.e., near-zero gap fraction). This allows us to use the maximum as a local normalization factor:\n\n( p_i = )\n( LAD_i = -(1 - p_i) / (k dz) )\n\nThis does not model occlusion directly, but gives a consistent LAD profile for column-wise clustering.\n\n\n\n\n\nlibrary(lidR)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(here)\nlibrary(XML)\nlibrary(stats)\nlibrary(tibble)\nlibrary(data.table)\nlibrary(rprojroot)\n\nzmax &lt;- 40  \ngrain.size &lt;- 1  \noutput_merged_las &lt;- here(\"data/ALS/merged_output.las\")  \n\noutput_envimet_als_3d &lt;- here(\"data/envimet/als_envimet_trees.pld\")  \nsource(\"../src/utils.R\")\n\n\n\n\nlas_file &lt;- here(\"data/ALS/tiles\")  # Path to ALS point cloud file \n\n las_fn = merge_las_tiles(\n    tile_dir = las_file,\n    output_file = output_merged_las,\n    chunk_size = 10000,\n    workers = 6\n  )\n\nlas &lt;- readLAS(las_fn)\nlas &lt;- normalize_height(las, knnidw(k = 6, p = 2))\nlas &lt;- filter_poi(las, Z &gt; 0)\n\n\n\n\n\nvoxels &lt;- voxel_metrics(las, ~length(Z), res = res_xy, dz = res_z)\n\n\n\n\n\nlad_df &lt;- convert_voxel_lad_long(voxels, res_z = res_z, k = k, scale_factor = scale_factor)\n\n\n\n\nWe reduce the number of ENVI-met profiles by grouping similar LAD profiles using k-means clustering. LAD profiles are pivoted to a wide matrix (z-layers as columns):\n\n# Create a unique key per voxel column (x/y location)\nlad_df$xy_key &lt;- paste(lad_df$x, lad_df$y)\n\n# Reshape the long-format LAD data to a wide matrix:\n# each row = 1 (x, y) location (column), each column = height bin (z), cell = LAD value\nlad_matrix &lt;- lad_df %&gt;% \n  tidyr::pivot_wider(\n    names_from = z,            # use height (z) as new column names\n    values_from = lad,         # fill cells with LAD values\n    values_fill = 0            # fill missing voxel heights with 0 (no LAD)\n  ) %&gt;%\n  column_to_rownames(\"xy_key\") %&gt;%  # set xy_key as row names\n  as.matrix()                       # convert to numeric matrix for clustering\n\n# Cluster LAD profiles using k-means\nclustering &lt;- kmeans(lad_matrix, centers = n_clusters, nstart = 100, iter.max = 200)\nlad_df$cluster &lt;- clustering$cluster[match(lad_df$xy_key, rownames(lad_matrix))]\n\n\n\n\nEach LAD cluster is assigned a unique identifier that begins with “S” and uses base36 encoding (0–9, A–Z):\n\n#' Convert an integer to a 6-character alphanumeric ENVIMET ID (Base36)\n#'\n#' Converts an integer into a 6-character string using base-36 encoding (digits + uppercase letters),\n#' padded with leading zeros and prefixed with `\"S\"`. This is useful for assigning unique `ENVIMET_ID`s\n#' in 3D plant libraries for ENVI-met.\n#'\n#' @param n An integer (or vector of integers) to convert to alphanumeric IDs.\n#' @param width The number of base-36 digits to use (default: `5`, resulting in IDs like `\"S0000A\"`).\n#'\n#' @return A character vector of base-36 encoded IDs (with `\"S\"` prefix).\n#' @export\n#'\n#' @examples\n#' int_to_base36(1)       # \"S00001\"\n#' int_to_base36(35)      # \"S0000Z\"\n#' int_to_base36(36)      # \"S00010\"\n#' int_to_base36(123456)  # \"S02N9S\"\n#' int_to_base36(1:3)     # \"S00001\" \"S00002\" \"S00003\"\nint_to_base36 &lt;- function(n, width = 5) {\n  chars &lt;- c(0:9, LETTERS)         # Base-36 character set\n  base &lt;- length(chars)\n  result &lt;- character()\n  \n  while (n &gt; 0) {\n    result &lt;- c(chars[(n %% base) + 1], result)\n    n &lt;- n %/% base\n  }\n  \n  result &lt;- paste(result, collapse = \"\")\n  \n  # Pad with leading zeros if necessary\n  padded &lt;- sprintf(paste0(\"%0\", width, \"s\"), result)\n  \n  # Replace any spaces with \"0\" and prefix with \"S\"\n  paste0(\"S\", substr(gsub(\" \", \"0\", padded), 1, width))\n}\n\n\n\n\nEach unique LAD column becomes a point in a GeoPackage, tagged with its ENVIMET_ID.\n\n# Assign ENVIMET_IDs per cluster\ncluster_ids &lt;- unique(lad_df$cluster)\ncluster_mapping &lt;- data.frame(\n  cluster = cluster_ids,\n  ENVIMET_ID = sapply(cluster_ids, int_to_base36)\n)\nlad_df &lt;- left_join(lad_df, cluster_mapping, by = \"cluster\")\n\nsf_points &lt;- st_as_sf(point_df, coords = c(\"x\", \"y\"), crs = crs_code)\nst_write(sf_points, output_gpkg, delete_layer = TRUE)\n\n\n\n\nThe LAD profile per cluster is exported to a .pld file using XML.\n\nexport_lad_to_envimet_p3d(\n  lad_df = lad_clustered, \n  file_out = output_envimet_als_3d\n)\n\n\n\n\n\nEach clustered LAD profile is interpreted as a pseudo-3D vegetation column. These are not derived from segmented individual trees but represent aggregated vertical structure typical for a 2 × 2 m area.\nThis approach provides a balance between realism and simplicity:\n\nIt allows realistic vertical vegetation profiles from ALS\nReduces complexity through clustering\nProvides efficient integration into ENVI-met via both:\n\nGIS point layers with ENVIMET_ID\nXML-based 3DPLANT definitions\n\n\nPseudo-3D trees enable realistic microclimate domains with vegetation heterogeneity without requiring full 3D reconstruction.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nTLS\nALS\n\n\n\n\nView Direction\nBottom-up\nTop-down\n\n\nOcclusion Bias\nUndersamples upper canopy\nUndersamples lower canopy\n\n\nLAD Estimation\nCumulative bottom-up (Beer)\nNormalized per column (max count)\n\n\nTypical Use Case\nDetailed single tree analysis\nLarge-area structure sampling\n\n\n\n\n\n\nThis pipeline offers an efficient method to integrate voxelized ALS data into ENVI-met’s 3DPLANT framework by:\n\nEstimating LAD profiles via a Beer–Lambert-based approximation\nClustering voxel columns into representative pseudo-3D vegetation types\nExporting both point geometries and XML-based plant profiles\n\nAdvantages: - Scalable to large ALS datasets - Preserves key structural heterogeneity - Compatible with ENVI-met simulation domains\nLimitations: - Assumes that the maximum voxel return represents full canopy cover, which may not hold in sparse stands - LAD estimation is empirical; it does not model true light attenuation or occlusion - The pseudo-3D approach does not represent individual trees or crown geometry - Clustering may smooth out fine-scale vertical variability\nFuture improvements could include stratified LAD normalization, occlusion-aware corrections, or hybrid ALS-TLS fusion for enhanced realism.\n\n\n\n\nBéland, M., et al. (2014). Remote Sensing of Environment\nCalders, K., et al. (2015). Methods in Ecology and Evolution\nJupp, D. L. B., et al. (2009). Remote Sensing of Environment\n\n\n\n\nsource(\"src/microclimate_ALS.R\")\nThis source contains the complete processing workflow from voxel metrics to XML generation."
  },
  {
    "objectID": "doc/tls_v1_3.html#why-als-requires-a-specific-lad-approach",
    "href": "doc/tls_v1_3.html#why-als-requires-a-specific-lad-approach",
    "title": "Untitled",
    "section": "",
    "text": "Unlike TLS (Terrestrial Laser Scanning), which scans from the bottom-up and suffers from occlusion in upper layers, ALS samples vegetation top-down. This means:\n\nALS oversamples upper canopy layers\nALS undersamples lower canopy due to occlusion\n\nTo correct for this sampling bias, we estimate LAD using a modified form of Beer’s Law, based on the normalized proportion of hits per voxel layer. The key difference lies in the way “gap probability” is estimated: rather than tracking cumulative occlusion, ALS uses the maximum return count per column as a proxy for full canopy closure."
  },
  {
    "objectID": "doc/tls_v1_3.html#lad-estimation-using-beers-law",
    "href": "doc/tls_v1_3.html#lad-estimation-using-beers-law",
    "title": "Untitled",
    "section": "",
    "text": "We model LAD using:\n\\[\nLAD = -\\frac{\\ln(1 - p)}{k \\cdot dz}\n\\]\nWhere: - ( p ) is the normalized proportion of hits per voxel column (( 0 &lt; p &lt; 1 )) - ( k ) is the light extinction coefficient - ( dz ) is the vertical resolution (voxel height)\nIn our script, we set: - ( k = 0.3 ) (typical value) - LAD values are scaled using a multiplicative factor (default 1.2)\n\n\nIn TLS-based LAD estimation, we assume that the LiDAR sensor is located near ground level and that returns are accumulated from bottom to top. In this setup, each voxel’s return count ( N_i ) is interpreted as contributing to the cumulative transmittance through the canopy.\nThe Beer–Lambert law is applied as:\n\\[\n\\text{LAD}_i = -\\frac{\\ln\\left(1 - \\frac{N_i}{N_{\\max}}\\right)}{k \\cdot \\Delta z}\n\\]\nHere: - ( N_i ): number of returns in voxel layer ( i ) - ( N_{} ): maximum number of returns in any voxel in the column (used to normalize return density) - ( z ): voxel height - ( k ): extinction coefficient\n\n\nThe ratio ( ) estimates the fraction of light intercepted at layer ( i ), assuming the densest layer represents near-total occlusion. Thus, the term ( 1 - ) represents the gap fraction — i.e., the probability that a beam of light traveling from the ground upward has not yet been occluded by vegetation up to that layer.\nThis interpretation fits the TLS scanning geometry, where lower layers are sampled first and occlusion increases with height.\n\n\n\n\nWe assume that the highest return count in the column corresponds to full canopy closure (i.e., near-zero gap fraction). This allows us to use the maximum as a local normalization factor:\n\n( p_i = )\n( LAD_i = -(1 - p_i) / (k dz) )\n\nThis does not model occlusion directly, but gives a consistent LAD profile for column-wise clustering."
  },
  {
    "objectID": "doc/tls_v1_3.html#full-workflow-voxelization-to-envi-met-3d-trees",
    "href": "doc/tls_v1_3.html#full-workflow-voxelization-to-envi-met-3d-trees",
    "title": "Untitled",
    "section": "",
    "text": "library(lidR)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(here)\nlibrary(XML)\nlibrary(stats)\nlibrary(tibble)\nlibrary(data.table)\nlibrary(rprojroot)\n\nzmax &lt;- 40  \ngrain.size &lt;- 1  \noutput_merged_las &lt;- here(\"data/ALS/merged_output.las\")  \n\noutput_envimet_als_3d &lt;- here(\"data/envimet/als_envimet_trees.pld\")  \nsource(\"../src/utils.R\")\n\n\n\n\nlas_file &lt;- here(\"data/ALS/tiles\")  # Path to ALS point cloud file \n\n las_fn = merge_las_tiles(\n    tile_dir = las_file,\n    output_file = output_merged_las,\n    chunk_size = 10000,\n    workers = 6\n  )\n\nlas &lt;- readLAS(las_fn)\nlas &lt;- normalize_height(las, knnidw(k = 6, p = 2))\nlas &lt;- filter_poi(las, Z &gt; 0)\n\n\n\n\n\nvoxels &lt;- voxel_metrics(las, ~length(Z), res = res_xy, dz = res_z)\n\n\n\n\n\nlad_df &lt;- convert_voxel_lad_long(voxels, res_z = res_z, k = k, scale_factor = scale_factor)\n\n\n\n\nWe reduce the number of ENVI-met profiles by grouping similar LAD profiles using k-means clustering. LAD profiles are pivoted to a wide matrix (z-layers as columns):\n\n# Create a unique key per voxel column (x/y location)\nlad_df$xy_key &lt;- paste(lad_df$x, lad_df$y)\n\n# Reshape the long-format LAD data to a wide matrix:\n# each row = 1 (x, y) location (column), each column = height bin (z), cell = LAD value\nlad_matrix &lt;- lad_df %&gt;% \n  tidyr::pivot_wider(\n    names_from = z,            # use height (z) as new column names\n    values_from = lad,         # fill cells with LAD values\n    values_fill = 0            # fill missing voxel heights with 0 (no LAD)\n  ) %&gt;%\n  column_to_rownames(\"xy_key\") %&gt;%  # set xy_key as row names\n  as.matrix()                       # convert to numeric matrix for clustering\n\n# Cluster LAD profiles using k-means\nclustering &lt;- kmeans(lad_matrix, centers = n_clusters, nstart = 100, iter.max = 200)\nlad_df$cluster &lt;- clustering$cluster[match(lad_df$xy_key, rownames(lad_matrix))]\n\n\n\n\nEach LAD cluster is assigned a unique identifier that begins with “S” and uses base36 encoding (0–9, A–Z):\n\n#' Convert an integer to a 6-character alphanumeric ENVIMET ID (Base36)\n#'\n#' Converts an integer into a 6-character string using base-36 encoding (digits + uppercase letters),\n#' padded with leading zeros and prefixed with `\"S\"`. This is useful for assigning unique `ENVIMET_ID`s\n#' in 3D plant libraries for ENVI-met.\n#'\n#' @param n An integer (or vector of integers) to convert to alphanumeric IDs.\n#' @param width The number of base-36 digits to use (default: `5`, resulting in IDs like `\"S0000A\"`).\n#'\n#' @return A character vector of base-36 encoded IDs (with `\"S\"` prefix).\n#' @export\n#'\n#' @examples\n#' int_to_base36(1)       # \"S00001\"\n#' int_to_base36(35)      # \"S0000Z\"\n#' int_to_base36(36)      # \"S00010\"\n#' int_to_base36(123456)  # \"S02N9S\"\n#' int_to_base36(1:3)     # \"S00001\" \"S00002\" \"S00003\"\nint_to_base36 &lt;- function(n, width = 5) {\n  chars &lt;- c(0:9, LETTERS)         # Base-36 character set\n  base &lt;- length(chars)\n  result &lt;- character()\n  \n  while (n &gt; 0) {\n    result &lt;- c(chars[(n %% base) + 1], result)\n    n &lt;- n %/% base\n  }\n  \n  result &lt;- paste(result, collapse = \"\")\n  \n  # Pad with leading zeros if necessary\n  padded &lt;- sprintf(paste0(\"%0\", width, \"s\"), result)\n  \n  # Replace any spaces with \"0\" and prefix with \"S\"\n  paste0(\"S\", substr(gsub(\" \", \"0\", padded), 1, width))\n}\n\n\n\n\nEach unique LAD column becomes a point in a GeoPackage, tagged with its ENVIMET_ID.\n\n# Assign ENVIMET_IDs per cluster\ncluster_ids &lt;- unique(lad_df$cluster)\ncluster_mapping &lt;- data.frame(\n  cluster = cluster_ids,\n  ENVIMET_ID = sapply(cluster_ids, int_to_base36)\n)\nlad_df &lt;- left_join(lad_df, cluster_mapping, by = \"cluster\")\n\nsf_points &lt;- st_as_sf(point_df, coords = c(\"x\", \"y\"), crs = crs_code)\nst_write(sf_points, output_gpkg, delete_layer = TRUE)\n\n\n\n\nThe LAD profile per cluster is exported to a .pld file using XML.\n\nexport_lad_to_envimet_p3d(\n  lad_df = lad_clustered, \n  file_out = output_envimet_als_3d\n)"
  },
  {
    "objectID": "doc/tls_v1_3.html#concept-of-pseudo-3d-tree-columns",
    "href": "doc/tls_v1_3.html#concept-of-pseudo-3d-tree-columns",
    "title": "Untitled",
    "section": "",
    "text": "Each clustered LAD profile is interpreted as a pseudo-3D vegetation column. These are not derived from segmented individual trees but represent aggregated vertical structure typical for a 2 × 2 m area.\nThis approach provides a balance between realism and simplicity:\n\nIt allows realistic vertical vegetation profiles from ALS\nReduces complexity through clustering\nProvides efficient integration into ENVI-met via both:\n\nGIS point layers with ENVIMET_ID\nXML-based 3DPLANT definitions\n\n\nPseudo-3D trees enable realistic microclimate domains with vegetation heterogeneity without requiring full 3D reconstruction."
  },
  {
    "objectID": "doc/tls_v1_3.html#tls-vs-als-lad-computation-summary",
    "href": "doc/tls_v1_3.html#tls-vs-als-lad-computation-summary",
    "title": "Untitled",
    "section": "",
    "text": "Aspect\nTLS\nALS\n\n\n\n\nView Direction\nBottom-up\nTop-down\n\n\nOcclusion Bias\nUndersamples upper canopy\nUndersamples lower canopy\n\n\nLAD Estimation\nCumulative bottom-up (Beer)\nNormalized per column (max count)\n\n\nTypical Use Case\nDetailed single tree analysis\nLarge-area structure sampling"
  },
  {
    "objectID": "doc/tls_v1_3.html#conclusion-and-limitations",
    "href": "doc/tls_v1_3.html#conclusion-and-limitations",
    "title": "Untitled",
    "section": "",
    "text": "This pipeline offers an efficient method to integrate voxelized ALS data into ENVI-met’s 3DPLANT framework by:\n\nEstimating LAD profiles via a Beer–Lambert-based approximation\nClustering voxel columns into representative pseudo-3D vegetation types\nExporting both point geometries and XML-based plant profiles\n\nAdvantages: - Scalable to large ALS datasets - Preserves key structural heterogeneity - Compatible with ENVI-met simulation domains\nLimitations: - Assumes that the maximum voxel return represents full canopy cover, which may not hold in sparse stands - LAD estimation is empirical; it does not model true light attenuation or occlusion - The pseudo-3D approach does not represent individual trees or crown geometry - Clustering may smooth out fine-scale vertical variability\nFuture improvements could include stratified LAD normalization, occlusion-aware corrections, or hybrid ALS-TLS fusion for enhanced realism."
  },
  {
    "objectID": "doc/tls_v1_3.html#references",
    "href": "doc/tls_v1_3.html#references",
    "title": "Untitled",
    "section": "",
    "text": "Béland, M., et al. (2014). Remote Sensing of Environment\nCalders, K., et al. (2015). Methods in Ecology and Evolution\nJupp, D. L. B., et al. (2009). Remote Sensing of Environment"
  },
  {
    "objectID": "doc/tls_v1_3.html#script-reference",
    "href": "doc/tls_v1_3.html#script-reference",
    "title": "Untitled",
    "section": "",
    "text": "source(\"src/microclimate_ALS.R\")\nThis source contains the complete processing workflow from voxel metrics to XML generation."
  },
  {
    "objectID": "doc/microclimate_predictor_stack_commented.html",
    "href": "doc/microclimate_predictor_stack_commented.html",
    "title": "Microclimate Predictor Stack Tutorial",
    "section": "",
    "text": "1 Introduction\nThis tutorial documents the modular processing chain for deriving microclimate-relevant predictors from ALS (Airborne Laser Scanning) data.\nIt is based on the script 20_microclimate_predictor_stack.R, which builds a raster predictor stack used in microclimate or ecological modeling.\n\n\n\n2 1. Overall Workflow Diagram\n\n\n\n\n\nflowchart TD\n    LAS[\"LAS Input Data\"]\n    DEM[\"Normalize & Create DEM/DSM/CHM\"]\n    PM[\"Pixel-Level Metrics\"]\n    SEG[\"Tree Segmentation\"]\n\n    TOPO[\"Topographic Variables\"]\n    VOX[\"Voxel Metrics: VCI, LAD, Entropy\"]\n    LAD[\"LAD Profiles\"]\n    CLU[\"Tree Cluster Analysis\"]\n\n    MERGE[\"Merge: Predictor Stack\"]\n    OUT[\"Final Raster Predictor Stack\"]\n\n    LAS --&gt; DEM\n    LAS --&gt; PM\n    LAS --&gt; SEG\n\n    DEM --&gt; TOPO\n    PM --&gt; VOX\n    SEG --&gt; LAD\n    LAD --&gt; CLU\n\n    TOPO --&gt; MERGE\n    VOX --&gt; MERGE\n    CLU --&gt; MERGE\n\n    MERGE --&gt; OUT\n\n\n\n\n\n\nThis diagram shows the data flow:\n\nThe LAS file is used in 3 parallel branches.\nTopographic, voxel, and tree-based metrics are computed independently.\nFinally, all are merged into one raster predictor stack.\n\n\n\n\n3 2. Project Setup\n# Load required packages and environment\nrequire(envimaR)\nrequire(rprojroot)\n\n# Determine root directory of project (requires .Rproj or .here file)\nroot_folder &lt;- find_rstudio_root_file()\n\n# Load envrmt list with all folder paths and EPSG settings\nsource(file.path(root_folder, \"src/000-rspatial-setup.R\"), echo = TRUE)\n\nenvimaR handles dynamic folder structures.\nenvrmt contains paths like path_lidar_raster, path_topo, etc.\nepsg_number, bbox and other global spatial variables are set here.\n\n\n\n\n4 3. Normalizing the LAS Catalog\nctg &lt;- readLAScatalog(las_fileFN)\nctg_base &lt;- normalize_height(ctg, knnidw(k = 6L, p = 2))\n\nA LAS catalog is loaded and normalized.\nGround points are removed to prepare for CHM and DSM creation.\n\n\n\n\n5 4. Terrain Models\ndem &lt;- rasterize_terrain(ctg, res = 1, knnidw(k = 6L, p = 2))\ndsm &lt;- rasterize_canopy(ctg, res = 1, algorithm = pitfree())\nchm &lt;- rasterize_canopy(ctg_base, res = 1, pitfree(c(0,2,5,10,15)))\n\nDEM (Digital Elevation Model) is created from ground returns.\nDSM (Surface Model) and CHM (Canopy Height Model) from canopy points.\n\n\n\n\n6 5. Topographic Derivatives\nslope &lt;- terrain(dem, \"slope\")\naspect &lt;- terrain(dem, \"aspect\")\nTPI &lt;- terrain(dsm, \"TPI\")\n\nDerived terrain parameters used for modeling light, moisture, and temperature.\n\n\n\n\n7 6. Pixel-Level Metrics\npixel_stdmetrics &lt;- pixel_metrics(ctg_base, .stdmetrics, res = 1)\npixel_LAD &lt;- pixel_metrics(ctg_base, ~as.numeric(cv(LAD(Z, dz = 1, k = 0.87)$lad)), res = 1)\npixel_entropy &lt;- pixel_metrics(ctg_base, ~as.numeric(entropy(Z, by = 1.0)), res = 1)\npixel_VCI &lt;- pixel_metrics(ctg_base, ~as.numeric(VCI(Z, zmax = 40, by = 1.0)), res = 1)\nThese voxel-based metrics represent vertical structure:\n\nLAD = Leaf Area Density\nVCI = Vertical Complexity Index\nEntropy = point height diversity\nipground = intensity of ground points (optional)\n\n\n\n\n8 7. Tree Segmentation and Metrics\nctg_seg &lt;- segment_trees(ctg_base, li2012())\nhulls &lt;- catalog_apply(ctg_seg, tree_fn)\nlad_vox &lt;- lad.voxels(ctg_base, grain.size = 1, k = 0.87, maxP = 40)\n\nTrees are segmented using the Li et al. (2012) method.\ntree_fn generates convex hulls or crown shapes.\nLAD profiles are voxelized and linked to hulls.\n\n\n\n\n9 8. Clustering Tree Profiles\nclust_model &lt;- KMeans_arma(data_clust, clusters = 10, n_iter = 500)\ntrees_lad$cluster &lt;- predict_KMeans(data_clust, clust_model)\n\nLAD metrics are dimensionally reduced (PCA or manually).\nClustering assigns structural class per tree.\nResult is written as vector layer and rasterized.\n\n\n\n\n10 9. Predictor Stack Creation\nforest_structure_metrics &lt;- c(rast(topoFN), rast(pmetricsFN), rast(tree_clus_rasFN))\nwriteRaster(forest_structure_metrics, predstack_forest_metricsFN, overwrite = TRUE)\n\nCombines topography, pixel metrics, and clusters into one multiband raster.\n\n\n\n\n11 10. Optional: Solar Irradiance via GRASS\nlinkGRASS7(dem, gisdbase = root_folder, location = \"MOF2\")\nexecGRASS(\"r.sun.hourly\", parameters = list(...))\n\nOptionally runs r.sun.hourly from GRASS to model solar radiation.\nResulting hourly radiation maps can be included in predictor stacks.\n\n\n\n\n12 Output Summary\n\n\n\n\n\n\n\n\nLayer\nType\nDescription\n\n\n\n\ntopo.tif\nRaster\nTerrain-derived variables\n\n\nall_pixel_metrics.tif\nRaster\nStructural voxel statistics\n\n\nlad_hull_raster.tif\nRaster\nLAD metrics aggregated to tree hulls\n\n\ntree_cluster.tif\nRaster\nCluster class per tree segment\n\n\npred_forest_structure.tif\nRaster\nFull predictor stack for modeling\n\n\ntrees_lad_clean.rds\nDataFrame\nTree-level statistics for analysis\n\n\n\n\n\n\n13 Questions or Extensions\n\nAdd modeling scripts (e.g. Random Forest, GLM, XGBoost)\nVisualize clusters with tmap or leaflet\nCombine with microclimate sensors or UAV data"
  },
  {
    "objectID": "doc/tls_v1_2.html#background-and-method",
    "href": "doc/tls_v1_2.html#background-and-method",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Background and Method",
    "text": "Background and Method\nThis section explains the theoretical principles of leaf area density (LAD) and describes how it can be determined using terrestrial laser scanning (TLS). Leaf area density is an important parameter in environmental modeling, for example for radiation balance and microclimate simulations. It indicates the leaf area per volume (m²/m³) and is therefore a decisive factor for microclimate simulations, radiation models, and energy flows in vegetation stands.\n\n\n\n\n\n\n\n\nApproach Type\nName / Description\nNature\n\n\n\n\nPulse-count based\nSimple linear normalization of return counts or voxel hits\nEmpirical, direct\n\n\nLinear normalization\nStraightforward normalization of pulse counts by voxel volume or max LAD\nEmpirical, basic\n\n\nPulse-density normalization\nAdjusts for occlusion and scan geometry\nSemi-empirical\n\n\nGap fraction models\nEstimate LAD/LAI from canopy openness statistics\nSemi-empirical\n\n\nBeer–Lambert conversion conversion\nUses exponential light attenuation to infer LAD\nPhysically-based\n\n\nVoxel-based inverse modeling\nOptimizes 3D LAD to match observed light attenuation or reflectance\nPhysically-based\n\n\nAllometric / geometric reconstruction\nReconstructs crown volume and distributes LAD using QSM or shape fitting\nGeometric, structural\n\n\n\n\nLinear normalization is a practical baseline: simple, fast, and reproducible.\nBeer–Lambert conversion introduces realism via physical light attenuation.\n\nMore advanced models (e.g. voxel inverse or QSM-based) aim for higher biophysical fidelity at the cost of complexity.\nThe present analysis is based on TLS with a medium-range RIEGL scanner (e.g., VZ-400). This captures millions of 3D points of the vegetation structure with high angular resolution. The point cloud is divided into uniform voxels, from which the leaf area density is estimated in two ways.\n\nLinear normalization (straightforwad)\n\\[\n\\text{LAD}_i = \\frac{N_i}{N_{\\max}} \\cdot \\text{LAD}_{\\max}\n\\] - \\(N_i\\): Number of laser points in voxel \\(i\\)\n- \\(N_{\\max}\\): Maximum across all voxels\n- \\(\\text{LAD}_{\\max}\\): Maximum LAD value from the literature (e.g., 5 m²/m³)\n\n\n\nBeer–Lambert conversion\n\\[\n\\text{LAD}_i = -\\frac{\\ln\\left(1 - \\frac{N_i}{N_{\\max}}\\right)}{k \\cdot \\Delta z}\n\\]\n\n\\(k\\): Extinction coefficient (typically 0.3–0.5)\n\\(\\Delta z\\): vertical voxel height\n\n\n\nOverall Workflow\nWhat happens in the script?\n\n\n\n\n\n\n\n\nStep\nDescription\nRelevant Code\n\n\n\n\n1. Read & Filter LAS\nLoad TLS data, optionally crop and clean it\nreadLAS() and las = filter_poi(...)\n\n\n2. Voxel Grid Setup\nSet up 3D grid at defined grain.size\npassed to pixel_metrics(..., res = grain.size)\n\n\n3. Count Pulses\nCount returns in each voxel height bin\npointsByZSlice() function\n\n\n4. Normalise Pulse Counts\nDivide by global max (relative LAD)\nin convert_to_LAD(): lad = (count / max) * LADmax\n\n\n5. Export Raster\nConvert metrics to raster stack\nterra::rast() from voxel_df\n\n\n6. Visualization\nPlot LAD profiles\nsee plotting section\n\n\n7. Export to Plant3D\nExports the LAD to ENVI-met\nsee export section",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#implemetation",
    "href": "doc/tls_v1_2.html#implemetation",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Implemetation",
    "text": "Implemetation\nTo use this ENVI-met tree modeling workflow in R, follow these steps to load and initialize the project correctly:",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#clone-github-repo-in-rstudio",
    "href": "doc/tls_v1_2.html#clone-github-repo-in-rstudio",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Clone GitHub Repo in RStudio",
    "text": "Clone GitHub Repo in RStudio\n\nOption 1: RStudio GUI\n\nGo to File → New Project → Version Control → Git\nEnter the repository URL:\nhttps://github.com/gisma-courses/tls-tree-climate.git\nChoose a project directory and name\nClick Create Project\n\n\n\n\nOption 2: Terminal\ngit clone https://github.com/gisma-courses/tls-tree-climate.git\nThen open the cloned folder in RStudio (via .Rproj file or “Open Project”).\n\nNote: Make sure Git is installed and configured in\nRStudio → Tools → Global Options → Git/SVN\n\nThe use of the {here} package depends on having a valid RStudio project. Without this, file paths may not resolve correctly.\n\n\nData Input Parameters and Paths\n\n\n\n\n\n\nThe input data set is a cleaned terrestrial laser scan of a single, isolated tree. All surrounding vegetation and ground points have been removed, so the file contains only the tree’s structure—trunk, branches, and foliage. Stored in standard LAS format, it provides high-resolution 3D point data suitable for voxelization, LAD calculation, or input into microclimate and radiative models. This detailed structural data is essential for generating true 3D tree entities in ENVI-met; without it, only simplified vegetation (SimplePlants) can be used.\n\n\n\nSet global parameters for the workflow, such as file paths, voxel resolution, and maximum LAD value for normalization.\n\nlibrary(lidR)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(here)\nlibrary(XML)\nlibrary(stats)\nlibrary(tibble)\nlibrary(data.table)\nlibrary(rprojroot)\n\nzmax &lt;- 40  \ngrain.size &lt;- 1  \nproject_root &lt;- here::here()  \n\n# Choose LAD method: \"linear\" or \"beer\"\n# Beer–Lambert conversion Notes:\n# - Avoids log(0) and 1 by clipping near-extreme values\n# - Use when cumulative light absorption or occlusion is relevant\n# - Suitable if extinction coefficient is known or estimated from prior studies\nlad_method &lt;- \"beer\"  # Set to \"linear\" or \"beer\"\n\n# Optional: extinction coefficient (used only for Beer–Lambert conversion)\nk_extinction &lt;- 0.25\n\n\noutput_voxels &lt;- file.path(project_root, \"data/TLS/LAD_voxDF.rds\")  \noutput_array &lt;- file.path(project_root, \"data/TLS/lad_array_m2m3.rds\")  \noutput_profile_plot &lt;- file.path(project_root, \"data/TLS/lad_vertical_profile.pdf\")  \noutput_envimet_tls_3d &lt;- file.path(project_root, \"data/envimet/tls_envimet_trees.pld\")  \noutput_envimet_als_3d &lt;- file.path(project_root, \"data/envimet/als_envimet_trees.pld\")  \n\n\n\n\nVoxelization of TLS data\nVoxelisation turns a 3D TLS point cloud into a grid of cubes (voxels), where each voxel holds structural information. The number of points per voxel is used to estimate Leaf Area Density (LAD), typically normalized relative to the voxel with the most returns.\n\nEach voxel = a 1×1×1 m³ cube\nCount the laser hits per voxel\nNormalize to maximum\nMultiply by a literature-based LAD_max (e.g. 5 m²/m³)\n\nThis gives a spatially distributed LAD profile suitable for further analysis or models like ENVI-met.\n\n\n\n\n\n\nView Code\n\n\n\n\n\n\nlibrary(terra)\n\n las=lidR::readLAS(\"../data/TLS/tree_08.laz\")\n\n\n[===========================&gt;                      ] 54% ETA: 1s     \n[===========================&gt;                      ] 54% ETA: 1s     \n[===========================&gt;                      ] 54% ETA: 1s     \n[===========================&gt;                      ] 54% ETA: 1s     \n[===========================&gt;                      ] 55% ETA: 1s     \n[===========================&gt;                      ] 55% ETA: 1s     \n[===========================&gt;                      ] 55% ETA: 1s     \n[===========================&gt;                      ] 55% ETA: 1s     \n[===========================&gt;                      ] 55% ETA: 1s     \n[===========================&gt;                      ] 55% ETA: 1s     \n[============================&gt;                     ] 56% ETA: 1s     \n[============================&gt;                     ] 56% ETA: 1s     \n[============================&gt;                     ] 56% ETA: 1s     \n[============================&gt;                     ] 56% ETA: 1s     \n[============================&gt;                     ] 56% ETA: 1s     \n[============================&gt;                     ] 56% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[============================&gt;                     ] 57% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 58% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[=============================&gt;                    ] 59% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 60% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[==============================&gt;                   ] 61% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 62% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[===============================&gt;                  ] 63% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 64% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[================================&gt;                 ] 65% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 66% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[=================================&gt;                ] 67% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 68% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[==================================&gt;               ] 69% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 70% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[===================================&gt;              ] 71% ETA: 1s     \n[====================================&gt;             ] 72% ETA: 1s     \n[====================================&gt;             ] 72% ETA: 1s     \n[====================================&gt;             ] 72% ETA: 1s     \n[====================================&gt;             ] 72% ETA: 1s     \n[====================================&gt;             ] 72% ETA: 1s     \n[====================================&gt;             ] 72% ETA: 1s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[====================================&gt;             ] 73% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 74% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[=====================================&gt;            ] 75% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 76% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[======================================&gt;           ] 77% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 78% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[=======================================&gt;          ] 79% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 80% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[========================================&gt;         ] 81% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 82% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[=========================================&gt;        ] 83% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 84% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[==========================================&gt;       ] 85% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 86% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[===========================================&gt;      ] 87% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 88% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[============================================&gt;     ] 89% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 90% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[=============================================&gt;    ] 91% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 92% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[==============================================&gt;   ] 93% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 94% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[===============================================&gt;  ] 95% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 96% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[================================================&gt; ] 97% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 98% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n[=================================================&gt;] 99% ETA: 0s     \n                                                                                \n\n  las@data$Z &lt;- las@data$Z - min(las@data$Z, na.rm = TRUE)  \n  maxZ &lt;- min(floor(max(las@data$Z, na.rm = TRUE)), zmax)  \n  las@data$Z[las@data$Z &gt; maxZ] &lt;- maxZ  \npointsByZSlice = function(Z, maxZ){\n  heightSlices = as.integer(Z) # Round down\n  zSlice = data.table::data.table(Z=Z, heightSlices=heightSlices) # Create a data.table (Z, slices))\n  sliceCount = stats::aggregate(list(V1=Z), list(heightSlices=heightSlices), length) # Count number of returns by slice\n  \n  ##############################################\n  # Add columns to equalize number of columns\n  ##############################################\n  colRange = 0:maxZ\n  addToList = setdiff(colRange, sliceCount$heightSlices)\n  n = length(addToList)\n  if (n &gt; 0) {\n    bindDt = data.frame(heightSlices = addToList, V1=integer(n))\n    sliceCount = rbind(sliceCount, bindDt)\n    # Order by height\n    sliceCount = sliceCount[order(sliceCount$heightSlices),]\n  }\n  \n  colNames = as.character(sliceCount$heightSlices)\n  colNames[1] = \"ground_0_1m\"\n  colNames[-1] = paste0(\"pulses_\", colNames[-1], \"_\", sliceCount$heightSlices[-1]+1, \"m\")\n  metrics = list()\n  metrics[colNames] = sliceCount$V1\n  \n  return(metrics)\n  \n} #end function pointsByZSlice\n\n# --- Main function ---\npreprocess_voxels &lt;- function(normlas, grain.size = 1, maxP =zmax, normalize = TRUE, as_raster = TRUE) {  \n  las &lt;- normlas  \n  \n  # Filter height range\n  las &lt;- filter_poi(las, Z &gt;= 0 & Z &lt;= maxP)  \n  if (lidR::is.empty(las)) return(NULL)\n  # Determine Z-slices\n  maxZ &lt;- floor(max(las@data$Z))  \n  maxZ &lt;- min(maxZ, maxP)  \n  \n  \n  # Compute voxel metrics\n  func &lt;- formula(paste0(\"~pointsByZSlice(Z, \", maxZ, \")\"))  \n  voxels &lt;- pixel_metrics(las, func, res = grain.size)  # Calculate metrics in each voxel (3D grid cell)\n  \n  # Optionally normalize values by voxel volume\n  if (normalize) {\n    vvol &lt;- grain.size^3  \n    voxels &lt;- voxels / vvol  \n  }\n  \n  # Return as both terra::SpatRaster and data.frame\n  result &lt;- list()  \n  \n  if (as_raster) {\n    result$raster &lt;- voxels  \n  }\n  \n  # Convert to data.frame\n  xy &lt;- terra::xyFromCell(voxels, seq_len(ncell(voxels)))  \n  vals &lt;- terra::values(voxels)  \n  df &lt;- cbind(xy, vals)  \n  colnames(df)[1:2] &lt;- c(\"X\", \"Y\")  \n  result$df &lt;- df  \n  \n  return(result)\n}\n\n\n\n\nvox_out &lt;- preprocess_voxels(las, grain.size = 1, maxP = zmax)  \n\n\n\n\n\nConversion to LAD (m²/m³)\nThe conversion to LAD (Leaf Area Density, in m²/m³) from TLS-based voxel pulse counts is done using a relative normalization heuristic which is adopted as a practical approximation in voxel-based canopy structure analysis using TLS (Terrestrial Laser Scanning) data.:\nFor each voxel layer (e.g. pulses_2_3m), the LAD is calculated as:\n\\[\n\\text{LAD}_{\\text{voxel}} = \\left( \\frac{\\text{pulse count in voxel}}{\\text{maximum pulse count over all voxels}} \\right) \\times \\text{LAD}_{\\text{max}}\n\\]\nWhere:\n\npulse count in voxel = number of returns in this voxel layer (from TLS)\nmax_pulse = the maximum pulse count found in any voxel (used for normalization)\nLAD_max = a fixed normalization constant (e.g. 5.0 m²/m³) chosen from literature or calibration\n\n\n\n\n\n\n\nTypical LADₘₐₓ Values by Species\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies / Structure Type\nLADₘₐₓ (m²/m³)\nSource / Notes\n\n\n\n\nFagus sylvatica (European beech)\n3.5–5.5\nCalders et al. (2015), Chen et al. (2018)\n\n\nQuercus robur (English oak)\n3.0–6.0\nHosoi & Omasa (2006), field studies with TLS voxelization\n\n\nConiferous trees (e.g. pine)\n4.0–7.0\nWilkes et al. (2017), higher LAD due to needle density\n\n\nMixed broadleaf forest\n3.0–6.0\nFlynn et al. (2023), canopy averaged estimates\n\n\nShrubs / understorey\n1.5–3.0\nChen et al. (2018),lower vertical structure density\n\n\nUrban street trees\n2.0–4.0\nSimon et al. (2020), depending on pruning and species\n\n\n\nLAD values refer to maximum expected per 1 m vertical voxel. Values depend on species, seasonality, and scanning conditions.\n\n\n\nWhat this means conceptually\nYou’re not measuring absolute LAD, but instead:\n\nUsing the number of TLS returns per voxel as a proxy for leaf density\nThen normalization all voxels relatively to the most “leaf-dense” voxel\nThe LAD_max defines what value the “densest” voxel should reach in terms of LAD\n\nThis is fast, simple, and works well when:\n\nYou want relative structure across the canopy\nYou don’t have absolute calibration (e.g. with destructive sampling or hemispheric photos)\n\nCaveats and assumptions\n\nThis approach assumes the TLS beam returns are proportional to leaf area, which is a simplification\nIt’s sensitive to occlusion and TLS positioning\nThe choice of LAD_max is crucial—common values from literature range from 3–7 m²/m³ for dense canopies\n\nThe LAD conversion in the following code is a relative, normalized mapping of TLS pulse counts to LAD values, normalized by the highest voxel return and normalized using a fixed LAD_max. This gives a plausible LAD field usable for analysis, visualization, or simulation input (e.g. for ENVI-met).\n\n\n\n\n\n\nView Code\n\n\n\n\n\n\nlibrary(terra)\nconvert_matrix_to_df &lt;- function(mat) {  \n  df &lt;- as.data.frame(mat)  \n  colnames(df) &lt;- attr(mat, \"dimnames\")[[2]]  \n  return(df)\n}\n\n# --- Preprocess LiDAR data into voxel metrics -------------------------------\nvox_out &lt;- preprocess_voxels(las, grain.size = 1, maxP = zmax)  # Calculate vertical pulse metrics\nvox_df &lt;- convert_matrix_to_df(vox_out$df)                      # Convert voxel array to data.frame\n\n#' Convert TLS voxel pulse data to LAD using Beer–Lambert conversion conversion with post-normalization\n#'\n#' @param df A data.frame with pulse columns (from TLS voxelization)\n#' @param grainsize Numeric, vertical voxel height (e.g., 1 m)\n#' @param k Extinction coefficient (default: 0.3)\n#' @param scale_factor Optional multiplicative scale factor (default: 1.2)\n#' @param lad_max Optional maximum LAD clamp (e.g. 2.5); set to NULL to disable\n#' @param lad_min Optional minimum LAD threshold (e.g. 0.05); set to NULL to disable\n#' @param keep_pulses Logical, whether to retain pulse columns (default: FALSE)\n#'\n#' @return Data.frame with LAD columns added\n#' @export\nconvert_to_LAD_beer &lt;- function(df,\n                                grainsize = 1,\n                                k = 0.3,\n                                scale_factor = 1.2,\n                                lad_max = 2.5,\n                                lad_min = 0.05,\n                                keep_pulses = FALSE) {\n  df_lad &lt;- df\n  pulse_cols &lt;- grep(\"^pulses_\", names(df_lad), value = TRUE)\n  \n  for (col in pulse_cols) {\n    lad_col &lt;- paste0(\"lad_\", sub(\"pulses_\", \"\", col))\n    p_rel &lt;- df_lad[[col]] / max(df_lad[[col]], na.rm = TRUE)\n    \n    # Avoid log(0) and 1\n    p_rel[p_rel &gt;= 1] &lt;- 0.9999\n    p_rel[p_rel &lt;= 0] &lt;- 1e-5\n    \n    # Apply Beer–Lambert conversion\n    lad_vals &lt;- -log(1 - p_rel) / (k * grainsize)\n    \n    # Apply normalization\n    lad_vals &lt;- lad_vals * scale_factor\n    \n    # Clamp LAD values if needed\n    if (!is.null(lad_max)) {\n      lad_vals &lt;- pmin(lad_vals, lad_max)\n    }\n    if (!is.null(lad_min)) {\n      lad_vals &lt;- pmax(lad_vals, lad_min)\n    }\n    \n    df_lad[[lad_col]] &lt;- lad_vals\n    \n    if (!keep_pulses) {\n      df_lad[[col]] &lt;- NULL\n    }\n  }\n  \n  return(df_lad)\n}\n\n\n#' Convert TLS Pulse Counts to Leaf Area Density (LAD)\n#'\n#' Transforms vertically binned pulse counts (from voxelized TLS data) into Leaf Area Density (LAD, m²/m³)\n#' by normalizing pulse values to a specified LAD maximum.\n#'\n#' @param df A `data.frame` containing voxelized TLS pulse data. Must include columns starting with `\"pulses_\"`, \n#'           each representing pulse returns per vertical layer (e.g. `pulses_1_2m`, `pulses_2_3m`, ...).\n#' @param grainsize Numeric. The voxel edge length in meters (assumed cubic). Default is `1`.\n#' @param LADmax Numeric. The maximum LAD value in m²/m³ for relative normalization. Common values: `4.0`–`6.0`. Default is `5.0`.\n#' @param keep_pulses Logical. If `FALSE` (default), the original pulse columns are removed from the output. If `TRUE`, they are retained alongside the LAD columns.\n#'\n#' @return A modified `data.frame` with new LAD columns (`lad_1_2m`, `lad_2_3m`, ...) in m²/m³, normalized relatively to `LADmax`.\n#'\n#' @details\n#' - Each `pulses_*` column is linearly normalized by the overall maximum value across all vertical bins and locations.\n#' - The result is a relative LAD estimate, useful for ecological modeling, input to microclimate simulations (e.g., ENVI-met), or structural analysis.\n#' - Voxel volume is implicitly considered constant due to cubic assumption (via `grainsize`) but is not explicitly used here.\n#'\n#' @examples\n#' \\dontrun{\n#'   df_vox &lt;- readRDS(\"TLS/voxel_metrics.rds\")\n#'   lad_df &lt;- convert_to_LAD(df_vox, grainsize = 1, LADmax = 5)\n#'   head(names(lad_df))  # Should show lad_* columns\n#' }\n#'\n#' @export\nconvert_to_LAD &lt;- function(df, grainsize = 1, LADmax = 5.0, keep_pulses = FALSE) {  \n  # df: Data frame mit voxelisierten TLS-Daten\n# grainsize: Voxelgröße in m (würfelförmig angenommen)\n# LADmax: maximaler LAD-Wert (Literaturbasiert, z. B. 5.0 m²/m³)\n  df_lad &lt;- df  \n  pulse_cols &lt;- grep(\"^pulses_\", names(df_lad), value = TRUE)  \n  \n  # Schichtanzahl = Anzahl Pulse-Spalten\n  n_layers &lt;- length(pulse_cols)  \n  \n  # Optional: originales Maximum zur linearen Skalierung (relativ)\n  max_pulse &lt;- max(df_lad[, pulse_cols], na.rm = TRUE)  \n  \n  # Umwandlung in LAD (m²/m³) – Skaliert auf LADmax oder absolut (siehe Kommentar)\n  for (col in pulse_cols) {\n    lad_col &lt;- paste0(\"lad_\", sub(\"pulses_\", \"\", col))  \n    \n    # Hier wird RELATIV zu max_pulse skaliert → einfache Normalisierung\n    df_lad[[lad_col]] &lt;- (df_lad[[col]] / max_pulse) * LADmax  \n    \n    # Optional: löschen der Pulse-Spalten\n    if (!keep_pulses) {\n      df_lad[[col]] &lt;- NULL  \n    }\n  }\n  \n  return(df_lad)\n}\n\n\n\n# method selection\nif (lad_method == \"beer\") {\n  message(\"✔ Using Beer–Lambert conversion LAD conversion...\")\n  df_lad &lt;- convert_to_LAD_beer(\n    vox_df,\n    grainsize = 1,\n    k = k_extinction,\n    scale_factor = 0.4,\n    lad_max = 2.5,\n    lad_min = 0.0\n  )\n} else if (lad_method == \"linear\") {\n  message(\"Using linear LAD conversion...\")\n  df_lad &lt;- convert_to_LAD(\n    vox_df,\n    grainsize = 1,\n    LADmax = 5.0\n  )\n} else {\n  stop(\"Unknown LAD conversion method: choose 'linear' or 'beer'\")\n}\n\n\n\n\n\nDT::datatable(head(df_lad, 5))\n\n\n\n\n\n\n\nRaster Stack Representation of 3D Vegetation (Voxel-Based)\nWe represent 3D vegetation using a voxel-based raster stack:\n\nSpace is divided into cubic voxels (e.g. 1 × 1 × 1 m).\nEach raster layer represents a height slice (e.g. 0–1 m, 1–2 m, …).\nVoxels store values like pulse counts or Leaf Area Density (LAD).\n\nThis 2D stack structure enables:\n\nVertical profiling of vegetation per XY column.\nLayer-wise analysis (e.g. median, entropy).\nIntegration with raster data like topography or irradiance.\nUse in raster-based ecological and microclimate models.\n\nIt supports both analysis and visualization of vertical structure with standard geospatial tools.\nENVI-met supports custom vegetation input via the SimplePlant method, which requires a vertical LAD profile per grid column. A raster stack derived from TLS data provides exactly this: each layer represents LAD in a specific height slice, and each XY cell corresponds to one vertical profile. This structure can be exported as CSV, ASCII rasters, or custom profile files.\nFor 3D vegetation parameterization in ENVI-met 5.8+, the raster stack enables preprocessing of spatially explicit LAD or LAI profiles, even if some reformatting is needed.\nThe raster stack also supports canopy clustering and prototyping. It allows classification of structural types, simplification of complex vegetation, and the creation of representative profiles for simulation.\n\n\nVisualization\n\nlibrary(terra)\n# In SpatRasterStack umwandeln\nxy &lt;- df_lad[, c(\"X\", \"Y\")]  \nlad_vals &lt;- df_lad[, grep(\"^lad_\", names(df_lad), value = TRUE)]  \n\nlad_raster &lt;- rast(cbind(xy, lad_vals), type = \"xyz\")  \nplot(lad_raster)\n\n\n\n\n\n\n\n\n\nLAD Profile Visualizations from TLS Data\nThe plot_lad_profiles() function visualizes vertical leaf area density (LAD) profiles derived from voxelized TLS (terrestrial laser scanning) data. LAD represents leaf surface area per unit volume (m²/m³). The function provides three main plot styles:\n\n\n1. XY Matrix Plot (plotstyle = \"each_median\")\n\nDisplays a grid of mini-profiles, each representing a 0.5 × 0.5 m (x/y) ground column.\nWithin each cell, a normalized vertical LAD profile is plotted:\n\nY-axis (height) is normalized from 0 to 1 per column.\nX-axis shows LAD values normalized relative to the global LAD maximum.\n\nUseful for comparing structural patterns across space.\n\n\n\n2. Overall Median Profile (plotstyle = \"all_median\")\n\nAggregates LAD values across all (x/y) locations by height bin.\nProduces a typical vertical profile using the median and smoothed with a moving average.\nHeight is shown in absolute units (e.g. meters).\nCaptures the dominant vertical canopy structure.\n\n\n\n3. Single Profile (plotstyle = \"single_profile\")\n\nExtracts and plots the LAD profile at a specific (x, y) coordinate.\nBoth LAD and height are shown in absolute units.\nPlots the true vertical structure at one location.\n\nThe matrix plot shows multiple vertical LAD profiles arranged in a grid, with each small plot corresponding to a specific spatial location. This allows the vertical vegetation structure to be viewed in relation to its position on the ground. To make the individual profiles comparable, both height and LAD values are normalized within the plot. A reference profile on the side shows the overall median LAD distribution by height, which helps interpret the scale and shape of the individual profiles.\n\n\n\n\n\n\nView Code\n\n\n\n\n\n\n# --- Reshape LAD data to long format ----------------------------------------\n\nlad_df &lt;- as.data.frame(lad_raster, xy = TRUE, na.rm = TRUE)     # Convert raster to data.frame\n\n# 1. Extract LAD columns and XY coordinates\npulse_cols &lt;- grep(\"^lad_\", names(lad_df), value = TRUE)\nxy_cols &lt;- c(\"x\", \"y\")  # Adjust to \"X\", \"Y\" if needed\n\n# 2. Reshape to long format (one row per LAD layer)\nlad_df &lt;- reshape(\n  data = lad_df[, c(xy_cols, pulse_cols)],\n  varying = pulse_cols,\n  v.names = \"LAD\",\n  timevar = \"layer\",\n  times = pulse_cols,\n  direction = \"long\"\n)\n\n# 3. Extract z-layer information from column names\nlad_df$z_low  &lt;- as.numeric(sub(\"lad_(\\\\d+)_.*\", \"\\\\1\", lad_df$layer))  \nlad_df$z_high &lt;- as.numeric(sub(\"lad_\\\\d+_(\\\\d+)m\", \"\\\\1\", lad_df$layer))  \n\n# 4. Compute mid-point height of each voxel layer\nlad_df$Height &lt;- (lad_df$z_low + lad_df$z_high) / 2  \n\n# 5. Round to whole meters to create height classes\nlad_df$Height_bin &lt;- round(lad_df$Height)  \n\n# --- Aggregate median LAD per 0.5 × 0.5 m column ----------------------------\nsetDT(lad_df)  # Use data.table for efficient aggregation\n\nlad_by_column &lt;- lad_df[  \n  , .(LAD_median = median(LAD, na.rm = TRUE)), \n  by = .(x, y, Height_bin)\n]\n\n# Convert back to regular data.frame\nlad_df &lt;- as.data.frame(lad_by_column)\n\nplot_lad_profiles &lt;- function(lad_df, plotstyle = c(\"each_median\", \"all_median\", \"single_profile\"),  \n                              single_coords = c(NA, NA)) {\n  plotstyle &lt;- match.arg(plotstyle)  \n  \n  # Combine x and y coordinates into a unique column ID\n  lad_df$col_id &lt;- paste(lad_df$x, lad_df$y, sep = \"_\")  \n  x_levels &lt;- sort(unique(lad_df$x))  \n  y_levels &lt;- sort(unique(lad_df$y))  \n  # Convert x/y coordinates to factor variables for matrix layout\n  lad_df$x_f &lt;- factor(lad_df$x, levels = x_levels)  \n  lad_df$y_f &lt;- factor(lad_df$y, levels = y_levels)  \n  n_x &lt;- length(x_levels)  \n  n_y &lt;- length(y_levels)  \n  \n  # Determine the maximum LAD value for relative normalization\n  lad_max &lt;- max(lad_df$LAD_median, na.rm = TRUE)  \n  height_range &lt;- range(lad_df$Height_bin, na.rm = TRUE)  \n  dx &lt;- 0.8  \n  dy &lt;- 0.8  \n  \n  par(mar = c(5, 5, 4, 5), xpd = TRUE)\n  \n \n\n  \n  # Differentiate by plot type: all profiles, overall profile, or single profile\n  if (plotstyle == \"each_median\") {\n # Load PNG legend\nlegend_img &lt;- png::readPNG(\"output.png\")\n\n# Define aspect-preserving image placement\nimg_height_units &lt;- 20\nimg_width_units &lt;- img_height_units * dim(legend_img)[2] / dim(legend_img)[1]  # preserve ratio\n\n# Define position\nimg_x_left &lt;- n_x + 1.5\nimg_x_right &lt;- img_x_left + img_width_units\nimg_y_bottom &lt;- 0\nimg_y_top &lt;- img_y_bottom + img_height_units\n\n# Begin plot\nplot(NA, xlim = c(1, n_x + img_width_units + 4), ylim = c(1, n_y),\n     type = \"n\", axes = FALSE, xlab = \"\", ylab = \"\",\n     main = \"Vertical LAD Profiles in XY Matrix\", asp = 1.2)\n\n\n# Draw all LAD profiles\nfor (i in seq_along(x_levels)) {\n  for (j in seq_along(y_levels)) {\n    profile &lt;- subset(lad_df, x == x_levels[i] & y == y_levels[j])\n    if (nrow(profile) == 0) next\n    lad_scaled &lt;- profile$LAD_median / lad_max\n    height_scaled &lt;- (profile$Height_bin - min(height_range)) / diff(height_range)\n    lines(x = lad_scaled * dx + i,\n          y = height_scaled * dy + j,\n          col = \"darkgreen\", lwd = 1)\n  }\n}\n\n# Axis labels for ground position\naxis(1, at = 1:n_x, labels = round(x_levels, 1), las = 2)\naxis(2, at = 1:n_y, labels = round(y_levels, 1), las = 2)\n\n# Add the image\nrasterImage(legend_img,\n            xleft = img_x_left,\n            xright = img_x_right,\n            ybottom = img_y_bottom,\n            ytop = img_y_top)\n\n    \n  } else if (plotstyle == \"all_median\") {\n    unique_heights &lt;- sort(unique(lad_df$Height_bin))  \n    lad_median &lt;- numeric(length(unique_heights))  \n    for (i in seq_along(unique_heights)) {\n      h &lt;- unique_heights[i]  \n      lad_median[i] &lt;- median(lad_df$LAD[lad_df$Height_bin == h], na.rm = TRUE)  \n    }\n    lad_smooth &lt;- stats::filter(lad_median, rep(1/3, 3), sides = 2)  \n    \n    plot(\n      lad_smooth, unique_heights,\n      type = \"l\",\n      col = \"darkgreen\",\n      lwd = 2,\n      xlab = \"Leaf Area Density (m²/m³)\",\n      ylab = \"Height (m)\",\n      main = \"Vertical LAD Profile (smoothed)\",\n      xlim = c(0, max(lad_smooth, na.rm = TRUE)),\n      ylim = range(unique_heights)\n    )\n    \n    text(\n      x = as.numeric(lad_smooth),\n      y = unique_heights,\n      labels = round(as.numeric(lad_smooth), 1),\n      pos = 4,\n      cex = 0.7,\n      col = \"black\"\n    )\n    grid()\n    \n    \n  } else if (plotstyle == \"single_profile\") {\n    x_target &lt;- single_coords[1]  \n    y_target &lt;- single_coords[2]  \n    tol &lt;- 1e-6  \n    \n    profile &lt;- subset(lad_df, abs(x - x_target) &lt; tol & abs(y - y_target) &lt; tol)  \n    \n    if (nrow(profile) == 0) {\n      # Show warning if no profile exists for selected coordinates\n      warning(\"No data for the selected coordinates.\")\n      plot.new()\n      title(main = paste(\"No profile at\", x_target, \"/\", y_target))\n      return(invisible(NULL))\n    }\n    \n    # Normalize height and LAD\n    height_range &lt;- range(profile$Height_bin, na.rm = TRUE)  \n    # Determine the maximum LAD value for relative normalization\n    lad_max &lt;- max(profile$LAD_median, na.rm = TRUE)  \n    \n    height_scaled &lt;- (profile$Height_bin - min(height_range)) / diff(height_range)  \n    height_unscaled &lt;- profile$Height_bin\n    # Determine the maximum LAD value for relative normalization\n    lad_scaled &lt;- profile$LAD_median / lad_max  \n    \n    plot(\n      x = lad_scaled,\n      y = height_unscaled, #height_scaled,\n      type = \"l\",\n      lwd = 2,\n      col = \"darkgreen\",\n      xlab = \"LAD (normalized)\",\n      ylab = \"Height (m)\",\n      main = paste(\"Profile at\", x_target, \"/\", y_target)\n    )\n  }\n}\n# --- Visualize LAD profiles -------------------------------------------------\n\n\n\n\n\n\n\nPlot the profiles\n\n# Option 1: Profile in each column\nplot_lad_profiles(lad_df, plotstyle = \"each_median\")\n\n\n\n\n\n\n\n# Option 2: Overall vertical LAD profile (median of all)\nplot_lad_profiles(lad_df, plotstyle = \"all_median\")\n\n\n\n\n\n\n\n# Option 3: Single profile at specified coordinates\nplot_lad_profiles(lad_df, plotstyle = \"single_profile\", single_coords = c(57.5, -94.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Pipe\n\n\n\n\n\n# Example: Run pipeline if script is sourced interactively\nif (interactive()) {\n  message(\"Running full voxelization pipeline...\")\n  las &lt;- lidR::readLAS(\"data/ALS/merged_output.las\")\n  las@data$Z &lt;- las@data$Z - min(las@data$Z, na.rm = TRUE)\n  zmax &lt;- 30  # example value, ensure it's defined\n  maxZ &lt;- min(floor(max(las@data$Z, na.rm = TRUE)), zmax)\n  las@data$Z[las@data$Z &gt; maxZ] &lt;- maxZ\n  \n  vox_out &lt;- preprocess_voxels(las, grain.size = 1, maxP = zmax)\n  message(\"Done.\")\n}\n\n\n\n\n\n\nView Code\n\n\n\n\n\n\nENVI-met 3D Tree Export\nThe next section describes more detailed how the key input values in the R function export_lad_to_envimet3d() are computed, derived or selected, and provides the rationale for each. The function converts a voxel-based Leaf Area Density (LAD) profile, typically obtained from Terrestrial Laser Scanning (TLS) data, into a structured XML file compatible with ENVI-met’s 3D tree model (.pld or PLANT3D).\nGiven the sensitivity of ENVI-met simulations to tree morphology and LAD distribution, the function ensures that the spatial dimensions, vertical layering and LAD intensity values are all correctly represented. Some parameters are optional, but can be derived from the data if not explicitly set.\nThe table below details each argument of the function, including its purpose, how it is determined and its necessity.\n\n\n\n\n\n\n\n\nCode Line\nMeaning\nReason\n\n\n\n\nlad_df &lt;-lad_df[!is.na(lad_df$LAD_median), ]\nRemoves entries with missing LAD values\nEnsures only valid data is used in the LAD calculation and XML export\n\n\nlad_df$i &lt;-as.integer(factor(lad_df$x))\nConverts x-coordinates to integer voxel column indices (i)\nRequired for ENVI-met LAD matrix indexing\n\n\nlad_df$j &lt;-as.integer(factor(lad_df$y))\nConverts y-coordinates to integer voxel row indices (j)\nSame as above, for the y-direction\n\n\nz_map &lt;-setNames( ...)\nMaps unique height bins to sequential vertical indices (k)\nTranslates height levels into voxel layers compatible with ENVI-met\n\n\nlad_df$k &lt;-z_map[as.character(lad_df$Height_bin)]\nApplies the vertical index to the LAD data\nAligns LAD values with ENVI-met vertical layer system\n\n\nlad_df$lad_value &lt;-round(lad_df$LAD_median * scale_factor, 5)\nScales LAD values and rounds to 5 digits\nBrings LAD values to a usable range for ENVI-met and ensures precision\n\n\ndataI &lt;-max(lad_df$i)\nGets the number of horizontal grid cells in i-direction (width)\nRequired as matrix size input for ENVI-met\n\n\ndataJ &lt;-max(lad_df$j)\nGets the number of horizontal grid cells in j-direction (depth)\nRequired as matrix size input for ENVI-met\n\n\nzlayers &lt;-max(lad_df$k)\nGets the number of vertical layers\nSets the height resolution of the LAD matrix\n\n\n\n\n\n\nAutomatic Grid Dimensions transformation\nCalculates the voxel grid dimensions in X, Y, and Z from the TLS-derived LAD profile.\nThe table below outlines how the core spatial and structural parameters of the tree model are computed from the input LAD_DF data frame. These derived values define the three-dimensional structure of the tree in terms of its horizontal extent, vertical layering and canopy dimensions.\nData I and data J represent the size of the voxel grid in the i and j dimensions, respectively, based on unique horizontal (x and y) and vertical (height bin) bins in the LAD profile.\n‘Width’ and ‘Depth’ describe the physical spread of the tree crown, inferred from the voxel grid extent if not manually set.\nHeight is computed by multiplying the number of vertical layers (zlayers) by the voxel resolution (cellSize), providing the total modelled height of the canopy.\nThese computed values are essential for correctly normalization and locating the 3D LAD matrix within the ENVI-met simulation domain to ensure visual and physiological realism.\n\n\n\n\n\n\n\n\nCode Line\nMeaning\nReason\n\n\n\n\nWidth  &lt;- if (is.null(Width)) dataI else Width\nUses the number of i-cells if Width is not provided\nAutomatically estimates tree width from voxel spread in x-direction\n\n\nDepth  &lt;- if (is.null(Depth)) dataJ else Depth\nUses the number of j-cells if Depth is not provided\nAutomatically estimates tree depth from voxel spread in y-direction\n\n\nHeight &lt;- zlayers * cellsize\nConverts number of vertical layers to metric height using cellsize\nComputes physical tree height in meters for ENVI-met\n\n\n\n\n# 1. Remove NA values from the LAD column\nlad_df &lt;- lad_df[!is.na(lad_df$LAD_median), ]\n\n# 2. Create discrete i and j indices for the horizontal position\n# (converts x and y coordinates into consecutive index values)\nlad_df$i &lt;- as.integer(factor(lad_df$x))\nlad_df$j &lt;- as.integer(factor(lad_df$y))\n\n# 3. Assign each Height_bin (z direction) a consecutive layer ID k\n# (z_map assigns an index layer to each unique height)\nz_map &lt;- setNames(seq_along(sort(unique(lad_df$Height_bin))), sort(unique(lad_df$Height_bin)))\nlad_df$k &lt;- z_map[as.character(lad_df$Height_bin)]\n\n# 4. Scale LAD values, e.g. to get from 0.02 to more realistic values such as 0.5–1.5\nlad_df$lad_value &lt;- round(lad_df$LAD_median * 1.2, 5)\n\n# 5. Calculate the maximum dimensions of the grid (for XML specifications)\ndataI &lt;- max(lad_df$i) # Width in cells (x-direction)\ndataJ &lt;- max(lad_df$j) # Depth in cells (y-direction)\nzlayers &lt;- max(lad_df$k) # Number of vertical layers (z-direction)\n\n\n\nTransmittance and Albedo\n\nAlbedo = 0.18\nTransmittance = 0.3\n\nAlbedo = 0.18: Albedo is the fraction of incoming solar radiation reflected by the canopy surface. For deciduous trees, values usually range between 0.15 and 0.20. 0.18 is a commonly used default for broadleaved species like Fagus sylvatica or Quercus robur in many ecological models (e.g., ENVI-met, MAESPA). It affects surface energy balance and radiation reflection in ENVI-met simulations.\nTransmittance = 0.3: Transmittance represents the proportion of shortwave radiation that passes through the canopy without being absorbed or reflected. Deciduous trees in full leaf have transmittance values between 0.1 and 0.4 depending on species and LAI. 0.3 reflects moderate canopy density, consistent with empirical observations for mid-summer crowns. It controls how much light reaches the ground and sub-canopy vegetation; affects microclimate and shading.\nBoth values can be adjusted to match field measurements or literature for specific species or leaf phenology. However you can use them as robust fallback defaults when exact species traits are unavailable.\n\n\nSeason-Profile\nDefines monthly LAD normalization.\nSeasonProfile = c(0.2, 0.2, 0.4, 0.7, 1.0, 1.0, 1.0, 0.8, 0.6, 0.3, 0.2, 0.2)\nThe SeasonProfile is a vector of 12 numeric values (one per month) weighting the relative Leaf Area Density (LAD) throughout the year. It models seasonal leaf development and senescence, controlling how much foliage is present in each month:\n\nValues range from 0.0 (no foliage) to 1.0 (full foliage).\nFor deciduous trees like Fagus sylvatica or Quercus robur, foliage develops in spring (April–May), peaks in summer (June–August), and declines in autumn (September–October).\n\nProfile Breakdown:\n\n\n\nMonths\nValue\nInterpretation\n\n\n\n\nJan–Feb, Nov–Dec\n0.2\nDormant / leafless\n\n\nMarch\n0.4\nBudburst begins\n\n\nApril\n0.7\nLeaf expansion\n\n\nMay–July\n1.0\nFull canopy\n\n\nAugust\n0.8\nLeaf maturity decline\n\n\nSeptember\n0.6\nSenescence onset\n\n\nOctober\n0.3\nStrong senescence\n\n\n\nThe SeasonProfile directly influences LAD in ENVI-met’s dynamic vegetation simulation — affecting transpiration, shading, and energy balance across the simulation year. Adjusting this vector allows tailoring of phenology to site-specific or species-specific data.\n\n\nL-SystemBased trees in ENVI-met (Experimetal)\nENVI-met optionally allows procedural generation of tree architecture using Lindenmayer Systems (L-Systems) — a formal grammar originally used to simulate plant growth patterns. When L-SystemBased = 1, the geometry of the tree is not derived from a static LAD matrix alone, but supplemented or replaced by rule-based 3D branching structures which supplement or replace the matrix. This is independent of the LAD profile but may affect shading and visualisation in the Albero interface of ENVI-met.\nL-SystemBased = 1\nAxiom = \"F(2)V\\V\\\\V/////B\"\nIterationDepth = 3\n\nExplanation of Key Parameters\n\n\n\n\n\n\n\nParameter\nMeaning\n\n\n\n\nL-SystemBased\nIf 1, enables L-system generation (uses rules to grow plant structure)\n\n\nAxiom\nStarting string (“seed”) for the L-system; defines base growth\n\n\nIterationDepth\nHow many times to apply production rules; higher means more detail\n\n\nTermLString\nOptional: Final symbol to be drawn/rendered (e.g. “L”)\n\n\nApplyTermLString\nIf 1, interprets the TermLString; otherwise, renders entire string\n\n\n\n\n\nDefault Settings\n\n\n\nL-System Branching as implemented by default\n\n\n&lt;L-SystemBased&gt;1&lt;/L-SystemBased&gt;\n&lt;Axiom&gt;F(2)V\\V\\\\V/////B&lt;/Axiom&gt;\n&lt;IterationDepth&gt;3&lt;/IterationDepth&gt;\n&lt;TermLString&gt;L&lt;/TermLString&gt;\n&lt;ApplyTermLString&gt;1&lt;/ApplyTermLString&gt;\n\nF(2): Move forward with length 2 (main trunk)\nV\\\\V/////B: Branching pattern with rotations (backslashes and slashes encode rotation commands); B may denote a terminal leaf or bud\nIterationDepth = 3: The production rules (if defined) will be applied 3 times to this axiom, generating a fractal-like tree structure.\n\n\nNote: In ENVI-met, the actual grammar rules are hard-coded and not customizable in .pld — only the axiom and iteration depth are user-defined. It is highly experimental and poorly documented\n\nUse L-SystemBased = 1 if:\n\nYou want visual structure added to otherwise sparse or low-resolution LAD matrices\nThe tree lacks realistic shape (for Albero visualization)\nUse L-SystemBased = 0 (default) if:\n\nYou already provide a dense voxel-based LAD (from TLS or similar)\nYou want strict control over the 3D structure via LAD profile only\n\n\n#| eval: false\n\n# --- Export final profile as Envi-met PLANT3D tree --------------------------\nexport_lad_to_envimet_p3d(\n  lad_df = lad_df,\n  ID = \"120312\",\n  Description = \"Fagus sylvatica TLS\",\n  AlternativeName = \"Fagus sylvatica\",\n  Albedo = 0.17,\n  Width = NULL,              # auto-detected\n  Depth = NULL,              # auto-detected\n  RootDiameter = 5.0,\n  cellsize = 1,\n  Transmittance = 0.3,\n  SeasonProfile = c(0.3, 0.3, 0.4, 0.6, 0.9, 1, 1, 1, 0.7, 0.4, 0.3, 0.3),\n  BlossomProfile = c(0, 0, 0.7, 0.1, 0, 0, 0, 0, 0, 0, 0, 0),\n  LSystem = TRUE,\n  scale_factor = 3,\n  file_out = output_envimet_tls_3d\n)\n\n#rstudioapi::navigateToFile(output_envimet_tls_3d)\n\n\nImport TLS-based .pld into ENVI-met via Albero Clipboard\nRequirements\n- ENVI-met 5.8+\n- .pld file (e.g. oak_tls_envimet.pld)\n- Albero editor (via Leonardo)\nSteps\n1. Open Albero\n→ Leonardo → Database → Plant Database\n2. Open Clipboard\n→ Click Clipboard (top-right)\n3. Import .pld\n→ Clipboard → Import → Load file\n4. Edit (optional)\n→ Adjust LAD, albedo, transmittance, name, etc.\n5. Send to Library\n→ Click “Send to Library”\n6. Use in ENVI-met\n→ In Leonardo/Spaces assign plant to your 3D model\nNotes\n- .pld contains LAD(z) values (m²/m³)\n- Use Advanced Settings to fine-tune visualization\n- Custom plants stored in your personal Albero library\n\n\n\nKey Benefits\n\nEfficient and scalable: The method avoids destructive sampling by using TLS return counts as proxies for leaf density. This makes it suitable for large-scale or repeated surveys without the need for time-consuming ground calibration.\nCaptures structural patterns: Normalizing the LAD values retains the vertical and spatial structure of vegetation, enabling meaningful comparison of crown shape, canopy layering, and vegetation density across space or time.\nDirectly usable in ENVI-met: The output is structured as a raster stack with height-specific layers, aligning with the input requirements of ENVI-met’s SimplePlant or 3D vegetation modules. This enables seamless integration into microclimate simulations.\n\n\n\nLimitations\n\nSimplified assumptions: The linear mapping of TLS returns to LAD assumes a proportional relationship, which simplifies the complex interaction between laser pulses and vegetation surfaces.\nScan geometry dependency: Occlusion, scan angle, and varying point densities can distort the return distribution, especially in dense or multi-layered vegetation.\nGeneric LAD normalization: The maximum LAD value used for normalization is taken from literature-based estimates rather than site-specific measurements, which can introduce bias in absolute LAD magnitudes.\n\n\n\nConclusion\nThis workflow offers a robust and accessible approach for analyzing vegetation structure and generating model-ready LAD profiles from TLS data. It is especially useful for relative comparisons and ecological modeling, but is not intended for absolute LAD quantification without additional calibration.\n\n\nReferences\n\nCalders et al. (2015). Nondestructive biomass estimation via TLS. Methods Ecol Evol, 6:198–208.https://doi.org/10.1111/2041-210X.12301\nChen et al. (2018): Estimation of LAI in open-canopy forests using TLS and path length models. Agric. For. Meteorol. 263, 323–333. https://doi.org/10.1016/j.agrformet.2018.09.006\nENVI-met PLANT3D specification: https://www.envi-met.net/documents/papers/overview30.pdf\nENVI-met Albero overview: https://envi-met.com/tutorials/albero-overview\nENVI-met KB – Obtaining Leaf Area Density: https://envi-met.info/doku.php?id=kb:lad#obtaining_leaf_area_density_data\nENVI-met dbmanager documentation: https://envi-met.info/doku.php?id=apps:dbmanager:start\nENVI-met Vegetation Tutorial (YouTube): https://www.youtube.com/watch?v=KGRLnXAXZds\nFlynn et al. (2023) – TLS-based vegetation index estimation; compares methods and highlights complexities in Mediterranean forest. Biogeosciences, 20(13), 2769–2784. doi:10.5194/bg-20-2769-2023\nHosoi & Omasa (2006). Voxel-based 3D tree modeling. IEEE TGRS, 44(12), 3610–3618. https://doi.org/10.1109/TGRS.2006.881743\nPrusinkiewicz & Lindenmayer (1990). The Algorithmic Beauty of Plants. Springer. https://doi.org/10.1007/978-1-4613-8476-2\nOshio & Asawa (2016). Solar transmittance of urban trees. IEEE TGRS, 54(9), 5483–5492. https://doi.org/10.1109/TGRS.2016.2565699\nSimon, Sinsel & Bruse (2020). Fractal trees in ENVI-met. Forests, 11(8), 869. https://doi.org/10.3390/f11080869\nWilkes et al. (2017). TLS acquisition strategies. Remote Sens Environ, 196, 140–153. https://doi.org/10.1016/j.rse.2017.04.030\nChen et al. (2018). LAI from TLS. Agr Forest Meteorol, 263, 323–333. https://doi.org/10.1016/j.agrformet.2018.09.006\nYin et al. (2019). Shading and thermal comfort. Sustainability, 11(5), 1355. https://doi.org/10.3390/su11051355\nZhang (2024). Green layouts in ENVI-met. Informatica, 48(23). https://doi.org/10.31449/inf.v48i23.6881 Certainly. Here’s the reference adapted to match your current compact style:",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#envi-met-3d-tree-export",
    "href": "doc/tls_v1_2.html#envi-met-3d-tree-export",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "ENVI-met 3D Tree Export",
    "text": "ENVI-met 3D Tree Export\nThe next section describes more detailed how the key input values in the R function export_lad_to_envimet3d() are computed, derived or selected, and provides the rationale for each. The function converts a voxel-based Leaf Area Density (LAD) profile, typically obtained from Terrestrial Laser Scanning (TLS) data, into a structured XML file compatible with ENVI-met’s 3D tree model (.pld or PLANT3D).\nGiven the sensitivity of ENVI-met simulations to tree morphology and LAD distribution, the function ensures that the spatial dimensions, vertical layering and LAD intensity values are all correctly represented. Some parameters are optional, but can be derived from the data if not explicitly set.\nThe table below details each argument of the function, including its purpose, how it is determined and its necessity.\n\n\n\n\n\n\n\n\nCode Line\nMeaning\nReason\n\n\n\n\nlad_df &lt;-lad_df[!is.na(lad_df$LAD_median), ]\nRemoves entries with missing LAD values\nEnsures only valid data is used in the LAD calculation and XML export\n\n\nlad_df$i &lt;-as.integer(factor(lad_df$x))\nConverts x-coordinates to integer voxel column indices (i)\nRequired for ENVI-met LAD matrix indexing\n\n\nlad_df$j &lt;-as.integer(factor(lad_df$y))\nConverts y-coordinates to integer voxel row indices (j)\nSame as above, for the y-direction\n\n\nz_map &lt;-setNames( ...)\nMaps unique height bins to sequential vertical indices (k)\nTranslates height levels into voxel layers compatible with ENVI-met\n\n\nlad_df$k &lt;-z_map[as.character(lad_df$Height_bin)]\nApplies the vertical index to the LAD data\nAligns LAD values with ENVI-met vertical layer system\n\n\nlad_df$lad_value &lt;-round(lad_df$LAD_median * scale_factor, 5)\nScales LAD values and rounds to 5 digits\nBrings LAD values to a usable range for ENVI-met and ensures precision\n\n\ndataI &lt;-max(lad_df$i)\nGets the number of horizontal grid cells in i-direction (width)\nRequired as matrix size input for ENVI-met\n\n\ndataJ &lt;-max(lad_df$j)\nGets the number of horizontal grid cells in j-direction (depth)\nRequired as matrix size input for ENVI-met\n\n\nzlayers &lt;-max(lad_df$k)\nGets the number of vertical layers\nSets the height resolution of the LAD matrix",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#automatic-grid-dimensions-transformation",
    "href": "doc/tls_v1_2.html#automatic-grid-dimensions-transformation",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Automatic Grid Dimensions transformation",
    "text": "Automatic Grid Dimensions transformation\nCalculates the voxel grid dimensions in X, Y, and Z from the TLS-derived LAD profile.\nThe table below outlines how the core spatial and structural parameters of the tree model are computed from the input LAD_DF data frame. These derived values define the three-dimensional structure of the tree in terms of its horizontal extent, vertical layering and canopy dimensions.\nData I and data J represent the size of the voxel grid in the i and j dimensions, respectively, based on unique horizontal (x and y) and vertical (height bin) bins in the LAD profile.\n‘Width’ and ‘Depth’ describe the physical spread of the tree crown, inferred from the voxel grid extent if not manually set.\nHeight is computed by multiplying the number of vertical layers (zlayers) by the voxel resolution (cellSize), providing the total modelled height of the canopy.\nThese computed values are essential for correctly normalization and locating the 3D LAD matrix within the ENVI-met simulation domain to ensure visual and physiological realism.\n\n\n\n\n\n\n\n\nCode Line\nMeaning\nReason\n\n\n\n\nWidth  &lt;- if (is.null(Width)) dataI else Width\nUses the number of i-cells if Width is not provided\nAutomatically estimates tree width from voxel spread in x-direction\n\n\nDepth  &lt;- if (is.null(Depth)) dataJ else Depth\nUses the number of j-cells if Depth is not provided\nAutomatically estimates tree depth from voxel spread in y-direction\n\n\nHeight &lt;- zlayers * cellsize\nConverts number of vertical layers to metric height using cellsize\nComputes physical tree height in meters for ENVI-met\n\n\n\n\n# 1. Remove NA values from the LAD column\nlad_df &lt;- lad_df[!is.na(lad_df$LAD_median), ]\n\n# 2. Create discrete i and j indices for the horizontal position\n# (converts x and y coordinates into consecutive index values)\nlad_df$i &lt;- as.integer(factor(lad_df$x))\nlad_df$j &lt;- as.integer(factor(lad_df$y))\n\n# 3. Assign each Height_bin (z direction) a consecutive layer ID k\n# (z_map assigns an index layer to each unique height)\nz_map &lt;- setNames(seq_along(sort(unique(lad_df$Height_bin))), sort(unique(lad_df$Height_bin)))\nlad_df$k &lt;- z_map[as.character(lad_df$Height_bin)]\n\n# 4. Scale LAD values, e.g. to get from 0.02 to more realistic values such as 0.5–1.5\nlad_df$lad_value &lt;- round(lad_df$LAD_median * 1.2, 5)\n\n# 5. Calculate the maximum dimensions of the grid (for XML specifications)\ndataI &lt;- max(lad_df$i) # Width in cells (x-direction)\ndataJ &lt;- max(lad_df$j) # Depth in cells (y-direction)\nzlayers &lt;- max(lad_df$k) # Number of vertical layers (z-direction)",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#transmittance-and-albedo",
    "href": "doc/tls_v1_2.html#transmittance-and-albedo",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Transmittance and Albedo",
    "text": "Transmittance and Albedo\n\nAlbedo = 0.18\nTransmittance = 0.3\n\nAlbedo = 0.18: Albedo is the fraction of incoming solar radiation reflected by the canopy surface. For deciduous trees, values usually range between 0.15 and 0.20. 0.18 is a commonly used default for broadleaved species like Fagus sylvatica or Quercus robur in many ecological models (e.g., ENVI-met, MAESPA). It affects surface energy balance and radiation reflection in ENVI-met simulations.\nTransmittance = 0.3: Transmittance represents the proportion of shortwave radiation that passes through the canopy without being absorbed or reflected. Deciduous trees in full leaf have transmittance values between 0.1 and 0.4 depending on species and LAI. 0.3 reflects moderate canopy density, consistent with empirical observations for mid-summer crowns. It controls how much light reaches the ground and sub-canopy vegetation; affects microclimate and shading.\nBoth values can be adjusted to match field measurements or literature for specific species or leaf phenology. However you can use them as robust fallback defaults when exact species traits are unavailable.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#season-profile",
    "href": "doc/tls_v1_2.html#season-profile",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Season-Profile",
    "text": "Season-Profile\nDefines monthly LAD normalization.\nSeasonProfile = c(0.2, 0.2, 0.4, 0.7, 1.0, 1.0, 1.0, 0.8, 0.6, 0.3, 0.2, 0.2)\nThe SeasonProfile is a vector of 12 numeric values (one per month) weighting the relative Leaf Area Density (LAD) throughout the year. It models seasonal leaf development and senescence, controlling how much foliage is present in each month:\n\nValues range from 0.0 (no foliage) to 1.0 (full foliage).\nFor deciduous trees like Fagus sylvatica or Quercus robur, foliage develops in spring (April–May), peaks in summer (June–August), and declines in autumn (September–October).\n\nProfile Breakdown:\n\n\n\nMonths\nValue\nInterpretation\n\n\n\n\nJan–Feb, Nov–Dec\n0.2\nDormant / leafless\n\n\nMarch\n0.4\nBudburst begins\n\n\nApril\n0.7\nLeaf expansion\n\n\nMay–July\n1.0\nFull canopy\n\n\nAugust\n0.8\nLeaf maturity decline\n\n\nSeptember\n0.6\nSenescence onset\n\n\nOctober\n0.3\nStrong senescence\n\n\n\nThe SeasonProfile directly influences LAD in ENVI-met’s dynamic vegetation simulation — affecting transpiration, shading, and energy balance across the simulation year. Adjusting this vector allows tailoring of phenology to site-specific or species-specific data.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#l-systembased-trees-in-envi-met-experimetal",
    "href": "doc/tls_v1_2.html#l-systembased-trees-in-envi-met-experimetal",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "L-SystemBased trees in ENVI-met (Experimetal)",
    "text": "L-SystemBased trees in ENVI-met (Experimetal)\nENVI-met optionally allows procedural generation of tree architecture using Lindenmayer Systems (L-Systems) — a formal grammar originally used to simulate plant growth patterns. When L-SystemBased = 1, the geometry of the tree is not derived from a static LAD matrix alone, but supplemented or replaced by rule-based 3D branching structures which supplement or replace the matrix. This is independent of the LAD profile but may affect shading and visualisation in the Albero interface of ENVI-met.\nL-SystemBased = 1\nAxiom = \"F(2)V\\V\\\\V/////B\"\nIterationDepth = 3\n\nExplanation of Key Parameters\n\n\n\n\n\n\n\nParameter\nMeaning\n\n\n\n\nL-SystemBased\nIf 1, enables L-system generation (uses rules to grow plant structure)\n\n\nAxiom\nStarting string (“seed”) for the L-system; defines base growth\n\n\nIterationDepth\nHow many times to apply production rules; higher means more detail\n\n\nTermLString\nOptional: Final symbol to be drawn/rendered (e.g. “L”)\n\n\nApplyTermLString\nIf 1, interprets the TermLString; otherwise, renders entire string\n\n\n\n\n\nDefault Settings\n\n\n\nL-System Branching as implemented by default\n\n\n&lt;L-SystemBased&gt;1&lt;/L-SystemBased&gt;\n&lt;Axiom&gt;F(2)V\\V\\\\V/////B&lt;/Axiom&gt;\n&lt;IterationDepth&gt;3&lt;/IterationDepth&gt;\n&lt;TermLString&gt;L&lt;/TermLString&gt;\n&lt;ApplyTermLString&gt;1&lt;/ApplyTermLString&gt;\n\nF(2): Move forward with length 2 (main trunk)\nV\\\\V/////B: Branching pattern with rotations (backslashes and slashes encode rotation commands); B may denote a terminal leaf or bud\nIterationDepth = 3: The production rules (if defined) will be applied 3 times to this axiom, generating a fractal-like tree structure.\n\n\nNote: In ENVI-met, the actual grammar rules are hard-coded and not customizable in .pld — only the axiom and iteration depth are user-defined. It is highly experimental and poorly documented\n\nUse L-SystemBased = 1 if:\n\nYou want visual structure added to otherwise sparse or low-resolution LAD matrices\nThe tree lacks realistic shape (for Albero visualization)\nUse L-SystemBased = 0 (default) if:\n\nYou already provide a dense voxel-based LAD (from TLS or similar)\nYou want strict control over the 3D structure via LAD profile only\n\n\n#| eval: false\n\n# --- Export final profile as Envi-met PLANT3D tree --------------------------\nexport_lad_to_envimet_p3d(\n  lad_df = lad_df,\n  ID = \"120312\",\n  Description = \"Fagus sylvatica TLS\",\n  AlternativeName = \"Fagus sylvatica\",\n  Albedo = 0.17,\n  Width = NULL,              # auto-detected\n  Depth = NULL,              # auto-detected\n  RootDiameter = 5.0,\n  cellsize = 1,\n  Transmittance = 0.3,\n  SeasonProfile = c(0.3, 0.3, 0.4, 0.6, 0.9, 1, 1, 1, 0.7, 0.4, 0.3, 0.3),\n  BlossomProfile = c(0, 0, 0.7, 0.1, 0, 0, 0, 0, 0, 0, 0, 0),\n  LSystem = TRUE,\n  scale_factor = 3,\n  file_out = output_envimet_tls_3d\n)\n\n#rstudioapi::navigateToFile(output_envimet_tls_3d)\n\n\nImport TLS-based .pld into ENVI-met via Albero Clipboard\nRequirements\n- ENVI-met 5.8+\n- .pld file (e.g. oak_tls_envimet.pld)\n- Albero editor (via Leonardo)\nSteps\n1. Open Albero\n→ Leonardo → Database → Plant Database\n2. Open Clipboard\n→ Click Clipboard (top-right)\n3. Import .pld\n→ Clipboard → Import → Load file\n4. Edit (optional)\n→ Adjust LAD, albedo, transmittance, name, etc.\n5. Send to Library\n→ Click “Send to Library”\n6. Use in ENVI-met\n→ In Leonardo/Spaces assign plant to your 3D model\nNotes\n- .pld contains LAD(z) values (m²/m³)\n- Use Advanced Settings to fine-tune visualization\n- Custom plants stored in your personal Albero library",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#key-benefits",
    "href": "doc/tls_v1_2.html#key-benefits",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Key Benefits",
    "text": "Key Benefits\n\nEfficient and scalable: The method avoids destructive sampling by using TLS return counts as proxies for leaf density. This makes it suitable for large-scale or repeated surveys without the need for time-consuming ground calibration.\nCaptures structural patterns: Normalizing the LAD values retains the vertical and spatial structure of vegetation, enabling meaningful comparison of crown shape, canopy layering, and vegetation density across space or time.\nDirectly usable in ENVI-met: The output is structured as a raster stack with height-specific layers, aligning with the input requirements of ENVI-met’s SimplePlant or 3D vegetation modules. This enables seamless integration into microclimate simulations.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#limitations",
    "href": "doc/tls_v1_2.html#limitations",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Limitations",
    "text": "Limitations\n\nSimplified assumptions: The linear mapping of TLS returns to LAD assumes a proportional relationship, which simplifies the complex interaction between laser pulses and vegetation surfaces.\nScan geometry dependency: Occlusion, scan angle, and varying point densities can distort the return distribution, especially in dense or multi-layered vegetation.\nGeneric LAD normalization: The maximum LAD value used for normalization is taken from literature-based estimates rather than site-specific measurements, which can introduce bias in absolute LAD magnitudes.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#conclusion",
    "href": "doc/tls_v1_2.html#conclusion",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "Conclusion",
    "text": "Conclusion\nThis workflow offers a robust and accessible approach for analyzing vegetation structure and generating model-ready LAD profiles from TLS data. It is especially useful for relative comparisons and ecological modeling, but is not intended for absolute LAD quantification without additional calibration.",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/tls_v1_2.html#references",
    "href": "doc/tls_v1_2.html#references",
    "title": "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants",
    "section": "References",
    "text": "References\n\nCalders et al. (2015). Nondestructive biomass estimation via TLS. Methods Ecol Evol, 6:198–208.https://doi.org/10.1111/2041-210X.12301\nChen et al. (2018): Estimation of LAI in open-canopy forests using TLS and path length models. Agric. For. Meteorol. 263, 323–333. https://doi.org/10.1016/j.agrformet.2018.09.006\nENVI-met PLANT3D specification: https://www.envi-met.net/documents/papers/overview30.pdf\nENVI-met Albero overview: https://envi-met.com/tutorials/albero-overview\nENVI-met KB – Obtaining Leaf Area Density: https://envi-met.info/doku.php?id=kb:lad#obtaining_leaf_area_density_data\nENVI-met dbmanager documentation: https://envi-met.info/doku.php?id=apps:dbmanager:start\nENVI-met Vegetation Tutorial (YouTube): https://www.youtube.com/watch?v=KGRLnXAXZds\nFlynn et al. (2023) – TLS-based vegetation index estimation; compares methods and highlights complexities in Mediterranean forest. Biogeosciences, 20(13), 2769–2784. doi:10.5194/bg-20-2769-2023\nHosoi & Omasa (2006). Voxel-based 3D tree modeling. IEEE TGRS, 44(12), 3610–3618. https://doi.org/10.1109/TGRS.2006.881743\nPrusinkiewicz & Lindenmayer (1990). The Algorithmic Beauty of Plants. Springer. https://doi.org/10.1007/978-1-4613-8476-2\nOshio & Asawa (2016). Solar transmittance of urban trees. IEEE TGRS, 54(9), 5483–5492. https://doi.org/10.1109/TGRS.2016.2565699\nSimon, Sinsel & Bruse (2020). Fractal trees in ENVI-met. Forests, 11(8), 869. https://doi.org/10.3390/f11080869\nWilkes et al. (2017). TLS acquisition strategies. Remote Sens Environ, 196, 140–153. https://doi.org/10.1016/j.rse.2017.04.030\nChen et al. (2018). LAI from TLS. Agr Forest Meteorol, 263, 323–333. https://doi.org/10.1016/j.agrformet.2018.09.006\nYin et al. (2019). Shading and thermal comfort. Sustainability, 11(5), 1355. https://doi.org/10.3390/su11051355\nZhang (2024). Green layouts in ENVI-met. Informatica, 48(23). https://doi.org/10.31449/inf.v48i23.6881 Certainly. Here’s the reference adapted to match your current compact style:",
    "crumbs": [
      "Using leaf area density (LAD) from TLS data in ENVI-met for 3D plants"
    ]
  },
  {
    "objectID": "doc/helper_functions.html",
    "href": "doc/helper_functions.html",
    "title": "Helper Functions for Microclimate Predictor Stack",
    "section": "",
    "text": "1 Introduction\nThis document explains the custom helper functions used in the microclimate_predictor_stack.R script for preprocessing and analyzing LiDAR data in R. The functions support pixel-level metrics computation, raster template creation, VRT mosaicking, and tree hull extraction.\n\n\n\n2 .stdmetrics()\n#' @title .stdmetrics\n#' @description Berechnet Standardmetriken für LiDAR Rasterzellen\n.stdmetrics &lt;- function(z, i, ...) {\n  return(list(\n    zmax = max(z, na.rm = TRUE),            # Maximum height\n    zmean = mean(z, na.rm = TRUE),          # Mean height\n    zsd = sd(z, na.rm = TRUE),              # Standard deviation of heights\n    zkurto = moments::kurtosis(z, na.rm = TRUE), # Kurtosis (peakedness of distribution)\n    zskew = moments::skewness(z, na.rm = TRUE),  # Skewness (asymmetry)\n    zq25 = quantile(z, 0.25, na.rm = TRUE), # 25th percentile\n    zq50 = quantile(z, 0.5, na.rm = TRUE),  # Median height\n    zq75 = quantile(z, 0.75, na.rm = TRUE), # 75th percentile\n    zpulse = length(z)                      # Number of returns (pulse count)\n  ))\n}\nUsed to derive standard height-based metrics from LiDAR returns per raster cell using pixel_metrics().\n\n\n\n3 get_vrt_img()\n#' @title get_vrt_img\n#' @description Creates a VRT from multiple GeoTIFF files in a directory\nget_vrt_img &lt;- function(name, path, pattern) {\n  tifs &lt;- list.files(path = path, pattern = paste0(pattern, \".tif$\"), full.names = TRUE)\n  vrt &lt;- file.path(path, paste0(name, \".vrt\"))\n  if (file.exists(vrt)) file.remove(vrt)\n  gdal_utils(util = \"buildvrt\", source = tifs, destination = vrt)\n  return(vrt)\n}\nUsed to dynamically generate a VRT (virtual raster stack) from multiple .tif files with a matching pattern, e.g. \"lad_metrics\".\n\n\n\n4 tree_fn()\n#' @title tree_fn\n#' @description Creates convex hulls from segmented trees in LAS catalogs\ntree_fn &lt;- function(las, ...) {\n  if (is.empty(las)) return(NULL)                   # Skip if empty\n  las &lt;- filter_poi(las, !is.na(treeID))            # Keep only trees\n  if (npoints(las) == 0) return(NULL)               # Skip if no points\n  dt &lt;- data.table::as.data.table(las@data)\n  dt &lt;- dt[, .(X = mean(X), Y = mean(Y)), by = treeID]  # Mean location per tree\n  points_sf &lt;- st_as_sf(dt, coords = c(\"X\", \"Y\"), crs = sf::st_crs(las))\n  hulls &lt;- st_convex_hull(st_union(points_sf))      # Create unified convex hull\n  return(hulls)\n}\nUsed with catalog_apply() to derive convex hull geometries from segmented tree point clouds.\n\n\n\n5 template_raster()\n#' @title template_raster\n#' @description Creates an empty raster template based on bounding box and resolution\ntemplate_raster &lt;- function(bbox, crs, res = 1.0) {\n  if (inherits(bbox, \"sf\")) bbox &lt;- st_bbox(bbox)\n  r &lt;- terra::rast(xmin = bbox[\"xmin\"], xmax = bbox[\"xmax\"],\n                   ymin = bbox[\"ymin\"], ymax = bbox[\"ymax\"],\n                   resolution = res, crs = crs)\n  return(r)\n}\nGenerates a blank terra::rast object for rasterizing vector geometries such as LAD polygons or tree hulls."
  },
  {
    "objectID": "assessment/slidelist.html",
    "href": "assessment/slidelist.html",
    "title": "Assessments",
    "section": "",
    "text": "Basic Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit, GitHub & Rstudio [DE]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit, GitHub & Rstudio [EN]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "base/about.html",
    "href": "base/about.html",
    "title": "About this site",
    "section": "",
    "text": "About this site\nThis page summarizes the essential workflows , basic literature and web resources from the distributed course systems , documents and field protocols into a knowledge base.\nAlthough the web space is topic-centered any keyword can be searched using the full text search.\nThe creation of new pages, the editing of existing pages can be triggered directly via the right column online.\nOffline there are several visual editors and full integration with Rstudio etc."
  },
  {
    "objectID": "base/impressum.html#content-responsibility",
    "href": "base/impressum.html#content-responsibility",
    "title": "Impressum",
    "section": "Content Responsibility",
    "text": "Content Responsibility\nThe responsibility for the content rests with the instructors. Statements, opinions and/or conclusions are the ones from the instructors and do not necessarily reflect the opinion of the representatives of Marburg University."
  },
  {
    "objectID": "base/impressum.html#content-license",
    "href": "base/impressum.html#content-license",
    "title": "Impressum",
    "section": "Content License",
    "text": "Content License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\nPrivacy Policy\n\n\nAs of 21. October 2021\n\n\nIntroduction\n\n\nWith the following data protection declaration, we would like to inform you about the types of your personal data (hereinafter also referred to as “data” for short) that we process, for what purposes and to what extent. The privacy policy applies to all processing of personal data carried out by us, both in the context of the provision of our services and in particular on our websites, in mobile applications and within external online presences, such as our social media profiles (hereinafter collectively referred to as “Online Offerings”).\n\n\nThe terms used are not gender-specific.\n\n\nResponsible\n\n\nDr Christoph ReudenbachDeutschhaustr 1035037 Marburg\n\n\nEmail address: reudenbach@uni-marburg.de.\n\n\nImprint: https://www.uni-marburg.de/de/impressum.\n\n\nOverview of Processing\n\n\nThe following overview summarizes the types of data processed and the purposes of their processing, and refers to the data subjects.\n\n\nTypes of Data Processed\n\n\n\nContent data (e.g. input in online forms).\n\n\nContact data (e.g. email, phone numbers).\n\n\nMeta/communication data (e.g. device information, IP addresses).\n\n\nUse data (e.g. websites visited, interest in content, access times).\n\n\n\nCategories of data subjects\n\n\n\nCommunication partners.\n\n\nUsers (e.g.. Website visitors, users of online services).\n\n\n\nPurposes of processing\n\n\n\nDirect marketing (e.g., by email or postal mail).\n\n\nContact requests and communications.\n\n\n\nRelevant legal basis\n\n\nThe following is an overview of the legal basis of the GDPR on the basis of which we process personal data. Please note that in addition to the provisions of the GDPR, national data protection regulations may apply in your or our country of residence or domicile. Furthermore, should more specific legal bases be decisive in individual cases, we will inform you of these in the data protection declaration.\n\n \n\n\nConsent (Art. 6 para. 1 p. 1 lit. a. DSGVO) - The data subject has given his or her consent to the processing of personal data concerning him or her for a specific purpose or purposes.\n\n\nRegistered interests (Art. 6 para. 1 p. 1 lit. f. DSGVO) - Processing is necessary to protect the legitimate interests of the controller or a third party, unless such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require the protection of personal data.\n\n\n\nNational data protection regulations in Germany: In addition to the data protection regulations of the General Data Protection Regulation, national regulations on data protection apply in Germany. These include, in particular, the Act on Protection against Misuse of Personal Data in Data Processing (Federal Data Protection Act - BDSG). In particular, the BDSG contains special regulations on the right to information, the right to erasure, the right to object, the processing of special categories of personal data, processing for other purposes and transmission, as well as automated decision-making in individual cases, including profiling. Furthermore, it regulates data processing for employment purposes (Section 26 BDSG), in particular with regard to the establishment, implementation or termination of employment relationships as well as the consent of employees. Furthermore, state data protection laws of the individual federal states may apply.\n\n \n\nSecurity measures\n\n\nWe take appropriate technical and organizational measures in accordance with the legal requirements, taking into account the state of the art, the implementation costs and the nature, scope, circumstances and purposes of the processing, as well as the different probabilities of occurrence and the extent of the threat to the rights and freedoms of natural persons, in order to ensure a level of protection appropriate to the risk.\n\n.\n\nMeasures include, in particular, ensuring the confidentiality, integrity, and availability of data by controlling physical and electronic access to data as well as access to, entry into, disclosure of, assurance of availability of, and segregation of data concerning them. Furthermore, we have established procedures to ensure the exercise of data subjects’ rights, the deletion of data, and responses to data compromise. Furthermore, we take the protection of personal data into account as early as the development or selection of hardware, software as well as procedures in accordance with the principle of data protection, through technology design and through data protection-friendly default settings.\n\n \n\nDeletion of data\n\n\nThe data processed by us will be deleted in accordance with legal requirements as soon as their consents permitted for processing are revoked or other permissions cease to apply (e.g. if the purpose of processing this data has ceased to apply or it is not necessary for the purpose).\n\n \n\nIf the data are not deleted because they are required for other and legally permissible purposes, their processing will be limited to these purposes. That is, the data will be blocked and not processed for other purposes. This applies, for example, to data that must be retained for reasons of commercial or tax law or whose storage is necessary for the assertion, exercise or defense of legal claims or for the protection of the rights of another natural person or legal entity.\n\n \n\nOur privacy notices may also include further information on the retention and deletion of data that takes precedence for the processing operations in question.\n\n \n\nUse of cookies\n\n\nCookies are text files that contain data from websites or domains visited and are stored by a browser on the user’s computer. The primary purpose of a cookie is to store information about a user during or after their visit within an online site. Stored information may include, for example, language settings on a website, login status, a shopping cart, or where a video was watched. We further include in the term cookies other technologies that perform the same functions as cookies (e.g., when user details are stored using pseudonymous online identifiers, also referred to as “user IDs”)\n\n.\n\nThe following cookie types and functions are distinguished:\n\n\n\nTemporary cookies (also: session or session cookies): Temporary cookies are deleted at the latest after a user has left an online offer and closed his browser.\n\n\nPermanent cookies: Permanent cookies remain stored even after closing the browser. For example, the login status can be saved or preferred content can be displayed directly when the user revisits a website. Likewise, the interests of users used for range measurement or marketing purposes can be stored in such a cookie.\n\n\nFirst-party cookies: First-party cookies are set by ourselves.\n\n\nThird-party cookies (also: third-party cookies): Third-party cookies are mainly used by advertisers (so-called third parties) to process user information.\n\n\nNecessary (also: essential or absolutely necessary) cookies: Cookies may be absolutely necessary for the operation of a website (e.g. to store logins or other user input or for security reasons).\n\n\nStatistics, marketing and personalization cookies: Furthermore, cookies are usually also used in the context of range measurement and when the interests of a user or his behavior (e.g. viewing certain content, use of functions, etc.) on individual web pages are stored in a user profile. Such profiles are used, for example, to show users content that matches their potential interests. This process is also referred to as “tracking”, i.e., tracking the potential interests of users. Insofar as we use cookies or “tracking” technologies, we will inform you separately in our privacy policy or in the context of obtaining consent.\n\n\n\nNotes on legal bases: On which legal basis we process your personal data using cookies depends on whether we ask you for consent. If this is the case and you consent to the use of cookies, the legal basis for the processing of your data is the declared consent. Otherwise, the data processed with the help of cookies is processed on the basis of our legitimate interests (e.g. in a business operation of our online offer and its improvement) or, if the use of cookies is necessary to fulfill our contractual obligations.\n\n.\n\nDuration of storage: If we do not provide you with explicit information about the storage period of permanent cookies (e.g. in the context of a so-called cookie opt-in), please assume that the storage period can be up to two years.\n\n.\n\nGeneral information on revocation and objection (opt-out):  Depending on whether the processing is based on consent or legal permission, you have the option at any time to revoke any consent given or to object to the processing of your data by cookie technologies (collectively referred to as “opt-out”). You can initially declare your objection by means of your browser settings, e.g. by deactivating the use of cookies (whereby this may also restrict the functionality of our online offer). An objection to the use of cookies for online marketing purposes can also be declared by means of a variety of services, especially in the case of tracking, via the websites https://optout.aboutads.info and https://www.youronlinechoices.com/. In addition, you can receive further objection notices in the context of the information on the service providers and cookies used.\n\n.\n\nProcessing of cookie data on the basis of consent: We use a cookie consent management procedure, in the context of which the consent of users to the use of cookies, or the processing and providers mentioned in the cookie consent management procedure can be obtained and managed and revoked by users. Here, the declaration of consent is stored in order not to have to repeat its query and to be able to prove the consent in accordance with the legal obligation. The storage can take place on the server side and/or in a cookie (so-called opt-in cookie, or with the help of comparable technologies), in order to be able to assign the consent to a user or their device. Subject to individual information on the providers of cookie management services, the following information applies: The duration of the storage of consent can be up to two years. Here, a pseudonymous user identifier is formed and stored with the time of consent, information on the scope of consent (e.g., which categories of cookies and/or service providers) as well as the browser, system and end device used.\n\n.\n\n\nTypes of data processed: Usage data (e.g. websites visited, interest in content, access times), meta/communication data (e.g. device information, IP addresses).\n\n\nPersons concerned: Users (e.g. website visitors, users of online services).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nSurveys and polls\n\n\nThe surveys and polls (hereinafter “surveys”) conducted by us are evaluated anonymously. Personal data is only processed insofar as this is necessary for the provision and technical implementation of the surveys (e.g. processing of the IP address to display the survey in the user’s browser or to enable a resumption of the survey with the help of a temporary cookie (session cookie)) or users have consented.\n\n.\n\nNotes on legal basis: If we ask participants for consent to process their data, this is the legal basis of the processing, otherwise the processing of participants’ data is based on our legitimate interests in conducting an objective survey.\n\n \n\n\nTypes of data processed: Contact data (e.g. email, phone numbers), content data (e.g. input in online forms), usage data (e.g. web pages visited, interest in content, access times), meta/communication data (e.g. device information, IP addresses).\n\n\nParticipants concerned: Communication partners.\n\n\nPurposes of processing: Contact requests and communication, direct marketing (e.g. by e-mail or postal mail).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nChange and Update Privacy Policy\n\n\nWe encourage you to periodically review the contents of our Privacy Policy. We adapt the Privacy Policy as soon as the changes in the data processing activities we carry out make it necessary. We will inform you as soon as the changes require an act of cooperation on your part (e.g. consent) or other individual notification.\n\n.\n\nWhere we provide addresses and contact information for companies and organizations in this Privacy Policy, please note that addresses may change over time and please check the information before contacting us.\n\n.\n\nRights of data subjects\n\n\nAs a data subject, you are entitled to various rights under the GDPR, which arise in particular from Art. 15 to 21 DSGVO:\n\n\n\nRight to object: You have the right to object at any time, on grounds relating to your particular situation, to the processing of personal data relating to you which is carried out on the basis of Art. 6(1)(e) or (f) DSGVO; this also applies to profiling based on these provisions. If the personal data concerning you is processed for the purpose of direct marketing, you have the right to object at any time to the processing of personal data concerning you for the purpose of such marketing; this also applies to profiling, insofar as it is associated with such direct marketing.\n\n\nRight of withdrawal in the case of consent: You have the right to withdraw any consent you have given at any time.\n\n\nRight of access: You have the right to request confirmation as to whether data in question is being processed and to information about this data, as well as further information and copy of the data in accordance with the legal requirements.\n\n\nRight of rectification: You have the right, in accordance with the legal requirements, to request the completion of the data concerning you or the correction of incorrect data concerning you.\n\n\nRight to erasure and restriction of processing: You have, in accordance with the law, the right to request that data concerning you be erased without undue delay, or alternatively, in accordance with the law, to request restriction of the processing of the data.\n\n\nRight to data portability: You have the right to receive data concerning you, which you have provided to us, in a structured, common and machine-readable format in accordance with the legal requirements, or to demand its transfer to another responsible party.\n\n\nComplaint to supervisory authority: Without prejudice to any other administrative or judicial remedy, you have the right to lodge a complaint with a supervisory authority, in particular in the Member State of your habitual residence, place of work or the place of the alleged infringement, if you consider that the processing of personal data concerning you infringes the requirements of the GDPR.\n\n\n.\n\nDefinitions of Terms\n\n\nThis section provides you with an overview of the terms used in this Privacy Policy. Many of the terms are taken from the law and defined primarily in Article 4 of the GDPR. The legal definitions are binding. The following explanations, on the other hand, are primarily intended to aid understanding. The terms are sorted alphabetically.\n\n \n\n\nPersonal data: “Personal data” means any information relating to an identified or identifiable natural person (hereinafter “data subject”); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier (eg. e.g. cookie) or to one or more special characteristics that are an expression of the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.\n\n\nController: The “controller” is the natural or legal person, public authority, agency or other body which alone or jointly with others determines the purposes and means of the processing of personal data.\n\n\nProcessing: “Processing” means any operation or set of operations which is performed upon personal data, whether or not by automatic means. The term is broad and includes virtually any handling of data, whether collecting, evaluating, storing, transmitting or deleting.\n\n\n\nCreated with free Datenschutz-Generator.de by Dr. Thomas Schwenke"
  },
  {
    "objectID": "base/impressum.html#comments-suggestions",
    "href": "base/impressum.html#comments-suggestions",
    "title": "Impressum",
    "section": "Comments & Suggestions",
    "text": "Comments & Suggestions"
  },
  {
    "objectID": "modeling/modeling-material.html",
    "href": "modeling/modeling-material.html",
    "title": "Data and Software",
    "section": "",
    "text": "Please find all Data Downloads at the Course Data Server Data folder for any file exchange and data related purposes."
  },
  {
    "objectID": "modeling/modeling-material.html#data-set-for-training-purposes",
    "href": "modeling/modeling-material.html#data-set-for-training-purposes",
    "title": "Data and Software",
    "section": "",
    "text": "Please find all Data Downloads at the Course Data Server Data folder for any file exchange and data related purposes."
  },
  {
    "objectID": "modeling/modeling-material.html#specific-modeling-software",
    "href": "modeling/modeling-material.html#specific-modeling-software",
    "title": "Data and Software",
    "section": "Specific modeling software",
    "text": "Specific modeling software\nPlease find all Downloads according to ENVI-met at the ENVI-met landing page"
  },
  {
    "objectID": "modeling/modeling-material.html#common-software",
    "href": "modeling/modeling-material.html#common-software",
    "title": "Data and Software",
    "section": "Common Software",
    "text": "Common Software\nShell — any command line environment will do for the exercises. For Linux we recommend the bash shell. For Windows the Windows command line can be used.\n\nQGIS has become one of the most promising and most integrative open source GIS systems over the last years. Through the processing plugin, it additionally integrates modules from the other leading free GIS solutions. We will need it (if necessary) to prepare or manipulate some of the data.\n\nRegarding installation, for Ubuntu Linux, the Ubuntu GIS package is a good choice. For Windows, we strongly recommend installing everything via the OSGeo4W environment and not the standalone QGIS installation tool.\nFor most of the data pre and postprocessing, we will use the statistical scripting language R and we highly recommend the Rstudio integrated developing environment.\nPlease follow the instructions according to your operating system."
  },
  {
    "objectID": "modeling/modeling-material.html#additional-data-sources",
    "href": "modeling/modeling-material.html#additional-data-sources",
    "title": "Data and Software",
    "section": "Additional data sources",
    "text": "Additional data sources\nYou can find information about the “Zukunftsquartier Hasenkopf” in various places:\n\nCity of Marburg Zukunftsquartier Hasenkopf\nSlides including the winning design\nCitizens’ initiative wirsindhasenkopf Flyer and here “We argue”\nEnvi-met Hasenkopf GIS and Modeldata\nDigital Elevation Model DEM and Digital Surface Model DSM files relevant for Marburg can be downloaded from the GDS website of the Hessian Administration for Land Management and Geoinformation\nFor downloading the OSM data it is recommended to use the OSMDownloader extension to QGIS. It simply provides the ability to draw a rectangle and download the complete and currently available OSM data to a file named hasenkopf.osm.\nIf the data has to be digitised manually, it is advisable to use an up-to-date aerial photograph from Bing or Google. These can be easily integrated via XYZ tiles\nThe planning data for the development and sealing were taken from page 23 of the presentation of the winning design via screenshot."
  },
  {
    "objectID": "doc/vignette-microclimf.html",
    "href": "doc/vignette-microclimf.html",
    "title": "Running microclimf",
    "section": "",
    "text": "Overview\nPackage install\nQuick start\nModel inputs\n\nMeteorological data\nVegetation parameters\nSoil parameters\nAdditional optional parameters\n\nRunning the point microclimate model\nSubsetting the microclimate model\nPreparing model inputs\nRunning the model\n\nSoil moisture\nRadiation\nSensible heat flux and wind\nGround surface temperature\nLatent heat\nAbove ground\n\nAbove canopy\nBelow canopy\n\nBelow ground\nRunning the whole model\nModel output and formats\nRunning the model with arrays of climate data\n\nRunning the model over large areas\nBioclim variables\nSnow"
  },
  {
    "objectID": "doc/vignette-microclimf.html#overview",
    "href": "doc/vignette-microclimf.html#overview",
    "title": "Running microclimf",
    "section": "Overview",
    "text": "Overview\nThis vignette describes the R package ‘microclimf’. The package contains a series of functions for modelling above and below canopy or below-ground microclimatic conditions across real landscapes, providing gridded outputs. In line with standard approaches for mechanistic microclimate modelling, the model is founded on the principles of energy balance, with the temperature of a surface or the air above it being contingent on how much energy is received or lost. Opaque surfaces in the environment, namely the canopy and the ground, absorb radiation from the sun, but also emit radiation as thermal energy. These surfaces also exchange sensible heat with the surrounding air and undergo latent heat fluxes, namely evaporative and evapotranspirative cooling. Some of the energy is also stored or released by the ground. Because the various components of the energy budget have a dependence on temperature, the temperature of the environment is calculated by assuming that energy budget always remain in balance and then re-arranging the energy balance equations to solve for temperature. However, because of various interdependencies, e.g. between the degree of surface heating and the exchange of sensible heat, and the temperature of the ground surface and the rate of storage by the ground, a closed-form mathematical solution to the energy budget equations cannot be derived. Rather the model must be solved iteratively, which is computational expensive if modelling over multiple grid cells.\nA key aim if microclimf is to ensure computational efficiency, which is achieved in four ways. First, it is assumed that the energy budget can be solved mathematically using the Penman-Monteith method (Penman 1948; Monteith 1965) if these interdependencies are ignored, resulting in only modest errors. If doing so for a single point location, the ratio of the temperature offset from ambient air temperature for that location relative to that for any other location is preserved when solving the model iteratively. Thus, running the model iteratively for a single point location and solving the model mathematically for all grid cells, provides a route to estimating the iterative solution for the entire landscape in a computationally efficient manner. Second, and a further advantage of running a point model separately is that one can subset outputs from the point model. While the point model must be run in hourly time-increments there is no need to do so for the grid model: one can instead opt to select from the point model, only those hours that correspond to e.g. the monthly maximum, minimum or median temperature. Thirdly, some simplifying assumptions are about the nature of vegetated canopies to avoid the need to describe vertical variation in leaf foliage density in detail when characterising below-canopy microclimates. This eliminates the need to evoke a multi-layer canopy model. Lastly, most of the heavy lifting is done by c++ code, which typically runs much more quickly than R code. The R functions are essentially wrappers for the underlying c++ code meaning users get the high-level expressiveness of R, but the computational performance of compiled c++."
  },
  {
    "objectID": "doc/vignette-microclimf.html#package-install",
    "href": "doc/vignette-microclimf.html#package-install",
    "title": "Running microclimf",
    "section": "Package install",
    "text": "Package install\nStart by installing the package form Github as follows:\n\nrequire(devtools)\ninstall_github(\"ilyamaclean/microclimf\")\n\nThe package has a few dependencies, which may also need to be installed or updated. If there are any install issue, a useful starting point for troubleshooting is to install the dependencies first. These are abind, ncdf4, Rcpp, sf, stats, terra, utils and zoo all of which are on CRAN."
  },
  {
    "objectID": "doc/vignette-microclimf.html#quick-start",
    "href": "doc/vignette-microclimf.html#quick-start",
    "title": "Running microclimf",
    "section": "Quick start",
    "text": "Quick start\nHere I provide brief instructions for how to run microclimf. More in-depth instructions are provided below. Four sets of input variables are needed: (1) a dataset of hourly weather, (2) a digital elevation dataset, (3) a dataset of vegetation parameters and (4) a dataset of soil properties. The datasets should have exactly the same format and units as the example datasets included with the package. The spatial resolution and extent of outputs is determined by the spatial resolution of the digital elevation dataset, and the spatial datasets of vegetation parameters and soil properties should also match the digital elevation dataset in terms of resolution and extent. It is important also that the x, y and z dimensions of the digital elevation dataset are equivalent – i.e. an equal area projection is used rather than say latitude and longitude, with units of x, y and height all identical and typically in metres.\nThe first step to run the point model in hourly time increments using function runpointmodel. One then has the option to subset the point model to say return monthly values and pass these as inputs to the grid model. Both the point model and grid model can be run in two modes: either with hourly weather data provided as a data frame in which case the point model is run once only and the weather is assumed identical across the study area. Alternatively the weather data can be provided as multi-layer SpatRasters, in which the point model is run for each grid cell of the SpatRaster and the weather is assumed to vary across the study region.\nIn the code below, weather data are provided as a data frame. The point microclimate model is run and then subset to return only those hours corresponding to the day in each month with the hottest and coldest temperature (as determined by the point model). These are then passed to the grid model. The model returns temperatures (leaf, ground and air), relative humidity, wind speed, and components of the radiation budget, all as 3D arrays, representing values for each pixel and time-period. In the final lines of codes, selected outputs are plotted\n\nlibrary(microclimf)\nlibrary(terra)\n# Runs point microclimate model with inbuilt datasets\nmicropoint &lt;- runpointmodel(climdata, reqhgt = 0.05, dtmcaerth, vegp, soilc)\n# Subset point model outputs\nmicropoint_mx &lt;- subsetpointmodel(micropoint, tstep = \"month\", what = \"tmax\")\nmicropoint_mn &lt;- subsetpointmodel(micropoint, tstep = \"month\", what = \"tmin\")\n# Run grid model 5 cm above ground with subset values and inbuilt datasets (takes ~20 seconds)\nmout_mx &lt;- runmicro(micropoint_mx, reqhgt = 0.05, vegp, soilc, dtmcaerth)\nmout_mn &lt;- runmicro(micropoint_mn, reqhgt = 0.05, vegp, soilc, dtmcaerth)\nattributes(mout_mx)\n# Plot air temperatures on hottest hour in micropoint (2017-06-20 13:00:00 UTC)\nmypal &lt;- colorRampPalette(c(\"darkblue\", \"blue\", \"green\", \"yellow\", \"orange\",  \"red\"))(255)\nplot(rast(mout_mx$Tz[,,134]), col = mypal)\n# Plot mean of monthly max and min\nmairt&lt;-apply((mout_mn$Tz + mout_mx$Tz) / 2, c(1,2), mean)\nplot(rast(mairt), col = mypal)\n\n\nMaximum (right) and mean (left) temperature 5 cm above ground. On warm sunny days, the temperature immediately above sunward facing slopes gets pretty hot. The colder areas are those that have shade cover."
  },
  {
    "objectID": "doc/vignette-microclimf.html#model-inputs",
    "href": "doc/vignette-microclimf.html#model-inputs",
    "title": "Running microclimf",
    "section": "Model inputs",
    "text": "Model inputs\nThree sets of parameters are needed to run the model: (i) standard hourly meterological climate-forcing variables representative of macroclimatic conditions across the study site, usually in the form of a data.frame with single values for each hour (though the option to include an array of coarse-gridded values is also available - see below). (ii) A suite of parameters describing properties of the canopy in the form of high-resolution gridded values. (iii) A suite of parameters describing properties of the soil in the form of high-resolution gridded values. A raster of digital elevation data is also required. Optionally, some additional parameters can be set when running the models, as detailed below.\nEach set of parameters is described in turn. Obtaining the right data to drive the microclimate model is often one of the most challenging aspects of modelling microclimate. Users may wish to explore the microclimdata package for automated downloading and processing of various datasets available globally or regionally for doing so. The package is available on Github: https://github.com/ilyamaclean/microclimdata\n\nMeteorological data\nThe inbuilt data,frame climdata gives an example of the hourly meteorological variables needed to run the model:\n\nlibrary(microclimf)\nhead(climdata)\n#&gt;              obs_time  temp   relhum    pres swdown difrad   lwdown windspeed\n#&gt; 1 2017-01-01 00:00:00 7.483 92.02474 101.852      0      0 310.6365     5.895\n#&gt; 2 2017-01-01 01:00:00 7.456 94.36533 101.801      0      0 310.8483     5.603\n#&gt; 3 2017-01-01 02:00:00 7.244 96.83536 101.765      0      0 309.2949     5.470\n#&gt; 4 2017-01-01 03:00:00 7.071 98.12848 101.739      0      0 308.5616     5.540\n#&gt; 5 2017-01-01 04:00:00 6.988 98.18165 101.721      0      0 307.3412     5.816\n#&gt; 6 2017-01-01 05:00:00 6.948 97.45834 101.710      0      0 307.0171     6.253\n#&gt;   winddir     precip\n#&gt; 1     283 0.04870449\n#&gt; 2     294 0.03049236\n#&gt; 3     307 0.01623711\n#&gt; 4     321 0.01246609\n#&gt; 5     334 0.02634117\n#&gt; 6     345 0.03545065\n\nThe data frame contains the following columns: obs_time – POSIXlt object of observation times for each climate variable, temp – temperatures (deg C), relhum - relative humidity (percentage), pres - atmospheric pressure (kPa), swdown - total downward shortwave radiation received by a horizontal surface (W/m^2), difrad - diffuse radiation (W/m^2), lwdown total downward longward radiation (W/m^2), windspeed - wind speed at reference height (m/s), winddir - wind direction in degrees and precip - hourly precipitation (mm). Precipitation is used to compute soil moisture and sub-model for this actually runs in daily time-steps. Thus, if only daily precipitation data are available, hourly data can be provided as daily values / 24.\nImportantly, the entries of obs_time must all be in UTC (Coordinated Universal Time).\nAny input weather dataset provided must use the same format, column names and units as in this example dataset. Most of these are standard meteorology variables that are readily available globally. If unknown for the study area, users may wish to explore the mcera5 package on github (https://github.com/dklinges9/mcera5) or the micorclimdata package detailed above. Diffuse radiation, is sometimes harder to come by as many standard weather stations only estimate total radiation. If unknown, it can be estimated using the difprop function in the microctools package (https://github.com/ilyamaclean/microctools). The microctools package, also contains a function converthumidity, for converting absolute or specific humidity or vapour pressure to relative humidity.\n\n\nVegetation parameters\nThe inbuilt dataset vegp gives an example of the vegetation parameters needed to run the model. Here the attributes are shown and individual parameters plotted. Data are all stored as SpatRasters though in the inbuilt dataset these are PackedSpatRasters (see terra::wrap).\nAll vegetated data can be provided as either single layer SpatRasters, in which case they are assumed time-invariant, or as multi-layer SpatRasters, in which case they are assumed to vary seasonally. For example, in the inbuilt dataset vegp$pai is a 12 layer SpatRaster corresponding to approximately monthly values when the model is run over an entire year. Had the SpatRaster contained 365 values, vegetation would be assummed to vary daily. No more than one value per day can be provided. A mixture of single layer and multi-layer SpatRasters can be provided - the model takes care of things. Note, however, that the model does not know which layers correspond to which time-period. It is simply that if e.g. 12 layers are provided, the entire time-sequence over which the model is run is divided into 12 approximately equal sized chunks and separate vegetation data used for each chunk. Note that the data are matched to the time-sequence of the original climate date used to run the point model, not the values returned by sub-setting the point model.\n\nlibrary(terra)\n#&gt; terra 1.8.60\nattributes(vegp)\n#&gt; $names\n#&gt; [1] \"pai\"   \"hgt\"   \"x\"     \"gsmax\" \"leafr\" \"clump\" \"leafd\" \"leaft\"\n#&gt; \n#&gt; $class\n#&gt; [1] \"vegparams\"\n# Plot spatial and temporal variation in pai\nplot(rast(vegp$pai)[[1]], main = \"Jan PAI\")\npaiarray &lt;- as.array(rast(vegp$pai))\nvegpmean&lt;-apply(paiarray, 3, mean, na.rm = TRUE)\nplot(vegpmean, type=\"l\", ylim = c(0, 0.25), main = \"Seasonal variation in PAI\")\n# Plot other variables\nplot(rast(vegp$hgt), main=\"Vegetation height\") \nplot(rast(vegp$x), main = \"Leaf angle distribution\") \nplot(rast(vegp$gsmax), main=\"Max. stomatal conductance\") \nplot(rast(vegp$clump)[[1]], main = \"Jan Canopy clumping factor\") # set to 0 \nplot(rast(vegp$leafr),col=gray(0:255/255), main = \"Leaf reflectance\")\nplot(rast(vegp$leafd), main = \"Mean leaf diameter\") # set to 0.05\nplot(rast(vegp$leaft),col=gray(0:255/255), main = \"Leaf transmittance\") # set equal to leafr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf users do know values of these vegetation parameters across their study area, they can be approximated from habitat type using the vegpfromhab function. This function takes as an input, a raster of habitat types numerically coded as follows:\n\nfor Evergreen needleleaf forest,\nfor Evergreen broadleaf forest,\nfor Deciduous needleleaf forest,\nfor Deciduous broadleaf forest,\nfor Mixed forest,\nfor Closed shrubland,\nfor Open shrubland,\nfor Woody savanna,\nfor Savanna,\nfor Short grassland,\nfor Tall grassland,\nfor Permanent wetland,\nfor Cropland,\nfor Urban and built-up,\nfor Cropland / Natural vegetation mosaic and\nfor Barren or sparsely vegetated\n\nIt returns an object of class vegparams as required by the model. Here this is illustrated using he inbuilt habitat SpatRast layer\n\nplot(rast(habitats), main = \"Habitat types\") # inbuilt habitat SpatRast layer\ntme&lt;-as.POSIXlt(c(0:8783)*3600,origin=\"2000-01-01 00:00\", tz = \"GMT\")\n# Create an object of class vegparams:\nveg&lt;-vegpfromhab(habitats,tme=tme)\n\n\n\n\n\n\n\n\nNote however, that be doing so, all values for a habitat type will be identical, when in reality this is unlikely to be the case. If one is unable to quantify the main determinants of microclimatic variation then there is little prospect of being able to model microclimatic conditions accurately and any outputs form the model should be treated with a high-degree of skepticism.\nThe model is most sensitive to pai (the total one sided area of both leaves and woody and dead vegetation per unit ground area) and hgt (vegetation height in metres). The former is needed primarily so that canopy cover can be estimated, but even for temperatures above canopy, pai partially determines the temperature profile. hgt is important as determines whether reqhgt is above or below vegetation and also dictates the shape of the temperature profile above vegetation.\nThe most sensible use-case for the vegpfromhab function is thus when one has alternative data that could be used to estimate pai and hgt that can be slotted into the output returned by this function. This is straightforward as the vegetation inputs to the model as returned by vegpfromhab is just a list of SpatRasters.\nThe model is less sensitive to other parameters. The parameter x represents how vertically or horizontally the leaves of the canopy are orientated and controls how much direct radiation is transmitted through the canopy at a given solar angle (when the sun is low above the horizon, less radiation is transmitted through vertically orientated leaves). Users may refer to Campbell (1986) for a detailed explanation. Values for deciduous woodland are typically around 1, but for grassland may be closer to 0.2. The parameter gsmax is the maximum stomatal conductance (mol / m^2 / s) of leaves and is needed for evapotranspiration calculations. Values typically range from 0.23 for deciduous broadleaf forest to 0.55 for wetland vegetation. Körner (1995) gives values for major vegetation types of the world. The parameter ‘leafr’ is the leaf reflectance to shortwave radiation, with typical values around 0.4.\nThe parameter clump dictates how much radiation passes through gaps in the canopy, and therefore represents the sub-pixel canopy clumpiness, with values ranging from 0 (uniform) to 1 (highly clumped). In general, it varies with vegetation height and plant area index. The function clumpestimate can be used to derive an approximate estimate. The parameter leafd is the mean diameter of leaves.\nThe vegpfromhab function assigns approximate values for leaf reflectance accordance to habitat type. However, it can also be estimated from surface albedo using function leafrfromalb. In applying this function, leaf transmittance is assumed proportional to leaf reflectance and a proportionality coefficient can be specified. In general, model outputs are not sensitive to this coefficient,\n\n\nSoil parameters\nThe inbuilt dataset soilc gives an example of the soil parameters needed to run the model. Here the attributes are shown and plotted:\n\nattributes(soilc)\n#&gt; $names\n#&gt; [1] \"soiltype\" \"groundr\" \n#&gt; \n#&gt; $class\n#&gt; [1] \"soilcharac\"\nplot(rast(soilc$soiltype), main = \"Soiltype\") # Clay loam throughout\nplot(rast(soilc$groundr), col=gray(0:255/255), main = \"Soil reflectance\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a list of the following:\n\nsoiltype - a PackedSpatRast object of integer soil types\ngroundr - a PackedSpatRast object of soil reflectance values for shortwave radiation (0 - 1)\n\n\nAgain, users in creating such a dataset, can store soiltype and groundr as either a PackedSpatRast or a SpatRast object.\nSoil type 7 corresponds to Clay loam. A full list of which numeric values correspond to which soil types, along with parameters associated with these soil types is shown in the soilparameters table:\n\nsoilparameters\n#&gt;          Soil.type Number  Smax  Smin     Ksat   b psi_e   Vq     Vm     Vo\n#&gt; 1             Sand      1 0.399 0.049 501.1200 1.7   0.7 0.30 0.3000 0.0010\n#&gt; 2       Loamy sand      2 0.402 0.054 146.8800 2.1   0.9 0.24 0.3550 0.0030\n#&gt; 3       Sandy loam      3 0.403 0.058  62.2080 3.1   1.5 0.18 0.4100 0.0070\n#&gt; 4             Loam      4 0.422 0.074  31.9680 4.5   1.1 0.12 0.4400 0.0180\n#&gt; 5        Silt loam      5 0.447 0.067  16.4160 4.7   2.1 0.00 0.4700 0.0830\n#&gt; 6  Sandy clay loam      6 0.388 0.089  10.3680 4.0   2.8 0.14 0.4640 0.0080\n#&gt; 7        Clay loam      7 0.419 0.091   5.5296 5.2   2.6 0.06 0.5090 0.0120\n#&gt; 8  Silty clay loam      8 0.441 0.089   3.6288 6.6   3.3 0.04 0.5080 0.0110\n#&gt; 9       Sandy clay      9 0.381 0.103   2.8512 6.0   2.9 0.15 0.4655 0.0035\n#&gt; 10      Silty clay     10 0.368 0.073   2.1600 7.9   3.4 0.00 0.6240 0.0080\n#&gt; 11            Clay     11 0.394 0.073   1.4688 7.6   3.7 0.00 0.6000 0.0060\n#&gt;        Mc      rho        mult      rmu        a      pwr\n#&gt; 1  0.0100 1.597779 0.000293942 0.037449 0.003018 0.963099\n#&gt; 2  0.0350 1.587082 0.000302099 0.038750 0.008320 1.037554\n#&gt; 3  0.0600 1.578984 0.000190685 0.024034 0.006535 0.667482\n#&gt; 4  0.0844 1.513506 0.000189847 0.023019 0.009031 0.807574\n#&gt; 5  0.1240 1.358636 0.000172124 0.020029 0.026067 1.572287\n#&gt; 6  0.3648 1.617506 0.000184663 0.021304 1.191259 4.077206\n#&gt; 7  0.5422 1.529643 0.000191202 0.021303 0.059765 1.134773\n#&gt; 8  0.3948 1.472509 0.000175762 0.019098 0.132554 1.907443\n#&gt; 9  0.5050 1.642237 0.000173624 0.016682 0.099531 0.820078\n#&gt; 10 0.5500 1.670682 0.000149402 0.014268 0.147172 1.156038\n#&gt; 11 1.0000 1.604273 0.000164801 0.016936 0.239373 1.514030\n\nThe model also copes with these values being provided as individual data layers if these are added to the list of SpatRasters contained in soilc. Note that in all instances soil properties (apart from soil moisture, which is modelled explicitly) are assumed time-invariant.\n\n\nAdditional optional parameters\nIn addition to specifying reqhgt the height (m) above or below ground for which microclimate estimates are required, there are also a set of optional parameters that can be provided to the run functions that control model behaviour. For the point model, these are as follows: * runchecks - logical indicating whether to call function checkinputs to run checks on format and units of input data (see details under model input functions). * windhgt - height above ground of wind speed data in weather (see details under wind) * soilm - a vector of hourly soil moisture values in upper 10 cm of the soil (calculated using a simple soil model if not supplied - see details under running the point model) * dTmx - maximum amount by which canopy or ground surface temperatures can exceed air temperatures when running the point model (see details under running the point model). * maxiter - integer indicating the maximum number of iterations to use when running the point model (see details under running the point model) Additionally, there are a number of options for internal use, which can generally be ignored by the user as they are calculated if not supplied, but in brief, these are: yearG - an option dictating whether or not to account for annual cycles in the ground flux when a year or more of data are provided, lat and long - the latitude and longitude of the location for which the point model is run, vegp_p, ground_p the vegetation and ground parameters used for running the point model, soiltype - the assumed soiltype at the location for which the model is run and mxhgt - the height to which weather data are adjusted, if not supplied calculated from vegetation height across the study area.\nFor the grid model, the following additional parameters can be supplied\n\ndtmc - a coarse-resolution digital elevation dataset matching the resolution of climate data and used to perform elevation adjustments only if climate data are provided as multi-layer SpatRasters.\naltcorrect - a single numeric value indicating whether to apply an elevation lapse rate correction to temperatures and pressures (0 = no correction, 1 = fixed lapse rate correction, 2 = humidity-dependent variable lapse rate correction)\nrunchecks - as for the point model.\npai_a an array of plant area index values above reqhgt. Estimated by assuming a plausible vertical distribution of leaf foliage density if left as ’NA` (see details under radiation).\ntfact an optional coefficient determining the sensitivity of spatial variation in soil moisture to variation in topographic wetness (see details under soil moisture).\nout an optional vector of logicals indicating which variables to return ordered as for the listed outputs when rehgt &gt; 0' (e.g. out[1] = TRUE indicates that Tz is returned, out[2] = TRUE that tleaf is returned etc). By default all variables are returned, but if, for example, only temperature or humidity are required as outputs, setting relevant values to FALSE can save a lot of memory.\nslr, apr and twi - optional SpatRaster objects of slope, aspect, and topographic wetness. If not supplied, these are calculated from the provided dtm, but users may wish to provide their own values to avoid edge effects.\nhor, svfa and wsa - optional array of the tangent of the angle to the horizon in 24 directions (used for calculating terrain shading), skyview factors (used for adjusting diffuse and downward longwave radiation) and wind shelter coefficients in 8 directions (used for determining wind speed). If not supplied, these are calculated from the provided dtm, but users may wish to provide their own values to avoid edge effects.\nmethod - set either as R or by default as Cpp. If set to Cpp the entire model is run using c++ code, and is therefore optimized for speed and memory allocation. If set to R individual components of the model coded as R wrappers are run as described below. This is marginally slower and much more memory hungry, but affords users greater flexibility to e.g. interrogate individual model components or to swap their own functions in and out of the model for individual components."
  },
  {
    "objectID": "doc/vignette-microclimf.html#running-the-point-microclimate-model",
    "href": "doc/vignette-microclimf.html#running-the-point-microclimate-model",
    "title": "Running microclimf",
    "section": "Running the point microclimate model",
    "text": "Running the point microclimate model\nTo ensure the grid model can be run without iteration, the first stage of modelling is to run a point microclimate model iteratively for a flat surface at the centre of the study area using as inputs to the model, vegetation and soil characteristics that are broadly representative of the study area. This is achieved automatically using function runpointmodel, using as inputs to the model, the same inputs that are supplied to the grid model as follows:\n\n# Run the point model\nmicropoint &lt;- runpointmodel(climdata, reqhgt = 0.05, dtmcaerth, vegp, soilc)\n\nInterrogating the attributes of micropoint allows us to see what the model returns:\n\nattributes(micropoint)\n\nThe function essentially gathers various things into a single list object. These are as follows:\n\nweather - the original supplied weather data, but height adjusted if vegetation within the study area exceeds the height of temperature or wind speed measurements.\ndfo - this is a data.frame that stores all the useful stuff that the grid model needs. Most are not worth remaking on as they are essentially used to handle adjustments made to the grid model outputs to avoid the need to run it iteratively (e.g. umu scales wind speeds without and without diabatic correction coefficients included), but four variables are worth remaking on. These are G the rate of heat storage by the soil (W/m^2), soilm - soil water content expressed as a volumetric fraction, Tg - ground surface temperature and Tc - the average temperature of vegetation. Plots of Tg and Tc are shown below.\nTbz if the model is run above ground this is just set to NA, but if run below ground, this is a vector of soil temperatures at depth reqhgt.\nlat and long are the latitude and longitude of the centre of the study area, which is the location for which the point model was run.\nzref represents the height above ground to which weather data have been adjusted. If none of the vegetation is greater than two meters in height the output zref is set to 2 m and windhgt is also two meters, the returned weather dataset is identical to that provided the function. However, for the grid microclimate model to derive below-canopy wind and temperature profiles, the reference height must be higher than the tallest vegetation in the study area. For that reason, if the tallest vegetation exceeds two meters, zref is set to the maximum height of the vegetation and the wind speeds and temperatures are adjusted for height in the weather dataset.\nsubs and tmeorig help the grid model handle sub-setting of the point model (see below). Prior to sub-setting, tmeorig is a POSIXlt object of dates and corresponding to dates and times in weather and dfo and subs is just a vector indicating which values have been returned (all prior to any sub-setting). Below, ground and vegetated surface temperatures are plotted to show what the outputs of the point model look like.\n\n\nmicropoint &lt;- runpointmodel(climdata, reqhgt = 0.05, dtmcaerth, vegp, soilc)\nmicrop &lt;- micropoint$dfo\ntme &lt;- as.POSIXct(micropoint$tmeorig)\npar(mar=c(5,5,3,3))\nplot(microp$Tg ~ tme, type=\"l\", ylim = c(-5, 50), col = rgb(1,0,0,0.5), xlab = \"Month\", ylab = \"Temperature\") # temperature of ground surface\npar(new = TRUE)\nplot(microp$Tc ~ tme, type=\"l\", ylim = c(-5, 50), col = rgb(0,0.5,0.5,0.5), xlab = \"\", ylab = \"\")\n\n\n\nThe equivalent to the runpointmodel function when climate data are provided as arrays is `runpointmodela. An example of its use is shown below."
  },
  {
    "objectID": "doc/vignette-microclimf.html#subsetting-the-microclimate-model",
    "href": "doc/vignette-microclimf.html#subsetting-the-microclimate-model",
    "title": "Running microclimf",
    "section": "Subsetting the microclimate model",
    "text": "Subsetting the microclimate model\nThe point microclimate model is usually run in hourly time-increments using complete time sequences of weather data to fully allow for the diurnal cycles in ground heat fluxes to be accounted for, but if desired, the grid model can be run for just say the hottest days in each month to derive maximum temperatures. This is achieved using function subsetpointmodel. This function takes an object of class ’pointmicroas an input and also returns an object of class ‘pointmicro, but with the request hours extracted from ’pointmicroIn the example below, the model is subset to return only those hours corresponding to the day in each month with the hottest temperature (as determined by the point model). The function has several inputs that control its behaviour. If 'tstep' is set toyearthe day in each year with the e.g. the hottest or coldest hourly temperature is identified, and if 'tstep' is set tomonththe days in each month in each year with e.g. the hottest or coldest hourly temperatures are returned. Ifwhatis set totmaxortminthe hottest or coldest hour within each month or year are identified. Ifwhatis set totmedianhourly temperatures within the month or year are ranked and the median hour identified. The final option is to provide a vector of the days in the time sequence to return data for using inputdays. If providedtstep` is ignored. It is necessary that all hours of a given day are returned for two reasons. First, it ensures that the ground heat flux in the grid microclimate model can be estimated as it depends on the full diurnal cycle. Second because the hottest hour on a flat surface may not be the hottest hour on e.g. a steeply south-westerly facing slope - temperatures will typically peak later in the day when the slope is directly facing the sun. Returning hourly values for an entire day ensures that these terrain effects can be properly handled by the grid model.\n\nmicropoint &lt;- subsetpointmodel(micropoint, tstep = \"month\", what = \"tmax\")"
  },
  {
    "objectID": "doc/vignette-microclimf.html#preparing-model-inputs",
    "href": "doc/vignette-microclimf.html#preparing-model-inputs",
    "title": "Running microclimf",
    "section": "Preparing model inputs",
    "text": "Preparing model inputs\nThe entire grid model is run using runmicro, but to illustrate its working, we here run each component of the model in stages. If running it in stages, the first stage is to gather the input variables and reformat them ready to run the model. There are two options for preparing the data for running the model. Firstly where the climate data are in the form of a data frame of hourly weather for a point location. Second, where the climate data are in the form of course-gridded SpatRasters of values. Both cases are handled flexibly by the function modelin.\nIn the examples that follow, the inbuilt datasets of parameter values and a dtm for the study area, dtmcaerth are used and the model is run using a data.frame of weather data. Subsequently, the equivalent workflow for when weather data are in the form of course-gridded SpatRasters is shown.\n\nmicro &lt;- modelin(micropoint, vegp, soilc, dtmcaerth)\n\nBy default modelin calls function checkinputs. This performs some basic checks on the vegetation and soil parameters data to check for consistency in extent to the dtm. It also ensure values in the climate datasets are typical of what would be expected, thereby helping to ensure the correct units are used.\nIn subsequent downscaling of wind, the drag effects of vegetation, determined by vegetation height and foliage area are accounted for and calculated at this stage. In so doing, it is necessary to accommodate the possibility that the wind speed is not just affected by the surface roughness in each pixel, but also by vegetation surrounding the location. This is accommodated for by applying xyf which effectively smooths the surface roughness coefficients using terra::aggregate where xyf is the aggregation factor. If xyf is set to NA, the roughness coefficients are averaged across the entire study area. In the example above, we do not specify a value for xyf, and the default of one is therefore applied, which means that no smoothing is performed."
  },
  {
    "objectID": "doc/vignette-microclimf.html#running-the-model",
    "href": "doc/vignette-microclimf.html#running-the-model",
    "title": "Running microclimf",
    "section": "Running the model",
    "text": "Running the model\n\nSoil moisture\nThe first step of the microclimate model is to estimate soil moisture. This is handled in microclimf, by spatially distributing the soil moisture values returned by the point microclimate model for each time increment using the the Bevan and Kirkby (1979) topographic wetness index, such that valleys and flat areas are assumed to have higher water content. Values are adjusted such that the average for the study area in each time step is equivalent to the value obtained by running the point model. Users have the option to control the sensitivity of this topographic adjustment. Irrespective of whether the model is run in hourly or daily time-increments, these calculations are performed using the soilmdistribute as in the example below.\n\nmicro&lt;-soilmdistribute(micro)\npar(mfrow = c(1,1)) # make sure output is a single panel figure\nplot(rast(micro$soilm[,,134]), col = rev(mypal))\n\n\n\n\n\nRadiation\nThe next stage, needed to calculate soil surface temperature, is to estimate radiation absorbed by the ground. Because the ground lies below canopy in some instances, it is necessary also to consider the transmission of radiation through the canopy. The radiation fluxes are modelled using function twostream, which implements a variant of the Dickenson-Sellers two-stream radiation model described in Yuan et al. (2017) J Adv Model Earth Sy 9: 113–129 to model radiation interception by the canopy. It also varies from the Dickenson-Sellers model in more explicitly handing sloped ground surfaces beneath a canopy.\nAbsorbed radiation is the total incoming radiation received by a surface less that transmitted or reflected. The total incoming radiation can be partitioned into three sources, each of which is modified by the environment in slightly different ways. The first is direct radiation from the sun. Here, absorption depends on the angle of the surface relative to perpendicular. This is the reason why equatorward-facing slopes are warmer than those that face poleward and is indeed the main reason why temperature increases with latitude. Here the solar beam is more concentrated, rather like shining a torch directly on a surface as opposed to obliquely. The second source is diffuse solar radiation: that scattered by particles and clouds in the atmosphere. The final source is longwave radiation emitted from surrounding surfaces and the sky. The latter two are isotropic (i.e. having the same value when measured in any direction). In consequence, for these sources, the direction of the surface is unimportant, and radiation interception is instead influenced by sky-view.\nFunction twostream calculates all of these fluxes. If reqhgt &gt; 0 it also calculates the fluxes at the height of interest, including the upward fluxes resulting from reflection by the ground surface and scattering by leaves within the canopy. Additionally, to aid with modelling of air temperatures, it also calculates the flux density of radiation absorbed by both the canopy and the ground surface.\nTo model radiation, the canopy as a turbid medium and the transmission of radiation by vegetation is thus described using an equation similar to Beer’s law, in which flux density of radiation is assumed dependent on the total one-side lead area per unit ground area and by an extinction coefficient for the canopy. For direct radiation, the extinction coefficient is assumed to depend on the distribution of leaf angles (with more vertically orientated leaves transmitting less radiation at lower solar altitudes). This is where the model input vegp$x comes into play. For isotropic sources of radiation (i.e. diffuse and longwave), leaf angle is assumed unimportant.\nIn the example below the flux density of shortwave and longwave radiation absorbed by the ground surface at 10:00 hours on 20th Jun 2017 is shown. Below that, the flux density of the upward and downward radiation streams 5 cm above the ground are shown (the downward flux comprises both direct and diffuse radiation, the upward flux is assumed entirely diffuse).\n\nmicro &lt;- twostream(micro, reqhgt = 0.05)\npar(mfrow=c(1,2))\nplot(rast(micro$radGsw[,,131]), col = mypal, main = \"Shortwave\")\nplot(rast(micro$radGlw[,,131]), col = mypal, main = \"Longwave\")\n\n\n\n\nplot(rast(micro$Rbdown[,,131]+micro$Rddown[,,131]), col = mypal, main = \"Downward shortwave\")\nplot(rast(micro$Rdup[,,131]), col = mypal, main = \"Upward shortwave\")\n\n\n\n\n\nSensible heat flux and wind\nA surface heated by solar radiation will loose some of this heat to the surrounding air, and by virtue of the laws of energy conservation, the air gains this heat. The exchange of heat between a surface and the surrounding air is termed sensible heat exchange and is influenced strongly wind speed. The next stage of modelling is therefore to calculate wind speed.\nThe function wind models two processes. Firstly, direction-dependent terrain shelter coefficients are applied. Secondly, the effects of vegetation on wind speeds are determined. If reqhgt is above the vegetation, the shaped of the wind speed above vegetation is determined by the degree of surface drag, in turn contingent upon vegetation height and the plant area index of vegetation. If reqhgt is below canopy, the effects of vegetation is to attenuate wind speeds, but the shape of the wind-height profile below and above canopy differs. For a given reqhgt some pixels may lie below canopy and some above. This is all handled automatically by function wind as in the example below.\n\nmicro &lt;- wind(micro, reqhgt = 0.05)\npar(mfrow=c(1,1))\nplot(rast(micro$uz[,,100]), col = mypal, main = \"Wind speed\")\n\n\n\nThe lower wind speeds in the valley caused by terrain sheltering are evident. The speckle in the figure is caused by vegetation.\nOne minor additional point to note is that when calling any of the component functions after creating the model input using e.g. modelin there is no need to run the components prior to that. The function automatically checks whether these have been run, and if necessary does so.\n\n\nGround surface temperature\nOnce radiation absorbed by the ground and wind speed have been calculated, ground surface temperature can be computed. This is done using either function soiltemp_hr or soiltemp_dy depending on whether the model input is daily or hourly. Whereas twostream and wind handle both hourly or daily data, because ground heat fluxes are calculated in different ways depending on whether inputs are hourly or daily, the functions used for each differ. In the example below, the ground surface temperature on the hottest hour of the year is calculated using soiltemp_hr and then plotted.\n\nmicropoint&lt;-runpointmodel(climdata,0.05,dtmcaerth,vegp,soilc)\nmicropoint&lt;-subsetpointmodel(micropoint, tstep = \"month\", what = \"tmax\")\nmicro &lt;- modelin(micropoint, vegp, soilc, dtmcaerth)\nmicro &lt;- soiltemp(micro, reqhgt = 0.05)\n# Plot ground temperature of hottest hour\npar(mfrow=c(1,1))\nplot(rast(micro$Tg[,,134]), col = mypal, main = \"Soil surface temperature\")\n\n\n\nAs can be seen, the soil surface temperatures on bare, south-facing slopes get pretty hot\n\n\nAbove ground\nAfter calculating ground surface temperature, there are two pathways, depending on whether microclimatic conditions below or above ground are required. If reqhgt &gt; 0 then function aboveground is called as in the example below. This essentially runs the full microclimate model. Details of the model outputs are specified below.\n\nmout &lt;- aboveground(micro, reqhgt = 0.05)\nplot(rast(mout$Tz[,,134]), col = mypal, main = \"Air temperature\")\n\n\n\nIn the example above air temperature in the hottest hour is plotted. Users are free to experiment with plotting other model outputs. The air temperature at 5 cm is somewhat lower than the ground surface temperature, but much higher than ambient temperature on south-facing slopes, as one would expect\n\nAbove canopy\nThe function aboveground automatically works out which pixels are below canopy, and which above, but the microclimate is modelled differs. To work out temperatures above canopy, the canopy (and soil surface) are treated as a single layer of homogeneous phytomass and the energy balance solved to derive the mean temperature of the canopy. The canopy is then assumed to exchange heat with the air above it such that close to the heat exchange surface of the canopy, air temperatures similar to canopy temperatures, but are increasingly close to air temperature at reference height (i.e. the at height of the input weather station data). The result is a logarithmic temperature-height (and humidity) profile as re-reproduced in the example below in which the model is run at multiple heights over a small area with spatially uniform, terrain, soil and vegetation properties.\n\ndem &lt;- aggregate(rast(dtmcaerth), 10) * 0\n# Create spatially uniform vegetation and soil parameters  dataset\nvegp2 &lt;- list(pai = array(0.05, dim = c(5, 5, 12)),\n              hgt = aggregate(rast(vegp$hgt), 10) * 0 + 0.005,\n              x = aggregate(rast(vegp$x), 10) * 0 + 1,\n              gsmax = aggregate(rast(vegp$x), 10) * 0 + 1,\n              leafr = aggregate(rast(vegp$x), 10) * 0 + 0.3,\n              clump = array(0, dim = c(5, 5, 12)),\n              leafd = aggregate(rast(vegp$x), 10) * 0 + 0.05,\n              leaft = aggregate(rast(vegp$x), 10) * 0 + 0.15)\nsoilc2 &lt;- list(soiltype = aggregate(rast(soilc$soiltype), 10),\n               groundr = aggregate(rast(soilc$groundr), 10) * 0 + 0.15)\n# Run and subset point model\nmicropoint &lt;- runpointmodel(climdata, reqhgt = 0.05, dem, vegp2, soilc2)\nmicropoint &lt;- subsetpointmodel(micropoint, tstep = \"month\", what = \"tmax\")\n# Create model input\nmicro &lt;- modelin(micropoint, vegp2, soilc2, dem)\n# Run model for multiple heights\nreqhgts&lt;-c(0.01,0.02,0.05,0.1,0.2,0.5,1)\ntemps &lt;- 0\nfor (i in 1:length(reqhgts)) {\n  mout &lt;- aboveground(micro, reqhgt = reqhgts[i])     \n  temps[i] &lt;- mout$Tz[2, 2, 132] # Extract temperature for hottest hour   \n}\npar(mar = c (5, 5, 2, 2))\nplot(reqhgts ~ temps, type = \"l\", lwd = 2, xlab = \"Temperature\", ylab = \"Height\")\n\n\n\nOne minor point worth noting is that when running the point model above ground, the input reqhgt is just used to determine whether to return the required variables for modelling microclimate above ground and the point model itself does not need to be run for separate heights as it just returns temperature and other microclimate variables for the heat exchange surface of the canopy. This is not the case below ground, as here the point model returns microclimate parameters specifically associated with the specified depth.\n\n\nBelow canopy\nBelow canopy, variation in the energy budget within the canopy must be more explicitly handled. The microclimf package contains a theoretically-grounded model emulator of Raupach’s localised near-field model. In this model, the temperature (or humidity) is assumed to comprise both a `near-field and ‘far field’ contribution. In essence, the canopy is assumed to comprise multiple layers and far-field contribution is result from heat (or vapour) emanating from the entire canopy downwind of the point of interest. When the net energy balance of the canopy is positive, the result is an approximately linear increase in temperature (or vapour) as one descends through the canopy because at lower heights. However, for far-field-height profile, the effects of ground surface temperature must be accounted for, and the air temperature close to the ground is thus close to ground surface temperature. An additional ‘near-field’ contribution is then calculated, in the effect determined by the energy budget and the foliage density close to the height of interest. To calculate the foliage density, plausible assumptions about the vertical distribution of foliage are made, such that foliage density is determined from the plant area index. The model is not unduly sensitive to assumptions about the vertical distribution of foliage.\nThe net result is a more complex temperature-height (and humidity) profile as re-reproduced in the example below in which the model is run at multiple heights over a small area with spatially uniform, terrain, soil and vegetation properties.\n\n# Create spatially uniform vegetation a\\nd soil parameters  dataset\nvegp2 &lt;- list(pai = array(3, dim = c(5, 5, 12)),\n              hgt = aggregate(rast(vegp$hgt), 10) * 0 + 10,\n              x = aggregate(rast(vegp$x), 10) * 0 + 1,\n              gsmax = aggregate(rast(vegp$x), 10) * 0 + 1,\n              leafr = aggregate(rast(vegp$x), 10) * 0 + 0.3,\n              clump = array(0, dim = c(5, 5, 12)),\n              leafd = aggregate(rast(vegp$x), 10) * 0 + 0.05,\n              leaft = aggregate(rast(vegp$x), 10) * 0 + 0.15)\nsoilc2 &lt;- list(soiltype = aggregate(rast(soilc$soiltype), 10),\n               groundr = aggregate(rast(soilc$groundr), 10) * 0 + 0.15)\n# Run and subset point model (reqhgt just used to determine whether above or below ground)\ndem &lt;- aggregate(rast(dtmcaerth), 10) * 0\nmicropoint &lt;- runpointmodel(climdata, reqhgt = 10, dem, vegp2, soilc2)\nmicropoint &lt;- subsetpointmodel(micropoint, tstep = \"month\", what = \"tmax\")\n# Create model input\nmicro &lt;- modelin(micropoint, vegp2, soilc2, dem)\n# Run model for multiple heights\nreqhgts&lt;- 10^(c(-10:10) / 10)\ntemps &lt;- 0\nfor (i in 1:length(reqhgts)) {\n  mout &lt;- aboveground(micro, reqhgt = reqhgts[i])     \n  temps[i] &lt;- mout$Tz[2, 2, 132] # Extract temperature for hottest hour   \n}\npar(mar = c (5, 5, 2, 2))\nplot(reqhgts ~ temps, type = \"l\", lwd = 2, xlab = \"Temperature\", ylab = \"Height\")\n\n\n\n\n\nBelow ground\nIf reqhgt is negative, functions below_hr (hourly) or below_dy (daily) are used and it is assumed that microclimatic conditions below ground are needed. The way the model works is to assume, that once ground surface temperatures are calculated, that both the annual and diurnal temperatures cycle is dampened by the specific heat capacity and thermal conductivity of the soil, themselves contingent on soil physical characteristics and water content. There is also a phase shift\nThis can be seen in the example below in which the model is run in hourly time-increments for the entire year at three different depths and time-series of soil temperature plotted.\n\n# Run microclimate model for reqhgt = -0.05m\nmicropoint1 &lt;- runpointmodel(climdata, reqhgt = -0.05, dem, vegp2, soilc2)\nmicro1 &lt;- modelin(micropoint1, vegp2, soilc2, dem)\nmout1 &lt;- belowground(micro1, reqhgt = -0.05)\nT1&lt;-mout1$Tz[2,2,]\n# Run microclimate model for reqhgt = -0.2m\nmicropoint2 &lt;- runpointmodel(climdata, reqhgt = -0.2, dem, vegp2, soilc2)\nmicro2 &lt;- modelin(micropoint2, vegp2, soilc2, dem)\nmout2 &lt;- belowground(micro2, reqhgt = -0.2)\nT2&lt;-mout2$Tz[2,2,]\n# Run microclimate model for reqhgt = -1m \nmicropoint3 &lt;- runpointmodel(climdata, reqhgt = -1, dem, vegp2, soilc2)\nmicro3 &lt;- modelin(micropoint3, vegp2, soilc2, dem)\nmout3 &lt;- belowground(micro3, reqhgt = -1)\nT3&lt;-mout3$Tz[2,2,]\n# PLot soil temperature time-series\nplot(T1, type=\"l\", ylim = c(0, 25), col = \"red\", ylab = \"Temperature\", xlab = \"Hour\")\npar(new = TRUE)\nplot(T2, type=\"l\", ylim = c(0, 25), ylab = \"\", xlab = \"\")\npar(new = TRUE)\nplot(T3, type=\"l\", col = \"darkgray\", lwd = 2, ylim = c(0, 25), ylab = \"\", xlab = \"\")\n\n\n\nIt should be noted that the below ground microclimate model works best if the model is run over full time sequences. A simpler approximation method is used when running in the model with sub-set versions of the model.\n\n\n\nRunning the whole model\nThe whole model is run using runmicro. With method = \"R\" this function is essentially a wrapper function that run the various component functions and the call either aboveground or belowground. With method = \"Cpp\" the full model is run using c++ code, which avoids the need to store full arrays of variables returned by model sub-components in internal memory. Code for running the model in entirety is presented in the quick start section above.\n\n\nModel output and formats\nWhen running runmicro the model returns an an object off class microout. If reqhgt &gt; 0 then the following outputs are returned by default, though note that the user have the option to specify which model outputs are returned using parameter `out’, which is a vector of logicals indicating which variables to return ordered as for the listed outputs when ‘reqhgt &gt; 0’ as below.\n\nModel outputs:\n\nTz - an array of air temperatures at reqhgt (deg C)\ntleaf - an array of leaf temperatures at reqhgt (or average canopy temperatures for pixels where reqhgt is above vegetation (deg C)\nT0 - an array of ground surface temperatures (deg C)\n\nrelhum - an array of relative humidities at reqhgt (Percentage)\n\nsoilm - an array of soil moisture fractions in the top 10 cm of the soil (m^3 / m^3)\nwindspeed - an array of wind speeds at reqhgt (m/s)\nRdirdown - an array of downward direct (beam) radiation fluxes at reqhgt (W/m^2)\nRdifdown - an array of the downward diffuse radiation fluxes at reqhgt (W/m^2)\nRlwdown - an array of the downward longwave radiation fluxes at reqhgt (W/m^2)\nRswup - an array of upward shortwave radiation fluxes at reqhgt (W/m^2). Assumed to comprise entirely diffuse radiation.\nRlwup - an array of upward longwave radiation fluxes at reqhgt (W/m^2).\n\n\nIf reqhgt = 0 the same model outputs are returned, with the exception of wind speed, which is always 0 at the ground surface and relative humidity, where a value for the gorund surface in addition to a value for soil moisture is meaningless.\nIf reqhgt &lt; 0 then the same model outputs are return, but tleaf, relhum, windspeed, Rdirdown, Rdifdown, Rlwdown, Rswup and Rlwup are all set to NA.\nThe returned model object is a list with each of these variables returned as arrays.\nIt is also possible to write the outputs as netCDF4 files using function writetonc. This function takes an object of class microout and writes the outputs to the working directory in netCDF4 format. To save disk space, the data are stored as integers and therefore e.g. temperature is mulitplied by 100 prior to writing the data out. To handle the observation times a POSIXlt object of times must be added to the model output prior to writing. In the following example, the model is run,the POSIXlt object added and data written out and then read back in using the terra package:\n\nmicropoint &lt;- runpointmodel(climdata, reqhgt = 0.05, dtmcaerth, vegp, soilc)\nmicropoint &lt;- subsetpointmodel(micropoint, tstep = \"month\", what = \"tmax\")\nmout &lt;- runmicro(micropoint, reqhgt = 0.05, vegp, soilc, dtmcaerth)\nmout$tme &lt;- as.POSIXlt(micropoint$weather$obs_time, tz = \"UTC\")\nwritetonc(mout, \"modelout.nc\", dtmcaerth, reqhgt = 0.05)\nTz&lt;-rast(\"modelout.nc\", \"Tz\")\npar(mfrow=c(1,1))\nplot(Tz[[12]]/100, col = mypal)\n\n\n\n\n\nRunning the model with arrays of climate data\nIn the examples above, the climate data provided as inputs to the model are provided as a data.frame. However, it may be the case that microclimate surfaces are required over larger areas over which the input climate varies. This situation is at the model input stage and when running the point microclimate model, and by converting the climate variables to multi-layer SpatRasters rather than vectors as in the example below. Here a dummy list of climate arrays is created. Functions runpointmodela and subsetpointmodela are then called. These functions run and subset the point microclimate model over every grid cell of the array. A coarse resolution dtm is then created that matches the extent, coordinate reference system and resolution of the climate arrays as this is used during the model input handling stage. The grid model can then be run as spreviously: the function checks the format of the data passed to it prior to running the model. HOwever, there are some additional inputs that need to be passed to the function to handle elevation adjustments to the coarse-resolution climate data.\n\n# Internal functions\n.rast &lt;- function(m,tem) {\n  r&lt;-rast(m)\n  ext(r)&lt;-ext(tem)\n  crs(r)&lt;-crs(tem)\n  r\n}\n.ta&lt;-function(x,dtm,xdim=5,ydim=5) {\n   a&lt;-array(rep(x,each=ydim*xdim),dim=c(ydim,xdim,length(x)))\n   .rast(a,dtm)\n}\n# Create dummy array datasets\ndtm &lt;- rast(dtmcaerth) # unpack raster\nclimarrayr&lt;-list(temp = .ta(climdata$temp, dtm),\n  relhum = .ta(climdata$relhum, dtm),\n  pres = .ta(climdata$pres, dtm),\n  swdown = .ta(climdata$swdown, dtm),\n  difrad = .ta(climdata$difrad, dtm),\n  lwdown = .ta(climdata$lwdown, dtm),\n  windspeed = .ta(climdata$windspeed, dtm),\n  winddir = .ta(climdata$winddir, dtm),\n  precip = .ta(climdata$precip, dtm))\ntme &lt;- as.POSIXlt(climdata$obs_time, tz=\"UTC\")\n# Run and subset point model array (using subset defaults)\nmicropointa &lt;- runpointmodela(climarrayr, tme, reqhgt = 0.05, dtm, vegp, soilc)\nmicropointa &lt;- subsetpointmodela(micropointa)\n# Create coare-resolution dtm matching resolution of climate data\ndtmc &lt;- aggregate(dtm, 10, fun = \"mean\", na.rm = TRUE)\n# Run model wiht no altitude correction\nmout &lt;- runmicro(micropointa, reqhgt = 0.05, vegp, soilc, dtm, dtmc, altcorrect = 0) \n\nThe model takes a little longer to run this way, both because of the need to run the point model over multiple grid cells and because the point model outputs are resampled prior to running the grid model, but the code is still fairly efficient.\nIf altcorrect == 0', no elevation correction is performed. Ifaltcorrect &gt; 0’ difference between each pixel of dtm and dtmc are calculated and an elevation lapse rate correction is applied to the temperature and pressure data to account for these elevation differences. If altcorrect= 1, a fixed lapse rate of 5 degrees per 100m is applied to the temperature data. If altcorrect= 2, humidity-dependent lapse rates are calculated and applied."
  },
  {
    "objectID": "doc/vignette-microclimf.html#running-the-model-over-large-areas",
    "href": "doc/vignette-microclimf.html#running-the-model-over-large-areas",
    "title": "Running microclimf",
    "section": "Running the model over large areas",
    "text": "Running the model over large areas\nR stores all data into internal memory and although, by using C++ code, memory requirements are substantially improved, the RAM requirements of the microclimate model can be pretty high if outputs are desired for numerous timesteps at high-resolution over large areas. To circumvent this issue the model can be run as tiles. However, the way that terrain shading effects and topographic wetness are calculated means that model outputs are prone to edge effects. To circumvent this issue, the function runmicro_big and runmicro_biga can be used, the former handling climate data provided as a data.frame and the latter as a list of arrays. These functions calculated the wind shelter and topographic wetness over the entire study area before running the model in tiles,\nIn the example below, the application of runmicro_big is shown. First some example data are downloaded from Zenodo. These data are 10m resolution model inputs for the entire Lizard Peninsula in Cornwall, an area of approximately 400 square kilometers.\nIn the first example, using runmicro_big, the model is run in tiles with the climate data provided as a data.frame. The code first runs the point microclimate model. The relevant terrain variables are then calculated over the whole study area and the model is then run in tiles. The example takes quite a while to run, but is included here for illustration purposes. Data are stored in a subfolder called microut in the directory specified by pathout.\nSome warnings are given when running the function to notify the user that direct radiation values in a few instances close to dawn and dusk are higher than expected clear-sky radiation values. This is taken care of by the model - the excess is assigned as diffuse radiation, so is nothing to worry about.\n\n# Download example data from Zenodo\nurl &lt;- \"https://zenodo.org/records/15008936/files/runmicrobig.zip\"\npathout&lt;-\"C:/Temp/tiles/\"\ndir.create(pathout)\nsetwd(pathout)\ndownload.file(url, \"modeldata.zip\")\nunzip(\"modeldata.zip\")\n# Read in spatial data\nbig_vegp &lt;- readRDS(\"vegp_big.RDS\")\nbig_soilc &lt;-readRDS(\"soilc_big.RDS\")\ndtm&lt;-rast(\"dem.tif\")\n# Read in climate dataframe\nclimdatadf &lt;- readRDS(\"climdatapoint.RDS\")\n# Run and subset point model\nmicropoint &lt;- runpointmodel(climdatadf, reqhgt = 0.05, dtm, big_vegp, big_soilc)\nmicropoint &lt;- subsetpointmodel(micropoint, tstep = \"month\", what = \"tmax\")\n# Run the model in tiles\nrunmicro_big(micropoint, reqhgt = 0.05, pathout = pathout, big_vegp, big_soilc, dtm)\n\nThe optimal tile size is automatically calculated. Entirely blank tiles (i.e anything with NA in the digital elevation dataset are skipped). Users have the option to specify the tile size or format of the data written to disk and there are quite a few other options for flexibly handling how the model is run. The help file associated with runmicro_big gives details. One useful feature is the ability to specify a tile overlap to avoid possible tiling effects. Model output variables for overlapping tiles can then, once converted to terra::SpatRasters, can be merged using function mosaicblend. This applies a distance weigting to the overlapping area so that sharp boundary effects are avoided. Another potentially useful option is to write the model outputs as compressed nc files.\nIn the second example, using runmicro_biga, 1km resolution gridded climate data are used to drive the model. It is assumed that data have already been dowenloaded and unzipped. Again, the the code first runs the point microclimate model, but here the point model must be run for each of the ~400 1km grid cells, which itself takes a little while. Again, the relevant terrain variables are then calculated over the whole study area and the model is then run in tiles. As with runmicro_big, the size of tiles and the degree of overlap can be specified as user inputs.\n\n# Assumes data already downloaded from Zenodo - see above\npathout&lt;-\"C:/Temp/tiles/\"\nsetwd(pathout)\nbig_vegp &lt;- readRDS(\"vegp_big.RDS\")\nbig_soilc &lt;-readRDS(\"soilc_big.RDS\")\ndtm&lt;-rast(\"dem.tif\")\n# Read in gridded climate data\nclimdatag &lt;- readRDS(\"climdatagrid.RDS\")\n# Create other input variables\ntme &lt;- as.POSIXlt(c(0:8783) * 3600, origin = \"2020-01-01 00:00\", tz = \"UTC\")\ndtmc &lt;- aggregate(dtm, 100, fun = \"mean\", na.rm = TRUE)\n# Run and subset point model over each 1km grid cell\nmicropointa &lt;- runpointmodela(climdatag, tme, reqhgt = 0.05, dtm, big_vegp, big_soilc)\nmicropointa &lt;- subsetpointmodela(micropointa, tstep = \"month\", what = \"tmax\")\n# Run microclimate model in tiles (data saved to \"C:/Temp/tiles2/\")\nrunmicro_big(micropointa, reqhgt = 0.05, pathout = \"C:/Temp/tiles2/\", big_vegp, big_soilc, dtm, dtmc, altcorrect = 0)\n\nNote that in contrast to when run using a point model input, altitudinal corrections can be handled using the control parameter altcorrect."
  },
  {
    "objectID": "doc/vignette-microclimf.html#bioclim-variables",
    "href": "doc/vignette-microclimf.html#bioclim-variables",
    "title": "Running microclimf",
    "section": "Bioclim variables",
    "text": "Bioclim variables\nIt is common practice to seek to model the distributions of species using bioclimate variables, and the most commonly used are those available from Worldclim. While it is far from the case that simply modelling species distributions using microclimate data will circumvent all the numerous issues associated with doing so using coarse-resolution macroclimate data, a function for modelling microclimate equivalents of the standard 19 bioclim variables is nonetheless provided.\nBecause rainfall typically does not vary that much at fine- spatial resolution, the function instead calculates soil moisture equivalents for the rainfall-associated bioclim variables (in the top 10 cm of the soil).\nTo enhance computational efficiency the microclimate model is run for selected days only. Thus, to compute “mean annual temperature”, the mean ambient temperature of each day in the input weather data is calculated, the day with median temperatures in each month selected and the mean across months calculated. This is not, strictly speaking, the same as the mean temperature, but differences are likely to be minor, and for each year of data supplied, there is an approximately 30-fold gain in computational efficiency by calculating e.g. BIO1 in this way. Similarly, to calculate maximum temperature (BIO1), the day of the year with the hottest ambient temperature is selected, and microclimate temperatures calculated on this day only. This ignores the possibility that on a slightly cooler, but sunnier day, microclimate temperatures may be hotter at certain locations. AS with the various runmodel options, If hourly = TRUE all hours within a given day are selected and calculations performed on hourly data. If If hourly = FALSE only the hours corresponding to times when hourly temperatures are at their daily maximum and minimum and selected. This results in a c. 10-fold increase in computational efficiency, but cannot pick out areas where terrain results in near-ground temperatures reaching a maximum later in the afternoon than the peak in ambient temperature. If weather data for more than one year are supplied, only one set of median, maximum and minimum monthly temperature data are selected representing an average across years. Resultantly, there is little computational penalty if providing data for multiple years in comparison to one year of data.\nHowever, here we illustrate the function for one year of data only using the default inputs provided with the package. Note that this function flexibly handles the weather data being provided as either a data.frame or as a list of arrays. In the example below it is provided as a data.frame. The function takes ~20 seconds to run. The function returns a multilayer SpatRast of each of the bioclim variables\nThe input parameter temp indicates whether to return air or leaf temperature-derived bioclim variables. If temp = leaf, for grid cells where reqhgt is above vegetation, vertically averaged canopy temperature is used.\n\nmypal &lt;- colorRampPalette(c(\"darkblue\", \"blue\", \"green\", \"yellow\", \"orange\",  \"red\"))(255)\n# Set back to inbuilt datasets\nvegp &lt;- microclimf::vegp\nsoilc &lt;- microclimf::soilc\nbioclim &lt;- runbioclim(climdata, 0.05, vegp, soilc, dtm, temp = \"air\")\nplot(bioclim[[1]], col = mypal) # BIO1"
  },
  {
    "objectID": "doc/vignette-microclimf.html#snow",
    "href": "doc/vignette-microclimf.html#snow",
    "title": "Running microclimf",
    "section": "Snow",
    "text": "Snow\nIn all the examples above, no account of snow cover is taken, primarily because the location used for the examples, being nearly sub-tropical, is snow free. However, the same may not be true of other places, and the option to account for snow cover is included. This is handled by setting snow = TRUE in the input of runmicro and then also passing the outputs of the snow depth model. There are two options for running the snow model: a quick and slower method. The distinction between the only matters when microclimate is modelling for a subset of days. Using the slow method a full hourly model is run for each grid cell and snow depth, as one would expect, is partially contingent on snow depth in the previous time-step. The resulting output is then subset if required. Using the quick method, a point snow model is run for every hour, but the full grid model is only run for the subset of days for which snow depths are required. To ensure a sensible snow budget the point model is used to calculate hourly melt, and the nature of the terrain and vegetation used to calculate a melt factor by which to multiply snow melt derived form the point model. The application of both approaches is shown below.\n\n# Run and subset micropoint model using inbuilt datasets\nclimdata$temp &lt;- climdata$temp - 8 # Make it colder so there is snow\nmicropoint &lt;- runpointmodel(climdata, reqhgt = 0.05, dtmcaerth, vegp, soilc) \nmicropoint &lt;- subsetpointmodel(micropoint, tstep = \"month\", what = \"tmin\")\n# Run the snow model using the quick method\nsmod1 &lt;- runsnowmodel(climdata, micropoint, vegp, soilc, dtmcaerth, method = \"fast\")\n# Run the snow model using the slow method\nsmod2 &lt;- runsnowmodel(climdata, micropoint, vegp, soilc, dtmcaerth, method = \"slow\")\n# Compare mean ground snow depths through time\nsdepth1 &lt;- apply(smod1$groundsnowdepth, 3, mean, na.rm = TRUE)\nsdepth2 &lt;- apply(smod2$groundsnowdepth, 3, mean, na.rm = TRUE)\nplot(sdepth1, type = \"l\", ylim = c(0, 0.2))\npar(new = TRUE)\nplot(sdepth2, type = \"l\", ylim = c(0, 0.2), col = \"blue\")\n\n\n\nIn addition to ground snow depth, total snow water equivalent and average snowpack and ground snow temperature are returned. Though the quick method is a bit crude, the resulting microclimate outputs are very similar to those obtained using the slow method, only really deviating when the two methjods give different estimates of snow cover. Here we demonstrate this by running and comparing the microclimate model using both snow outputs.\n\n# Run microclimate model with snow using outputs from the quick model\nmout1 &lt;- runmicro(micropoint, reqhgt = 0.05, vegp, soilc, dtmcaerth, snow = TRUE, snowmod = smod1)\n# Run microclimate model with snow using outputs from the quick model\nmout2 &lt;- runmicro(micropoint, reqhgt = 0.05, vegp, soilc, dtmcaerth, snow = TRUE, snowmod = smod2)\nTz1 &lt;- apply(mout1$Tz, 3, mean, na.rm = TRUE)\nTz2 &lt;- apply(mout2$Tz, 3, mean, na.rm = TRUE)\nplot(Tz1, type = \"l\", ylim = c(-10, 25), ylab = \"Temperature\")\npar(new = T)\nplot(Tz2, type = \"l\", ylim = c(-10, 25), col = \"blue\", ylab = \"\")"
  }
]