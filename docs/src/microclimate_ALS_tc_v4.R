#' --- ENVI-met 3DPLANT column generator from voxelized ALS data ---
#' Full pipeline from ALS voxelization to XML export for ENVI-met 3DPLANT.
#' Includes normalization, topographic metrics, LAD calculation, clustering, and XML writing.

# === Load Libraries ===
library(lidR)
library(terra)
library(dplyr)
library(tidyr)
library(sf)
library(here)
library(XML)
library(stats)
library(tibble)
library(rprojroot)
library(tools)
library(RANN)
library(clusternomics)
library(e1071)
library(entropy)
library(NbClust)
library(matrixStats)

# === Configuration ===
ts <- data.frame(
  ID = 1:12,
  value = c("agriculture", "alder", "ash", "beech", "douglas_fir", "larch",
            "oak", "pastures", "roads", "settlements", "spruce", "water")
)
valid_species <- c("alder", "ash", "beech", "douglas_fir", "larch", "oak", "spruce")
valid_ids <- ts$ID[ts$value %in% valid_species]

visualize <- FALSE
las_file <- here("data/ALS/tiles/")
res_xy <- 2
res_z <- 2
k <- 0.3
scale_factor <- 1.2
crs_code <- 25832
output_gpkg <- "data/envimet/envimet_p3dtree_points.gpkg"
xml_output_file <- "data/envimet/als_envimet_trees.pld"
species_raster <- rast("data/aerial/treespecies_cleaned.tif")

dir.create("data/output", showWarnings = FALSE, recursive = TRUE)
source("src/new_utils.R")

# === â¬› STAGE: LAS Merging and Normalization ===
las_fn <- merge_las_tiles(las_file, "data/ALS/merged_output.laz", chunk_size = 10000, workers = 6)
las <- readLAS(las_fn)
crs(las) <- "EPSG:25832"

# === â¬› STAGE: Ground Classification and DEM/CHM Generation ===
# âš  MEMORY: large point cloud + rasters
chm_pre <- rasterize_canopy(las, res = res_xy, algorithm = pitfree(c(0,1,3,6,9,12,16)))
rugosity <- terra::focal(chm_pre, w = matrix(1, 3, 3), fun = sd, na.rm = TRUE)
mean_rug <- global(rugosity, fun = "mean", na.rm = TRUE)[[1]]

csf_params <- if (mean_rug > 1) {
  message("Detected complex/dense canopy â€“ using fine CSF settings")
  csf(cloth_resolution = 0.5, rigidness = 2, class_threshold = 0.4, iterations = 800)
} else {
  message("Detected open canopy â€“ using coarse CSF settings")
  csf(cloth_resolution = 1.5, rigidness = 4, class_threshold = 0.6, iterations = 300)
}

las <- classify_ground(las, csf_params)
dem_algo <- eval(parse(text = recommend_dem_interpolation(las, res_xy)))
dem <- rasterize_terrain(las, res = res_xy, algorithm = dem_algo)
dsm <- rasterize_canopy(las, res = res_xy, algorithm = p2r())

las_norm <- normalize_height(las, algorithm = knnidw(k = 6L, p = 2))

# âœ… Free original LAS
#las <- NULL; gc()

# === â¬› STAGE: CHM + DSM + Topographic Metrics ===
pit_algo <- pitfree(c(0, 1, 3, 6, 9, 12, 16))
chm <- rasterize_canopy(las_norm, res = res_xy, algorithm = pit_algo)
slope <- terrain(dem, "slope", unit = "radians")
aspect <- terrain(dem, "aspect", unit = "degrees")
TPI <- terra::focal(terrain(dsm, "TPI"), w = matrix(1,3,3), fun = mean)
TPI[TPI < 0] <- -1; TPI[TPI > 0] <- 1

topo <- c(dem, dsm, chm, slope, aspect, TPI)
names(topo) <- c("dem", "dsm", "chm", "slope", "aspect", "TPI")
writeRaster(topo, "data/ALS/topo_stack.tif", overwrite = TRUE)
plot(topo)

# === â¬› STAGE: Voxelization + LAD ===
# âš  MEMORY: voxel and LAD matrices can be huge
voxels <- preprocess_voxels(las_norm, res_xy, res_z)
lad_df <- convert_to_LAD_beer(voxels, grainsize = res_z, k = k, scale_factor = scale_factor)
saveRDS(voxels,"data/voxels.rds")

# âœ… Free voxelized LAS
#las_norm <- NULL; voxels <- NULL; gc()



# === â¬› STAGE: Compute structural indices ===
# Only keep numeric LAD columns

lad_matrix <- select(lad_df, starts_with("lad_")) |> as.matrix()
rownames(lad_matrix) <- paste(lad_df$X, lad_df$Y, sep = "_")

# More efficient summary statistics
lad_df$LAD_mean        <- matrixStats::rowMeans2(lad_matrix, na.rm = TRUE)
lad_df$LAD_max         <- matrixStats::rowMaxs(lad_matrix, na.rm = TRUE)
lad_df$LAD_height_max  <- max.col(lad_matrix, ties.method = "first") * res_z

# Skewness, kurtosis, entropy â€” still using apply (no native rowSkewness etc.)
lad_df$LAD_skewness    <- apply(lad_matrix, 1, skewness)
gc()
lad_df$LAD_kurtosis    <- apply(lad_matrix, 1, kurtosis)
gc()
lad_df$LAD_entropy     <- apply(lad_matrix, 1, entropy)
gc()

# === â¬› STAGE: Ecological indices ===
n_layers <- ncol(lad_matrix)
top_third <- seq(ceiling(2 * n_layers / 3), n_layers)
lad_df$Gap_Fraction <- rowMeans(lad_matrix == 0, na.rm = TRUE)
lad_df$Canopy_Cover_Top <- rowMeans(lad_matrix[, top_third] > 0, na.rm = TRUE)
lad_df$LAD_CV <- apply(lad_matrix, 1, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE))

# Vertical evenness
lad_sums <- rowSums(lad_matrix, na.rm = TRUE)
lad_df$Vertical_Evenness <- sapply(seq_len(nrow(lad_matrix)), function(i) {
  p <- lad_matrix[i, ] / lad_sums[i]
  p <- p[p > 0]
  -sum(p * log(p)) / log(length(p))
})

# âœ… Remove LAD matrix if done
gc()

# === â¬› STAGE: Add topographic & species data ===
topo_stack <- rast("data/ALS/topo_stack.tif")

# Erzeuge ein sf-Objekt aus lad_df
sf_lad <- st_as_sf(as.data.frame(lad_df), coords = c("X", "Y"), crs = crs_code)
geom_only <- st_geometry(sf_lad)
lad_df$elev   <- exactextractr::exact_extract( topo_stack[["dem"]], st_buffer(geom_only, dist = 0.1), "mean")
lad_df$slope  <- exactextractr::exact_extract( topo_stack[["slope"]], st_buffer(geom_only, dist = 0.1), "mean")
lad_df$aspect <- exactextractr::exact_extract( topo_stack[["aspect"]], st_buffer(geom_only, dist = 0.1), "mean")
lad_df$TPI    <- exactextractr::exact_extract( topo_stack[["TPI"]], st_buffer(geom_only, dist = 0.1), "mean")
lad_df$CHM    <- exactextractr::exact_extract( topo_stack[["chm"]], st_buffer(geom_only, dist = 0.1), "mean")
lad_df$species_class <- exactextractr::exact_extract( species_raster, st_buffer(geom_only, dist = 0.1), "mean")

saveRDS(lad_df,"data/lad_df.rds")

# âœ… Remove sf and stack
#sf_lad <- NULL; topo_stack <- NULL;geom_only=NULL; gc()

# === â¬› STAGE: Combine data for clustering ===
lad_matrix <- lad_df %>%
  select(starts_with("lad_pulses_"), starts_with("LAD_"),LAD_skewness,LAD_kurtosis,LAD_CV ,LAD_entropy , Vertical_Evenness) %>%
  as.matrix()
rownames(lad_matrix) <- paste(lad_df$X, lad_df$Y, sep = "_")

# === â¬› STAGE: Filter for valid species and complete rows ===
valid_idx <- lad_df$species_class %in% valid_ids
lad_df <- lad_df[valid_idx, ]
lad_matrix <- lad_matrix[valid_idx, ]

pulse_cols <- grep("^lad_pulses_", colnames(lad_matrix), value = TRUE)
lad_pulses <- lad_matrix[, pulse_cols]

# Filter valid rows for clustering
valid_rows <- apply(lad_pulses, 1, function(x) all(is.finite(x) & !is.na(x)))
lad_pulses <- lad_pulses[valid_rows, ]
lad_df <- lad_df[valid_rows, ]

# === â¬› STAGE: PCA sampling + NbClust ===
set.seed(42)
sample_idx <- sample(seq_len(nrow(lad_pulses)), floor(nrow(lad_pulses) * 0.01))
sample_data <- lad_pulses[sample_idx, ]

cat("Sample size before NA filtering:", nrow(sample_data), "\n")
keep_cols <- which(colMeans(is.na(sample_data)) <= 0.2)
sample_data <- sample_data[, keep_cols]
sample_data <- sample_data[, apply(sample_data, 2, var, na.rm = TRUE) > 1e-10]
sample_data <- na.omit(sample_data)
cat("Sample size after cleaning:", nrow(sample_data), "\n")

# âš  MEMORY: PCA and NbClust can explode RAM use
pca_res <- prcomp(sample_data, scale. = TRUE)
pc_info <- suggest_n_pcs(pca_res, variance_cutoff = 0.9)
sample_data_pca <- pca_res$x[, 1:pc_info$n_pcs]

nb <- NbClust(sample_data_pca, distance = "euclidean", min.nc = 2, max.nc = 30, method = "kmeans")
optimal_k <- as.integer(names(which.max(table(nb$Best.nc[1, ]))))


# Best number of clusters
cat("ðŸ”¢ Best number of clusters (Best.nc):", optimal_k, "\n\n")



        # --- Bereinige ungÃ¼ltige Zeilen fÃ¼r Clustering ---
        valid_rows <- apply(lad_pulses, 1, function(x) all(is.finite(x) & !is.na(x)))
        lad_pulses <- lad_pulses[valid_rows, ]
        valid_rows <- apply(lad_df, 1, function(x) all(is.finite(x) & !is.na(x)))
        lad_df_clean <- lad_df[valid_rows, ]

        # 1. Clustering-Spalten selektieren
        lad_dftocluster <- lad_df_clean %>%
          select(starts_with("lad_pulses_"),
                 starts_with("LAD_"),
                 LAD_skewness, LAD_kurtosis, LAD_CV, LAD_entropy, Vertical_Evenness) %>%
          mutate(across(everything(), ~ ifelse(. <= 2e-5, 0, .)))
        
        #lad_df <- lad_df[valid_rows, ]
        lad_features <- lad_df_clean %>%
          select(starts_with("lad_pulses_"), starts_with("LAD_"), LAD_skewness, LAD_kurtosis, LAD_CV, LAD_entropy, Vertical_Evenness) %>%
          mutate(across(everything(), ~ ifelse(. <= 2e-5, 0, .)))   
        # Entferne leere Zeilen
        lad_dftocluster <- lad_dftocluster[rowSums(lad_dftocluster > 0) > 3, ]
        
        lad_dftocluster_scaled <- scale(lad_dftocluster)
  
        
# --- Clustering (KMeans_arma von ClusterR) ---
km_arma <- ClusterR::KMeans_arma(
  data = lad_dftocluster,
  clusters = optimal_k,
  n_iter = 100,
  seed_mode = "random_subset"
)

# --- Cluster-Zuweisung berechnen ---
lad_df_clean$cluster <- as.integer(
  ClusterR::predict_KMeans(
    data = lad_dftocluster,
km_arma
  )
)

library(dplyr)

# Schritt 1: Synthetische Mittelprofile erzeugen (alle LADs mitteln je Cluster)
cluster_profiles <- lad_df_clean %>%
  group_by(cluster) %>%
  summarise(across(starts_with("lad_"), mean, na.rm = TRUE), .groups = "drop")

# Schritt 2: Eindeutige ENVIMET_ID je Cluster erzeugen
cluster_profiles <- cluster_profiles %>%
  mutate(ENVIMET_ID = paste0("PLANT", formatC(cluster, width = 3, flag = "0")))
# Schritt 3: ID zurÃ¼ck an alle Originalpunkte hÃ¤ngen
lad_df_clean <- lad_df_clean %>%
  left_join(cluster_profiles %>% select(cluster, ENVIMET_ID), by = "cluster")

        # ENVI-met exportieren
        export_envimet_profiles(
          lad_df_clean = lad_df_clean,
          res_z = 1  # oder 2 â€“ je nachdem, wie deine VoxelhÃ¶he lautet
        )
        
        # microclimf Input exportieren
        export_microclimf_profiles(
          lad_df_clean = lad_df_clean
        )
        
     export_full_lad_profiles_gpkg(
          lad_df_clean = lad_df_clean,
          out_gpkg = "data/lad_profiles_full.gpkg",
          layer_name = "lad_profiles"
        )
        # --- Plot spatial distribution of clusters (optional diagnostics) ---
        if (visualize) {
          library(ggplot2)
          ggplot(lad_df, aes(x = X, y = Y, color = as.factor(cluster))) +
            geom_point(size = 0.8) +
            coord_equal() +
            scale_color_viridis_d(name = "Cluster") +
            labs(title = "Spatial distribution of LAD clusters") +
            theme_minimal()
        }

        